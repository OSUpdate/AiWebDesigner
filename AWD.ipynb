{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AWD.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"0XMrKgafq6c9","colab_type":"code","outputId":"5e464aa8-f161-467c-f524-341cd3224c61","executionInfo":{"status":"ok","timestamp":1561315761341,"user_tz":-540,"elapsed":4245,"user":{"displayName":"서민성","photoUrl":"","userId":"06770200570856547998"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["reset"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AslSpDCdVL_W","colab_type":"code","outputId":"74bc0a1f-699a-4e6d-d6d6-5488d64a47a3","executionInfo":{"status":"ok","timestamp":1565495921252,"user_tz":-540,"elapsed":4069,"user":{"displayName":"서민성","photoUrl":"","userId":"06770200570856547998"}},"colab":{"base_uri":"https://localhost:8080/","height":523}},"source":["# 사용되는 GPU 상태 보여주는 코드\n","import tensorflow as tf\n","from tensorflow.python.client import device_lib\n","\n","tf.test.gpu_device_name()\n","device_lib.list_local_devices()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[name: \"/device:CPU:0\"\n"," device_type: \"CPU\"\n"," memory_limit: 268435456\n"," locality {\n"," }\n"," incarnation: 17093504356585333123, name: \"/device:XLA_CPU:0\"\n"," device_type: \"XLA_CPU\"\n"," memory_limit: 17179869184\n"," locality {\n"," }\n"," incarnation: 14828404428544861581\n"," physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n"," device_type: \"XLA_GPU\"\n"," memory_limit: 17179869184\n"," locality {\n"," }\n"," incarnation: 15815470637660546102\n"," physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n"," device_type: \"GPU\"\n"," memory_limit: 14892338381\n"," locality {\n","   bus_id: 1\n","   links {\n","   }\n"," }\n"," incarnation: 4771973201577832715\n"," physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"nBjR06km1G4C","colab_type":"code","outputId":"c51678f3-1223-4ae0-bd35-d72799498cf1","executionInfo":{"status":"ok","timestamp":1565578546028,"user_tz":-540,"elapsed":27377,"user":{"displayName":"서민성","photoUrl":"","userId":"06770200570856547998"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["# 구글 코랩에 구글 드라이브 연동하는 코드\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j1dQaXD-9zJ_","colab_type":"code","outputId":"f30d77b9-7232-489a-f8d3-63235097fd59","executionInfo":{"status":"ok","timestamp":1561309870254,"user_tz":-540,"elapsed":1095,"user":{"displayName":"서민성","photoUrl":"","userId":"06770200570856547998"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["# 모델 테스트용 데이터 준비: tf_Model 이 입력으로 1000*1000 이미지를 받을 때 사용했던 거\n","import numpy as np\n","import tensorflow as tf\n","import cv2\n","\n","# 모델이 만들어지는지 테스트용 데이터 생성, 20개\n","image = []\n","for i in range(1, 21):\n","    image.append(np.zeros((1000, 1000, 1)))\n","\n","image = np.array(image)\n","    \n","# 라벨 데이터 준비\n","t_list = np.zeros((len(image),5))\n","\n","val = 0\n","cnt = 0\n","for i in range(len(image)):\n","    t_list[i][val] = 1\n","    \n","    cnt+=1\n","    if cnt == 4:\n","        cnt=0\n","        val+=1\n","\n","# 이미지와 라벨 모양 출력\n","print(image.shape)\n","print(t_list.shape)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(20, 1000, 1000, 1)\n","(20, 5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RSKnk6REQOwj","colab_type":"code","outputId":"512c0fe3-3b2b-440b-b8de-e95bf380b0f9","executionInfo":{"status":"ok","timestamp":1561309962139,"user_tz":-540,"elapsed":60427,"user":{"displayName":"서민성","photoUrl":"","userId":"06770200570856547998"}},"colab":{"base_uri":"https://localhost:8080/","height":179}},"source":["# 모델 차원 테스트\n","# 아래에서 모델을 생성한 후 위에있는 테스트 데이터를 사용해서 \n","# 전방향 진행에 대한 차원의 변화 확인\n","x_b=image\n","y_b=t_list\n","\n","print('x_b shape: ', x_b.shape)\n","print('y_b shape: ', y_b.shape)\n","\n","model.print_proc(x_b, y_b)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["x_b shape:  (20, 1000, 1000, 1)\n","y_b shape:  (20, 5)\n","(20, 450, 450, 16)\n","(20, 200, 200, 32)\n","(20, 90, 90, 64)\n","(20, 518400)\n","(20, 125)\n","(20, 2)\n","(20, 5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TRM0FPigL7Yc","colab_type":"code","colab":{}},"source":["# 직접 만든 모델\n","# 학습이 잘 이루어지지 않음\n","import tensorflow as tf\n","import numpy as np\n","\n","class tf_Model:\n","    def __init__(self):\n","        # 세션 준비\n","        tf.reset_default_graph()\n","        self.sess = tf.Session()\n","        \n","    def var_set(self, params):\n","        x_h = params[0]\n","        x_w = params[1]\n","        x_c = params[2]\n","        \n","        out_n = params[3]\n","        \n","        f_s1 = params[4]\n","        f_s2 = params[5]\n","        f_s3 = params[6]\n","        \n","        f_n1 = params[7]\n","        f_n2 = params[8]\n","        f_n3 = params[9]\n","        \n","        a_n1 = params[10]\n","        a_n2 = params[11]\n","        a_n3 = params[12]\n","        \n","        # 입력 데이터와 라벨을 담을 변수 준비\n","        self.x = tf.placeholder(tf.float32,shape=[None,x_h,x_w,x_c], name='x')\n","        self.y_label = tf.placeholder(tf.float32,shape=[None,out_n], name='y')\n","        \n","        # 가중치 및 바이어스 초기화\n","        self.weight_conv1 = tf.Variable(tf.truncated_normal([f_s1,f_s1,x_c,f_n1],stddev=0.1), name='W1')\n","        self.bias_conv1 = tf.Variable(tf.constant(0.1,shape=[f_n1]), name='b1')\n","        \n","        #2. 두번째 계층의 가중치와 바이어스(컨블루션 계층)\n","        self.weight_conv2 = tf.Variable(tf.truncated_normal([f_s2,f_s2,f_n1,f_n2],stddev=0.1), name='W2')\n","        self.bias_conv2 = tf.Variable(tf.constant(0.1,shape=[f_n2]), name='b2')\n","        \n","        #3. 세번째 계층의 가중치와 바이어스(컨블루션 계층)\n","        self.weight_conv3 = tf.Variable(tf.truncated_normal([f_s3,f_s3,f_n2,f_n3],stddev=0.1), name='W3')\n","        self.bias_conv3 = tf.Variable(tf.constant(0.1,shape=[f_n3]), name='b3')\n","        \n","        #4. 네번째 계층의 가중치와 바이어스(완전연결 계층)\n","        self.weight_affine1 = tf.Variable(tf.truncated_normal([a_n1,a_n2], stddev=0.1), name='W4')\n","        self.bias_affine1 = tf.Variable(tf.truncated_normal([a_n2]), name='b4')\n","        \n","        \n","        #5. 다섯번째 계층의 가중치와 바이어스(완전연결 계층)\n","        self.weight_affine2 = tf.Variable(tf.truncated_normal([a_n2,a_n3], stddev=0.1), name='W5')\n","        self.bias_affine2 = tf.Variable(tf.constant(0.1,shape=[a_n3]), name='b5')\n","        \n","        #6.  여섯번째 계층의 가중치와 바이어스(완전연결 계층)\n","        self.weight_affine3 = tf.Variable(tf.truncated_normal([a_n3,out_n], stddev=0.1), name='W6')\n","        self.bias_affine3 = tf.Variable(tf.constant(0.1,shape=[out_n]), name='b6')\n","        \n","        #5. 드롭아웃 기준 설정\n","        self.dropout_ratio = tf.placeholder(tf.float32, name='dropout_ratio')\n","        \n","        \n","        #1. 첫번째 계층(컨볼루션-활성화-폴링)\n","        self.conv1      = tf.nn.conv2d(self.x,self.weight_conv1,strides=[1,1,1,1], padding='SAME')\n","        self.conv1_add  = tf.add(self.conv1, self.bias_conv1)\n","        self.b_n1       = tf.layers.batch_normalization(self.conv1_add)\n","        self.actv1      = tf.nn.softplus(self.b_n1)\n","        self.pool1      = tf.nn.max_pool(self.actv1,ksize=[1,2,2,1],strides=[1,2,2,1], padding='SAME')\n","        \n","        #2. 두번째 계층(컨볼루션-활성화-폴링)\n","        self.conv2      = tf.nn.conv2d(self.pool1,self.weight_conv2,strides=[1,1,1,1], padding='SAME')\n","        self.conv2_add  = tf.add(self.conv2, self.bias_conv2)\n","        self.b_n2       = tf.layers.batch_normalization(self.conv2_add)\n","        self.actv2      = tf.nn.softplus(self.b_n2)\n","        self.pool2      = tf.nn.max_pool(self.actv2,ksize=[1,2,2,1],strides=[1,2,2,1], padding='SAME')\n","        \n","        #3. 세번째 계층(컨볼루션-활성화-폴링)\n","        self.conv3      = tf.nn.conv2d(self.pool2,self.weight_conv3,strides=[1,1,1,1], padding='SAME')\n","        self.conv3_add  = tf.add(self.conv3, self.bias_conv3)\n","        self.b_n3       = tf.layers.batch_normalization(self.conv3_add)\n","        self.actv3      = tf.nn.softplus(self.b_n3)\n","        self.pool3      = tf.nn.max_pool(self.actv3,ksize=[1,2,2,1],strides=[1,2,2,1], padding='SAME')\n","        \n","        #4. 네번째 계층 수행 전 데이터 변환(각 데이터당 2차원데이터 -> 1차원데이터 변환)\n","        self.flat = tf.reshape(self.pool3,[-1,a_n1])\n","        \n","        #5. 네번째 계층(가중치와 행렬곱-활성화-드롭아웃처리)\n","        self.affine1 = tf.add(tf.matmul(self.flat,self.weight_affine1), self.bias_affine1, name='affine1')\n","        self.b_n4  = tf.layers.batch_normalization(self.affine1)\n","        self.actv4 = tf.nn.softplus(self.b_n4)\n","        self.drop1 = tf.nn.dropout(self.actv4,self.dropout_ratio)\n","        \n","        #6. 다섯번째 계층(가중치와 행렬곱-활성화-드롭아웃처리)\n","        self.affine2 = tf.add(tf.matmul(self.drop1,self.weight_affine2), self.bias_affine2, name='affine2')\n","        self.b_n5  = tf.layers.batch_normalization(self.affine2)\n","        self.actv5 = tf.nn.softplus(self.b_n5)\n","        self.drop2 = tf.nn.dropout(self.actv5,self.dropout_ratio)\n","        \n","        #7. 여섯번째 계층(가중치와 행렬곱-활성화)\n","        self.affine3 = tf.add(tf.matmul(self.drop2,self.weight_affine3), self.bias_affine3)\n","        self.softmax = tf.nn.softmax(self.affine3, name='soft_max')\n","        \n","        # 오차 계산\n","        self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.affine3, labels=self.y_label, name='cross_entropy')\n","        self.cost = tf.reduce_mean(self.cross_entropy)\n","        \n","        # 가중치 수정\n","        self.train_step = tf.train.AdamOptimizer(0.001).minimize(self.cost)\n","        \n","        # 정확도 측정\n","        self.correct_prediction = tf.equal(tf.argmax(self.softmax,1), tf.argmax(self.y_label,1), name='corr')\n","        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32), name='acc')\n","                \n","    def model_make(self, params):\n","        '''\n","        여기서 텐서플로우로 인공 신경망 모델을 만든다\n","        매개변수로 주는 리스트 순서\n","        [x_h, x_w, x_c, out_n, f_s1, f_s2, .. , f_n1, f_n2, .. , a_n1, a_n2, .. ]\n","        \n","        x_h, x_w, x_c: 입력하는 데이터의 높이, 너비, 차원\n","        \n","        out_n     : 출력 계층의 노드 갯수\n","        \n","        f_s[숫자] : 컨볼루션 필터의 크기\n","        f_n[숫자] : 컨볼루션 계층의 필터 갯수\n","        \n","        a_n[숫자] : Affine 계층의 노드 갯수\n","        \n","        '''\n","        self.var_set(params)\n","        self.sess.run(tf.global_variables_initializer())\n","            \n","    def model_save(self, path):\n","        saver = tf.train.Saver()\n","        save_path = saver.save(self.sess, path)\n","        print(\"SavePath: %s\"%(save_path))\n","        \n","    def model_load(self, path):\n","        saver = tf.train.Saver()\n","        saver.restore(self.sess, path)\n","        \n","    def model_train(self, x_b, y_b):\n","        self.sess.run(self.train_step,feed_dict={self.x: x_b, self.y_label: y_b, self.dropout_ratio: 0.5})\n","                        \n","    def get_relu(self, x_b, y_b):\n","        res = self.sess.run(self.actv5,feed_dict={self.x: x_b, self.y_label: y_b, self.dropout_ratio: 1.0})\n","        return res\n","        \n","    def get_softmax(self, x_b, y_b):\n","        res = self.sess.run(self.softmax,feed_dict={self.x: x_b, self.y_label: y_b, self.dropout_ratio: 1.0})\n","        return res\n","    \n","    def get_cross_entropy(self, x_b, y_b):\n","        res = self.sess.run(self.cross_entropy,feed_dict={self.x: x_b, self.y_label: y_b, self.dropout_ratio: 1.0})\n","        return res\n","    \n","    def get_correct_prediction(self, x_b, y_b):\n","        res = self.sess.run(self.correct_prediction,feed_dict={self.x: x_b, self.y_label: y_b, self.dropout_ratio: 1.0})\n","        return res\n","    \n","    def get_accuracy(self, x_b, y_b):\n","        res = self.sess.run(self.accuracy,feed_dict={self.x: x_b, self.y_label: y_b, self.dropout_ratio: 1.0})\n","        return res\n","    \n","    def get_xy(self, x_b):\n","        res = self.sess.run(self.affine2,feed_dict={self.x: x_b, self.dropout_ratio: 1.0})\n","        return res\n","    \n","    def print_proc(self, x_b, y_b):\n","        res = self.sess.run(self.pool1,feed_dict={self.x: x_b, self.dropout_ratio: 0.5})\n","        print(np.array(res).shape)\n","        res = self.sess.run(self.pool2,feed_dict={self.x: x_b, self.dropout_ratio: 0.5})\n","        print(np.array(res).shape)\n","        res = self.sess.run(self.pool3,feed_dict={self.x: x_b, self.dropout_ratio: 0.5})\n","        print(np.array(res).shape)\n","        res = self.sess.run(self.flat,feed_dict={self.x: x_b, self.dropout_ratio: 0.5})\n","        print(np.array(res).shape)\n","        res = self.sess.run(self.drop1,feed_dict={self.x: x_b, self.dropout_ratio: 0.5})\n","        print(np.array(res).shape)\n","        res = self.sess.run(self.drop2,feed_dict={self.x: x_b, self.dropout_ratio: 0.5})\n","        print(np.array(res).shape)\n","        res = self.sess.run(self.affine3,feed_dict={self.x: x_b, self.dropout_ratio: 0.5})\n","        print(np.array(res).shape)\n","        \n","    def print_accs(self, x_b, y_b):\n","        res1 = self.sess.run(self.cross_entropy,feed_dict={self.x: x_b, self.y_label: y_b, self.dropout_ratio: 1.0})\n","        res2 = self.sess.run(self.accuracy,feed_dict={self.x: x_b, self.y_label: y_b, self.dropout_ratio: 1.0})\n","        print(res1, res2)\n","        \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tcaly6JmNwiA","colab_type":"code","colab":{}},"source":["# tf_Model 모델의 파라미터 준비\n","\n","# 입력 데이터의 크기 지정\n","x_h = 224\n","x_w = 224\n","x_c = 3\n","\n","# 출력 갯수 지정\n","out_n = 5\n","\n","# 필터 크기 지정\n","f_s1 = 7\n","f_s2 = 5\n","f_s3 = 3\n","\n","# 컨볼루션 층의 필터 갯수 지정\n","f_n1 = 16\n","f_n2 = 32\n","f_n3 = 64\n","\n","# 출력 데이터 크기 지정\n","out_1 = x_h//2   # 첫 conv 계층 지난 후 크기\n","out_2 = out_1//2 # 두번째 conv 계층 지난 후 크기\n","out_3 = out_2//2 # 세번째 conv 계층 지난 후 크기\n","\n","# 완전연결 계층의 노드 수 지정\n","a_n1 = out_3*out_3*f_n3\n","a_n2 = 125\n","a_n3 = 2\n","\n","params = [x_h, x_w, x_c, out_n, f_s1, f_s2, f_s3, f_n1, f_n2, f_n3, a_n1, a_n2, a_n3]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A4SY6BOW291D","colab_type":"code","colab":{}},"source":["# 모델 만들기\n","model = tf_Model()\n","model.model_make(params)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LEW0LVuM3Hhs","colab_type":"code","colab":{}},"source":["# 모델 불러오기\n","model_path = 'drive/My Drive/model/'\n","model = tf_Model()\n","model.model_make(params)\n","model.model_load(model_path + 'awd.ckpt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EnqcLEEM72aO","colab_type":"code","colab":{}},"source":["'''\n","깃 허브의 오픈소스를 조금 수정한 것\n","Res Net - 50 : 동작이 검증된 이미지에 대한 CNN 모델\n","출처: https://github.com/KaihuaTang/ResNet50-Tensorflow-Face-Recognition/blob/master/ResNet.py\n","'''\n","\n","import tensorflow as tf\n","import math\n","\n","class Res_Model:\n","    # 클래스 초기화 함수\n","    def __init__(self):\n","        self.data_dict = None\n","\n","        self.var_dict = {}\n","        self.trainable = True\n","        self.open_tensorboard = False\n","        self.is_training = True\n","        # 세션 준비\n","        tf.reset_default_graph()\n","        self.sess = tf.Session()\n","        \n","    def res_block_3_layers(self, bottom, channel_list, name, change_dimension = False, block_stride = 1):\n","        \"\"\"\n","        bottom: input values (X)\n","        channel_list : number of channel in 3 layers\n","        name: block name\n","        \"\"\"\n","        if (change_dimension):\n","            short_cut_conv = self.conv_layer(bottom, 1, bottom.get_shape().as_list()[-1], channel_list[2], block_stride, name + \"_ShortcutConv\")\n","            block_conv_input = self.batch_norm(short_cut_conv)\n","        else:\n","            block_conv_input = bottom\n","\n","        block_conv_1 = self.conv_layer(bottom, 1, bottom.get_shape().as_list()[-1], channel_list[0], block_stride, name + \"_lovalConv1\")\n","        block_norm_1 = self.batch_norm(block_conv_1)\n","        block_relu_1 = tf.nn.relu(block_norm_1)\n","\n","        block_conv_2 = self.conv_layer(block_relu_1, 3, channel_list[0], channel_list[1], 1, name + \"_lovalConv2\")\n","        block_norm_2 = self.batch_norm(block_conv_2)\n","        block_relu_2 = tf.nn.relu(block_norm_2)\n","\n","        block_conv_3 = self.conv_layer(block_relu_2, 1, channel_list[1], channel_list[2], 1, name + \"_lovalConv3\")\n","        block_norm_3 = self.batch_norm(block_conv_3)\n","        block_res = tf.add(block_conv_input, block_norm_3)\n","        relu = tf.nn.relu(block_res)\n","\n","        return relu\n","\n","    # 배치 정규화\n","    def batch_norm(self, inputsTensor):\n","        \"\"\"\n","        Batchnorm\n","        \"\"\"\n","        _BATCH_NORM_DECAY = 0.99\n","        _BATCH_NORM_EPSILON = 1e-12\n","        return tf.layers.batch_normalization(inputs=inputsTensor, axis = 3, momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True, scale=True, training=self.is_training)\n","\n","    # 평균값 풀링\n","    def avg_pool(self, bottom, kernal_size = 2, stride = 2, name = \"avg\"):\n","        \"\"\"\n","        bottom: input values (X)\n","        kernal_size : n * n kernal\n","        stride : stride\n","        name : block_layer name\n","        \"\"\"\n","        return tf.nn.avg_pool(bottom, ksize=[1, kernal_size, kernal_size, 1], strides=[1, stride, stride, 1], padding='VALID', name=name)\n","\n","    # 최대값 풀링\n","    def max_pool(self, bottom, kernal_size = 2, stride = 2, name = \"max\"):\n","        \"\"\"\n","        bottom: input values (X)\n","        kernal_size : n * n kernal\n","        stride : stride\n","        name : block_layer name\n","        \"\"\"\n","        return tf.nn.max_pool(bottom, ksize=[1, kernal_size, kernal_size, 1], strides=[1, stride, stride, 1], padding='SAME', name=name)\n","\n","    # 컨볼루션계층\n","    def conv_layer(self, bottom, kernal_size, in_channels, out_channels, stride, name):\n","        \"\"\"\n","        bottom: input values (X)\n","        kernal_size : n * n kernal\n","        in_channels: number of input filters\n","        out_channels : number of output filters\n","        stride : stride\n","        name : block_layer name\n","        \"\"\"\n","        with tf.variable_scope(name):\n","            filt, conv_biases = self.get_conv_var(kernal_size, in_channels, out_channels, name)\n","\n","            conv = tf.nn.conv2d(bottom, filt, [1,stride,stride,1], padding='SAME')\n","            bias = tf.nn.bias_add(conv, conv_biases)\n","\n","            tf.summary.histogram('weight', filt)\n","            tf.summary.histogram('bias', conv_biases)\n","\n","            return bias\n","\n","    # 완전연결계층\n","    def fc_layer(self, bottom, in_size, out_size, name):\n","        \"\"\"\n","        bottom: input values (X)\n","        in_size : number of input feature size\n","        out_size : number of output feature size\n","        \"\"\"\n","        with tf.variable_scope(name):\n","            weights, biases = self.get_fc_var(in_size, out_size, name)\n","\n","            x = tf.reshape(bottom, [-1, in_size])\n","            fc = tf.nn.bias_add(tf.matmul(x, weights), biases)\n","\n","            tf.summary.histogram('weight', weights)\n","            tf.summary.histogram('bias', biases)\n","\n","            return fc\n","\n","    # 컨볼루션 계층에 필요한 변수 계산, 반환\n","    def get_conv_var(self, filter_size, in_channels, out_channels, name):\n","        \"\"\"\n","        filter_size : 3 * 3\n","        in_channels : number of input filters\n","        out_channels : number of output filters\n","        name : block_layer name\n","        \"\"\"\n","        initial_value = tf.truncated_normal([filter_size, filter_size, in_channels, out_channels], 0.0, stddev = 1 / math.sqrt(float(filter_size * filter_size)))\n","        filters = self.get_var(initial_value, name, 0, name + \"_filters\")\n","\n","        initial_value = tf.truncated_normal([out_channels], 0.0, 1.0)\n","        biases = self.get_var(initial_value, name, 1, name + \"_biases\")\n","\n","        return filters, biases\n","\n","    # 완전연결 계층에 필요한 변수 계산, 반환\n","    def get_fc_var(self, in_size, out_size, name):\n","        \"\"\"\n","        in_size : number of input feature size\n","        out_size : number of output feature size\n","        name : block_layer name\n","        \"\"\"\n","        initial_value = tf.truncated_normal([in_size, out_size], 0.0, stddev = 1 / math.sqrt(float(in_size)))\n","        weights = self.get_var(initial_value, name, 0, name + \"_weights\")\n","\n","        initial_value = tf.truncated_normal([out_size], 0.0, 1.0)\n","        biases = self.get_var(initial_value, name, 1, name + \"_biases\")\n","\n","        return weights, biases\n","\n","\n","    def get_var(self, initial_value, name, idx, var_name):\n","        \"\"\"\n","        load variables from Loaded model or new generated random variables\n","        initial_value : random initialized value\n","        name: block_layer name\n","        index: 0,1 weight or bias\n","        var_name: name + \"_filter\"/\"_bias\"\n","        \"\"\"\n","        if((name, idx) in self.var_dict):\n","            print(\"Reuse Parameters...\")\n","            print(self.var_dict[(name, idx)])\n","            return self.var_dict[(name, idx)]\n","\n","        if self.data_dict is not None and name in self.data_dict:\n","            value = self.data_dict[name][idx]\n","        else:\n","            value = initial_value\n","\n","        if self.trainable:\n","            var = tf.Variable(value, name=var_name)\n","        else:\n","            var = tf.constant(value, dtype=tf.float32, name=var_name)\n","\n","        self.var_dict[(name, idx)] = var\n","\n","        # print var_name, var.get_shape().as_list()\n","        assert var.get_shape() == initial_value.get_shape()\n","\n","        return var\n","        \n","    def model_make(self):\n","        \"\"\"\n","        load variable from npy to build the Resnet or Generate a new one\n","        :param rgb: rgb image [batch, height, width, 3] values scaled [0, 1]\n","        \"\"\"\n","        self.x = tf.placeholder(tf.float32, shape = [None, 224, 224, 3])\n","        self.y = tf.placeholder(tf.int64, shape = [None])\n","        \n","        # Preprocessing: Turning RGB to BGR - Mean.\n","        BGR_MEAN = [104.7546, 124.328, 167.1754]\n","        red, green, blue = tf.split(axis=3, num_or_size_splits=3, value=self.x)\n","        assert red.get_shape().as_list()[1:] == [224, 224, 1]\n","        assert green.get_shape().as_list()[1:] == [224, 224, 1]\n","        assert blue.get_shape().as_list()[1:] == [224, 224, 1]\n","        bgr = tf.concat(axis=3, values=[\n","            blue - BGR_MEAN[0],\n","            green - BGR_MEAN[1],\n","            red - BGR_MEAN[2],\n","        ])\n","        assert bgr.get_shape().as_list()[1:] == [224, 224, 3]\n","\n","        self.conv1 = self.conv_layer(bgr, 7, 3, 64, 2, \"conv1\")\n","        self.conv_norm_1 = self.batch_norm(self.conv1)\n","        self.conv1_relu = tf.nn.relu(self.conv_norm_1)\n","\n","        self.pool1 = self.max_pool(self.conv1_relu, 3, 2, \"pool1\")\n","        self.block1_1 = self.res_block_3_layers(self.pool1, [64, 64, 256], \"block1_1\", True)\n","        self.block1_2 = self.res_block_3_layers(self.block1_1, [64, 64, 256], \"block1_2\")\n","        self.block1_3 = self.res_block_3_layers(self.block1_2, [64, 64, 256], \"block1_3\")\n","\n","        self.block2_1 = self.res_block_3_layers(self.block1_3, [128, 128, 512], \"block2_1\", True, 2)\n","        self.block2_2 = self.res_block_3_layers(self.block2_1, [128, 128, 512], \"block2_2\")\n","        self.block2_3 = self.res_block_3_layers(self.block2_2, [128, 128, 512], \"block2_3\")\n","        self.block2_4 = self.res_block_3_layers(self.block2_3, [128, 128, 512], \"block2_4\")\n","\n","        self.block3_1 = self.res_block_3_layers(self.block2_4, [256, 256, 1024], \"block3_1\", True, 2)\n","        self.block3_2 = self.res_block_3_layers(self.block3_1, [256, 256, 1024], \"block3_2\")\n","        self.block3_3 = self.res_block_3_layers(self.block3_2, [256, 256, 1024], \"block3_3\")\n","        self.block3_4 = self.res_block_3_layers(self.block3_3, [256, 256, 1024], \"block3_4\")\n","        self.block3_5 = self.res_block_3_layers(self.block3_4, [256, 256, 1024], \"block3_5\")\n","        self.block3_6 = self.res_block_3_layers(self.block3_5, [256, 256, 1024], \"block3_6\")\n","\n","        self.block4_1 = self.res_block_3_layers(self.block3_6, [512, 512, 2048], \"block4_1\", True, 2)\n","        self.block4_2 = self.res_block_3_layers(self.block4_1, [512, 512, 2048], \"block4_2\")\n","        self.block4_3 = self.res_block_3_layers(self.block4_2, [512, 512, 2048], \"block4_3\")\n","\n","        self.pool2 = self.avg_pool(self.block4_3, 7, 1, \"pool2\")\n","\n","        self.fc1 = self.fc_layer(self.pool2, 2048, 2, \"fc2\")\n","        self.fc2 = self.fc_layer(self.fc1, 2, 5, \"fc5\")\n","\n","        self.loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.fc2, labels = self.y)\n","        self.cost = tf.reduce_sum(self.loss)\n","\n","        self.softmax = tf.nn.softmax(self.fc2)\n","        self.arg_m   = tf.argmax(self.softmax,1)\n","        self.correct_prediction = tf.equal(self.arg_m, self.y)\n","        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n","\n","        self.train = tf.train.MomentumOptimizer(1e-05, 0.9).minimize(self.cost)\n","\n","        self.prob = tf.nn.softmax(self.fc2, name=\"prob\")\n","                \n","        self.sess.run(tf.global_variables_initializer())\n","            \n","    def model_save(self, path):\n","        saver = tf.train.Saver()\n","        save_path = saver.save(self.sess, path)\n","        print(\"SavePath: %s\"%(save_path))\n","        \n","    def model_load(self, path):\n","        saver = tf.train.Saver()\n","        saver.restore(self.sess, path)\n","        \n","    def model_train(self, x_b, y_b):\n","        self.sess.run(self.train,feed_dict={self.x: x_b, self.y: y_b})\n","\n","    def get_accuracy(self, x_b, y_b):\n","        res = self.sess.run(self.accuracy,feed_dict={self.x: x_b, self.y: y_b})\n","        return res\n","    \n","    def get_xy(self, x_b):\n","        res = self.sess.run(self.fc1,feed_dict={self.x: x_b})\n","        res2 = self.sess.run(self.arg_m,feed_dict={self.x: x_b})\n","        \n","        return res, res2\n","    \n","    def print_inf(self, x_b):\n","        res = self.sess.run(self.fc1,feed_dict={self.x: x_b})\n","        print(res)\n","        res = self.sess.run(self.softmax,feed_dict={self.x: x_b})        \n","        print(res)\n","        res = self.sess.run(self.arg_m,feed_dict={self.x: x_b})\n","        print(res)\n","        print()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wUtsjln99rFc","colab_type":"code","colab":{}},"source":["# Res를 위한 모델 만들기\n","model = Res_Model()\n","model.model_make()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fXm3qSXT2HC","colab_type":"code","outputId":"d4bbc733-6d91-4190-ed74-91aa90c1416b","executionInfo":{"status":"ok","timestamp":1565531900725,"user_tz":-540,"elapsed":13191,"user":{"displayName":"서민성","photoUrl":"","userId":"06770200570856547998"}},"colab":{"base_uri":"https://localhost:8080/","height":163}},"source":["# Res 모델 불러오기\n","model_path = 'drive/My Drive/AWD/'\n","model = Res_Model()\n","model.model_make()\n","model.model_load(model_path + 'res.ckpt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0811 13:58:09.804135 140679071790976 deprecation.py:323] From <ipython-input-2-2d41771c0b9a>:55: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n","W0811 13:58:17.795596 140679071790976 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"gWxmOFy2II92","colab_type":"code","outputId":"2505acca-cb3b-47d7-ce5a-51e191bf15de","executionInfo":{"status":"ok","timestamp":1561320746271,"user_tz":-540,"elapsed":1349,"user":{"displayName":"서민성","photoUrl":"","userId":"06770200570856547998"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["# tf_Model 모델 학습을 위한 준비\n","import numpy as np\n","import cv2\n","\n","\n","# 라벨 데이터 준비 - 100개, ont_hot 인코딩\n","\n","t_list = np.zeros((100,5))\n","\n","val = 0\n","cnt = 0\n","for i in range(100):\n","    t_list[i][val] = 1\n","    \n","    cnt+=1\n","    if cnt == 20:\n","        cnt=0\n","        val+=1\n","\n","print(t_list.shape)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(100, 5)\n","(100, 224, 224, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T08huUhz-12B","colab_type":"code","outputId":"de24179c-bc2b-4a8a-b36b-dc73656e51bb","executionInfo":{"status":"ok","timestamp":1565545134930,"user_tz":-540,"elapsed":746,"user":{"displayName":"서민성","photoUrl":"","userId":"06770200570856547998"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["# Res 모델 학습을 위한 라벨 준비(전체 데이터)\n","import numpy as np\n","import pickle\n","\n","cls_path = 'drive/My Drive/clusters.pickle'\n","\n","data = None\n","\n","with open(cls_path, 'rb') as f:\n","    data = pickle.load(f)\n","    \n","t_list = np.zeros((2323))\n","\n","for key in data.keys():\n","    for name in data[key]:\n","        t_list[name - 1] = key\n","\n","t_list = t_list.astype(np.int32)\n","print(\"numpy dtype: \", t_list.dtype)\n","\n","print(\"t_list shape: \", t_list.shape)\n","print(t_list)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["numpy dtype:  int32\n","t_list shape:  (2323,)\n","[0 3 4 ... 2 3 4]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4V7W8vk4-f49","colab_type":"code","outputId":"a69ac6f2-82d9-416e-dbc1-74f6c90abf0e","executionInfo":{"status":"ok","timestamp":1565565760268,"user_tz":-540,"elapsed":7502,"user":{"displayName":"서민성","photoUrl":"","userId":"06770200570856547998"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# 전체 이미지 데이터 준비\n","import cv2\n","import numpy as np\n","\n","img_path = 'drive/My Drive/new_gray_png/'\n","\n","images = []\n","for i in range(2323):\n","    full_path = img_path + str(i + 1) + '.png'\n","    img = cv2.imread(full_path, cv2.IMREAD_UNCHANGED)\n","    images.append(img)\n","\n","images = np.array(images)\n","\n","print(images.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(2323, 224, 224, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E6Zc8a9vVmwS","colab_type":"code","outputId":"d7690ae9-42fc-4153-f1c5-253b6a6cf998","executionInfo":{"status":"ok","timestamp":1565559158924,"user_tz":-540,"elapsed":1493,"user":{"displayName":"서민성","photoUrl":"","userId":"06770200570856547998"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["# 훈련 이미지 데이터 준비\n","import cv2\n","import numpy as np\n","import pickle\n","\n","# 분류된 훈련할 데이터 집합 가져오기(사전 자료구조)\n","T_path = 'drive/My Drive/train.pickle'\n","\n","data = None\n","with open(T_path, 'rb') as f:\n","    data = pickle.load(f)\n","\n","img_path = 'drive/My Drive/new_gray_png/'\n","\n","images = []\n","\n","for k in data.keys():\n","    for name in data[k]:\n","        f_p = img_path + str(name) + '.png'\n","        img = cv2.imread(f_p, cv2.IMREAD_UNCHANGED)\n","        images.append(img)\n","\n","images = np.array(images)\n","\n","print(\"Images shape\", images.shape)\n","\n","# 5개의 클래스당 20개씩의 훈련데이터 = 100개\n","t_list = np.zeros((100))\n","\n","cnt = 0\n","val = 0\n","for i in range(100):\n","    t_list[i] = val\n","    \n","    cnt += 1\n","    if cnt == 20:\n","        cnt = 0\n","        val += 1\n","\n","t_list = t_list.astype(np.int32)\n","print(\"numpy dtype: \", t_list.dtype)\n","print(\"t_list shape: \", t_list.shape)\n","print(t_list)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Images shape (100, 224, 224, 3)\n","numpy dtype:  int32\n","t_list shape:  (100,)\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aYPQ6rVo9__F","colab_type":"code","outputId":"6df42641-463d-4964-f49b-f2ad2c5aab98","executionInfo":{"status":"ok","timestamp":1565565259223,"user_tz":-540,"elapsed":1171049,"user":{"displayName":"서민성","photoUrl":"","userId":"06770200570856547998"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Res 모델을 위한 훈련시키기\n","\n","print_cnt = 10\n","repeat_cnt = 5000\n","\n","data_size = 100\n","batch_size = 20\n","\n","b_l = np.random.choice(data_size, data_size, replace=False)\n","\n","for i in range(repeat_cnt + 1):\n","    \n","    if len(b_l) > batch_size:\n","        data_mask = b_l[:batch_size]\n","        b_l = b_l[batch_size:]\n","        \n","    else:\n","        t_l = b_l\n","        b_l = np.random.choice(data_size, data_size, replace=False)\n","\n","        b_l = np.concatenate((t_l,b_l), axis=None)\n","\n","        data_mask = b_l[:batch_size]\n","        b_l = b_l[batch_size:]\n","    \n","    x_b = images[data_mask]\n","    y_b = t_list[data_mask]\n","    \n","    \n","    if i % print_cnt == 0:\n","        \n","        train_accuracy = model.get_accuracy(x_b, y_b)\n","        print(\"step %d, training accuracy %.4f\"%(i, train_accuracy))\n","        \n","        \n","        xy, am = model.get_xy(x_b)\n","        print(\"pred: \", am)\n","        print(\"ans : \", y_b)\n","        \n","        \n","    \n","    model.model_train(x_b, y_b)\n","\n","print(\"End Of Program-Train\")\n","\n","# 모델을 저장\n","model_path = 'drive/My Drive/AWD/'\n","model.model_save(model_path + 'res.ckpt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["step 0, training accuracy 0.7500\n","pred:  [1 2 1 2 3 4 0 2 4 4 0 3 4 3 1 3 2 1 2 1]\n","ans :  [1 2 1 0 0 3 0 2 4 4 0 0 4 3 1 0 2 1 2 1]\n","step 10, training accuracy 0.9000\n","pred:  [2 3 1 4 1 1 3 0 2 0 3 2 4 3 4 0 1 2 4 3]\n","ans :  [4 3 1 4 1 1 3 3 2 0 3 2 4 3 4 0 1 2 4 3]\n","step 20, training accuracy 0.8000\n","pred:  [3 1 4 2 0 4 2 1 2 4 4 1 3 4 3 3 2 4 0 1]\n","ans :  [3 2 4 4 0 4 2 1 2 4 4 0 4 4 3 3 2 4 0 1]\n","step 30, training accuracy 0.5500\n","pred:  [3 4 0 3 4 0 2 4 1 2 0 3 4 3 1 2 1 0 0 2]\n","ans :  [0 4 0 0 2 0 2 2 1 1 2 3 3 3 1 3 1 0 0 4]\n","step 40, training accuracy 0.8500\n","pred:  [1 0 1 4 4 4 0 4 1 1 3 4 4 2 2 3 3 4 0 1]\n","ans :  [1 0 2 4 4 4 0 4 1 1 3 2 4 2 2 3 3 4 3 1]\n","step 50, training accuracy 0.9000\n","pred:  [3 1 1 4 4 3 1 3 2 4 1 2 4 4 0 0 3 4 0 0]\n","ans :  [2 1 1 4 4 3 1 3 1 4 1 2 4 4 0 0 3 4 0 0]\n","step 60, training accuracy 0.7000\n","pred:  [1 4 1 1 1 1 4 0 3 0 4 4 1 3 1 4 0 3 4 0]\n","ans :  [1 2 1 1 1 1 4 0 2 0 4 2 1 3 1 4 3 0 2 0]\n","step 70, training accuracy 0.9500\n","pred:  [2 1 4 1 1 1 4 0 3 1 4 3 0 3 1 1 3 4 4 0]\n","ans :  [2 1 4 1 1 1 4 3 3 1 4 3 0 3 1 1 3 4 4 0]\n","step 80, training accuracy 0.8500\n","pred:  [4 3 4 3 1 2 3 4 3 1 0 1 0 3 3 0 1 4 3 2]\n","ans :  [4 0 3 3 1 2 3 4 0 1 0 1 0 3 3 0 1 4 3 2]\n","step 90, training accuracy 0.7500\n","pred:  [2 3 1 2 4 4 2 4 3 4 4 0 0 0 0 2 1 4 0 2]\n","ans :  [4 3 1 2 4 4 4 4 3 4 4 0 0 0 0 4 2 3 0 2]\n","step 100, training accuracy 0.9500\n","pred:  [2 4 3 4 2 1 4 1 4 3 0 3 0 3 3 1 0 4 3 2]\n","ans :  [2 4 3 4 2 1 4 1 4 3 0 0 0 3 3 1 0 4 3 2]\n","step 110, training accuracy 0.7500\n","pred:  [1 2 0 4 3 2 4 1 4 2 1 2 3 3 0 3 2 1 3 1]\n","ans :  [1 2 3 2 3 2 4 2 4 2 1 2 3 3 3 4 2 1 3 1]\n","step 120, training accuracy 0.7500\n","pred:  [4 4 2 3 4 0 2 2 1 4 0 1 2 4 1 4 4 0 0 1]\n","ans :  [4 4 2 3 4 0 2 4 1 3 0 1 2 2 1 4 4 0 3 3]\n","step 130, training accuracy 0.7500\n","pred:  [3 1 3 4 2 1 0 1 0 1 4 1 3 4 0 3 0 2 4 3]\n","ans :  [3 1 3 2 3 1 0 0 0 1 4 1 3 4 3 3 0 4 4 3]\n","step 140, training accuracy 0.7500\n","pred:  [3 3 4 0 1 3 2 0 4 4 4 2 2 0 3 2 2 4 1 2]\n","ans :  [4 3 2 2 1 4 2 0 4 4 4 2 2 3 3 2 2 4 1 2]\n","step 150, training accuracy 0.8000\n","pred:  [1 2 1 2 3 1 1 0 4 2 2 3 4 0 2 4 0 2 0 0]\n","ans :  [2 4 1 4 3 1 1 0 4 2 2 3 4 0 2 4 0 2 3 0]\n","step 160, training accuracy 0.7000\n","pred:  [1 3 4 0 0 3 2 0 2 1 0 2 4 2 0 1 4 1 3 4]\n","ans :  [1 3 4 3 3 3 2 0 2 1 0 2 4 2 3 3 2 1 0 4]\n","step 170, training accuracy 0.8500\n","pred:  [1 4 4 3 1 0 2 0 4 1 1 1 1 0 0 4 1 2 3 2]\n","ans :  [1 2 4 3 1 0 2 0 2 1 1 1 1 3 0 4 1 2 3 2]\n","step 180, training accuracy 0.7500\n","pred:  [3 2 0 4 2 2 1 4 3 3 0 4 0 0 1 4 4 1 1 4]\n","ans :  [3 4 0 4 2 2 0 4 3 2 0 4 0 0 1 4 2 1 1 2]\n","step 190, training accuracy 0.8000\n","pred:  [3 0 0 0 1 2 3 3 4 3 0 2 2 4 1 2 2 4 1 3]\n","ans :  [3 0 0 3 1 2 3 3 2 3 0 3 2 4 1 2 2 3 1 3]\n","step 200, training accuracy 0.7500\n","pred:  [4 4 1 1 0 4 3 3 3 0 4 4 0 0 1 4 1 2 4 4]\n","ans :  [4 4 2 1 0 4 3 3 3 0 3 3 0 0 1 3 1 2 4 2]\n","step 210, training accuracy 0.9500\n","pred:  [0 1 1 2 3 3 1 2 0 4 2 4 1 1 3 2 3 4 3 2]\n","ans :  [0 1 1 2 3 3 1 2 0 4 2 2 1 1 3 2 3 4 3 2]\n","step 220, training accuracy 0.9000\n","pred:  [1 2 2 0 2 4 3 4 4 0 3 0 4 2 4 1 0 2 1 2]\n","ans :  [1 2 2 0 2 4 3 4 4 0 3 3 4 2 4 1 0 4 1 2]\n","step 230, training accuracy 0.8500\n","pred:  [2 1 3 3 3 3 2 0 0 1 4 1 4 1 3 3 0 3 2 2]\n","ans :  [4 1 3 3 3 3 2 0 0 0 4 1 4 1 3 3 0 4 2 2]\n","step 240, training accuracy 0.8500\n","pred:  [1 4 1 2 4 0 1 4 1 1 3 3 1 1 0 4 3 0 0 2]\n","ans :  [1 3 1 2 3 0 1 4 1 1 3 0 1 1 0 4 3 0 0 2]\n","step 250, training accuracy 0.8500\n","pred:  [2 3 4 1 1 4 2 0 2 4 3 2 0 2 4 2 3 2 3 0]\n","ans :  [2 3 4 1 1 4 2 0 2 4 3 2 0 2 3 2 0 4 3 0]\n","step 260, training accuracy 0.7000\n","pred:  [3 4 4 3 4 3 1 4 1 3 1 0 1 4 3 4 1 4 2 2]\n","ans :  [3 4 4 2 4 3 1 4 1 0 1 0 1 4 3 3 1 3 4 4]\n","step 270, training accuracy 0.9000\n","pred:  [2 3 2 0 2 4 1 0 2 3 0 2 3 4 0 3 1 3 4 4]\n","ans :  [4 3 2 0 2 2 1 0 2 3 0 2 3 4 0 3 1 3 4 4]\n","step 280, training accuracy 0.8500\n","pred:  [1 2 1 2 1 2 1 1 2 4 0 0 2 2 3 3 2 0 4 3]\n","ans :  [1 2 1 2 1 2 1 1 3 4 0 0 4 2 3 3 2 0 4 4]\n","step 290, training accuracy 0.9000\n","pred:  [0 0 3 4 2 0 0 0 0 3 1 3 1 0 4 4 1 3 3 1]\n","ans :  [0 0 3 4 2 0 0 0 0 4 1 3 1 0 4 4 1 3 0 1]\n","step 300, training accuracy 0.8500\n","pred:  [2 3 4 4 3 0 4 1 4 3 2 2 3 0 2 3 0 1 2 4]\n","ans :  [2 3 4 4 0 0 4 1 3 3 2 2 3 0 2 3 0 1 4 4]\n","step 310, training accuracy 0.8000\n","pred:  [4 3 0 2 3 2 3 4 2 1 2 3 0 3 4 2 0 4 0 4]\n","ans :  [4 3 0 4 3 2 0 4 2 1 4 3 0 0 4 2 0 4 0 4]\n","step 320, training accuracy 0.8500\n","pred:  [4 4 4 1 2 4 2 3 0 1 2 1 0 1 0 0 3 3 0 2]\n","ans :  [4 4 4 1 2 4 2 3 0 1 4 1 0 1 3 0 3 0 0 2]\n","step 330, training accuracy 0.8000\n","pred:  [2 2 0 3 0 4 0 2 3 0 0 1 4 3 3 1 3 0 4 2]\n","ans :  [2 2 3 0 0 4 0 3 3 0 0 1 3 3 3 1 3 0 4 2]\n","step 340, training accuracy 0.8500\n","pred:  [3 1 3 3 3 0 0 4 1 1 1 4 2 3 0 1 2 2 3 3]\n","ans :  [3 1 0 3 3 0 0 4 1 1 1 2 2 3 0 1 2 2 3 0]\n","step 350, training accuracy 0.9000\n","pred:  [2 1 4 2 3 1 1 3 0 0 2 0 2 3 4 3 1 4 2 1]\n","ans :  [2 1 2 2 3 1 1 3 3 0 2 0 2 3 4 3 1 4 2 1]\n","step 360, training accuracy 0.9000\n","pred:  [4 3 3 1 1 0 2 1 3 4 1 0 0 1 3 3 2 1 0 3]\n","ans :  [4 3 3 1 1 0 2 1 3 2 1 0 0 1 3 3 2 1 0 0]\n","step 370, training accuracy 0.6500\n","pred:  [3 0 1 1 0 3 2 1 0 2 4 3 2 4 0 0 4 0 3 4]\n","ans :  [0 0 1 1 0 0 4 1 0 2 4 3 2 3 0 3 4 0 4 3]\n","step 380, training accuracy 0.7500\n","pred:  [3 2 2 2 4 4 3 0 1 1 0 3 2 1 2 0 4 3 2 4]\n","ans :  [3 2 2 2 4 2 0 0 1 1 0 3 2 2 2 0 3 3 2 2]\n","step 390, training accuracy 0.7500\n","pred:  [1 4 3 4 4 3 0 4 1 1 2 0 3 0 0 1 0 1 2 2]\n","ans :  [1 2 4 4 4 3 0 4 1 1 2 0 4 0 0 1 3 1 1 2]\n","step 400, training accuracy 0.7000\n","pred:  [1 0 4 1 2 3 1 4 3 4 0 2 3 1 1 4 1 0 2 3]\n","ans :  [1 0 4 1 1 0 1 2 3 2 0 2 3 1 1 3 1 0 1 3]\n","step 410, training accuracy 0.9000\n","pred:  [3 1 4 2 1 0 1 3 3 4 4 4 0 4 1 0 1 0 0 2]\n","ans :  [3 1 4 2 1 0 1 3 3 4 4 4 0 2 1 0 1 0 3 2]\n","step 420, training accuracy 0.8500\n","pred:  [1 4 1 4 1 1 3 3 3 2 3 1 0 2 4 2 1 3 4 3]\n","ans :  [1 4 1 4 1 1 3 3 0 2 3 1 0 2 4 1 1 3 4 0]\n","step 430, training accuracy 0.8000\n","pred:  [2 1 3 2 4 2 1 4 2 0 3 2 3 0 3 0 3 4 1 4]\n","ans :  [2 1 3 2 4 2 1 4 2 0 3 2 3 0 0 0 4 3 1 2]\n","step 440, training accuracy 0.9000\n","pred:  [4 4 1 2 2 4 2 4 2 0 3 0 1 0 3 2 2 3 3 1]\n","ans :  [2 4 1 2 2 4 2 4 4 0 3 0 1 0 3 2 2 3 3 1]\n","step 450, training accuracy 0.7500\n","pred:  [0 2 1 0 0 1 3 3 4 0 1 1 4 3 4 1 4 1 1 1]\n","ans :  [0 2 1 3 3 1 4 3 4 3 1 1 3 3 4 1 4 1 1 1]\n","step 460, training accuracy 0.9000\n","pred:  [2 0 1 0 3 2 0 2 3 0 3 1 3 1 4 3 4 2 2 2]\n","ans :  [2 0 1 0 3 2 0 2 3 0 3 1 3 1 2 3 4 2 2 1]\n","step 470, training accuracy 0.8500\n","pred:  [4 4 2 2 4 0 4 2 4 4 1 0 3 0 0 2 3 3 4 1]\n","ans :  [4 4 2 2 4 0 2 4 4 4 1 0 3 0 0 2 3 0 4 1]\n","step 480, training accuracy 0.9000\n","pred:  [1 4 1 1 3 0 3 1 0 2 4 2 1 2 4 2 4 1 3 2]\n","ans :  [1 4 1 1 3 0 3 1 0 2 4 2 1 2 2 2 4 1 2 2]\n","step 490, training accuracy 0.9000\n","pred:  [0 4 2 0 3 3 1 2 0 2 0 3 4 3 4 2 1 2 1 3]\n","ans :  [0 4 2 0 3 3 1 2 3 2 0 3 4 3 2 2 1 2 1 3]\n","step 500, training accuracy 0.6500\n","pred:  [0 4 4 0 2 2 3 2 0 2 1 2 4 3 3 2 1 3 3 2]\n","ans :  [0 4 3 0 2 4 4 2 3 2 1 4 3 3 3 2 2 3 3 2]\n","step 510, training accuracy 0.8000\n","pred:  [1 0 4 1 2 2 3 2 4 3 0 3 2 3 0 2 0 3 4 1]\n","ans :  [1 0 4 1 2 2 3 2 4 0 0 0 4 3 0 2 0 0 4 1]\n","step 520, training accuracy 0.8500\n","pred:  [3 4 4 4 3 4 0 3 4 2 1 0 0 2 2 0 4 1 1 0]\n","ans :  [4 4 2 4 0 4 0 3 4 2 1 0 0 2 2 0 4 1 1 0]\n","step 530, training accuracy 0.9500\n","pred:  [3 0 0 3 1 1 3 3 2 2 1 0 2 4 3 2 0 4 3 2]\n","ans :  [3 0 0 3 1 1 3 0 2 2 1 0 2 4 3 2 0 4 3 2]\n","step 540, training accuracy 0.8000\n","pred:  [4 3 2 1 1 4 3 4 0 4 0 1 1 2 2 2 4 4 1 3]\n","ans :  [4 3 2 1 1 4 0 3 2 4 0 1 1 2 2 2 2 4 1 3]\n","step 550, training accuracy 0.8000\n","pred:  [4 2 1 3 0 1 3 1 1 3 4 0 0 1 1 2 2 3 4 2]\n","ans :  [4 2 1 0 0 1 3 1 1 3 4 0 1 1 1 2 2 4 2 2]\n","step 560, training accuracy 0.8000\n","pred:  [3 0 4 2 4 3 1 4 3 1 4 4 1 3 2 2 4 0 1 2]\n","ans :  [3 3 4 2 4 0 1 4 3 1 4 4 1 3 2 4 3 0 1 2]\n","step 570, training accuracy 0.8000\n","pred:  [4 4 4 0 0 0 2 2 0 4 1 2 3 1 1 0 0 4 4 2]\n","ans :  [4 4 3 0 0 3 2 4 0 4 1 2 3 1 1 0 0 4 4 3]\n","step 580, training accuracy 0.9000\n","pred:  [3 3 4 3 3 3 4 0 2 0 0 2 4 4 1 1 0 1 1 0]\n","ans :  [3 0 4 3 3 3 4 0 2 0 0 2 3 4 1 1 0 1 1 0]\n","step 590, training accuracy 0.9000\n","pred:  [3 1 2 1 4 2 2 2 3 1 1 0 3 2 1 4 3 3 3 1]\n","ans :  [3 1 2 1 4 2 2 2 3 1 1 0 3 2 1 4 0 3 4 1]\n","step 600, training accuracy 0.7500\n","pred:  [1 1 0 1 3 1 0 2 2 4 3 1 1 4 0 3 3 3 0 3]\n","ans :  [1 1 0 1 0 1 0 1 2 2 3 1 1 4 0 3 3 1 0 2]\n","step 610, training accuracy 0.8500\n","pred:  [4 4 3 2 3 1 1 0 4 4 4 1 2 2 0 1 0 0 3 4]\n","ans :  [4 3 3 2 3 1 1 0 4 4 4 1 2 2 0 1 0 0 0 3]\n","step 620, training accuracy 0.9500\n","pred:  [4 4 3 1 1 2 4 3 4 0 1 0 0 4 1 3 2 3 2 0]\n","ans :  [2 4 3 1 1 2 4 3 4 0 1 0 0 4 1 3 2 3 2 0]\n","step 630, training accuracy 0.8500\n","pred:  [0 4 2 3 4 4 1 3 2 4 1 4 1 1 3 0 1 4 4 1]\n","ans :  [0 4 2 3 4 4 1 0 2 4 1 4 1 1 0 3 1 4 4 1]\n","step 640, training accuracy 0.9000\n","pred:  [0 2 1 4 3 1 1 4 4 3 2 1 3 2 0 4 1 1 3 0]\n","ans :  [0 2 1 2 4 1 1 4 4 3 2 1 3 2 0 4 1 1 3 0]\n","step 650, training accuracy 0.8500\n","pred:  [3 4 4 1 3 0 4 1 0 0 2 2 1 2 2 1 4 1 3 4]\n","ans :  [2 2 4 1 3 0 4 1 0 0 2 2 1 2 2 1 4 1 3 2]\n","step 660, training accuracy 0.9000\n","pred:  [4 1 3 3 1 1 2 1 3 3 2 2 0 0 3 2 3 1 0 1]\n","ans :  [4 1 3 3 1 1 2 1 3 3 2 2 0 3 3 2 4 1 0 1]\n","step 670, training accuracy 0.8500\n","pred:  [3 2 2 1 1 4 0 0 1 3 1 1 3 4 4 3 4 1 1 2]\n","ans :  [3 2 2 1 1 4 0 0 1 3 1 1 3 2 3 3 3 1 1 2]\n","step 680, training accuracy 0.8500\n","pred:  [2 4 0 4 2 1 4 4 4 0 2 1 0 4 0 1 0 4 3 1]\n","ans :  [2 4 0 4 2 1 2 4 4 0 3 1 0 3 0 1 0 4 3 1]\n","step 690, training accuracy 1.0000\n","pred:  [3 3 2 4 2 1 4 0 1 0 1 2 1 1 4 0 0 1 3 4]\n","ans :  [3 3 2 4 2 1 4 0 1 0 1 2 1 1 4 0 0 1 3 4]\n","step 700, training accuracy 0.8000\n","pred:  [1 3 2 1 4 2 1 2 0 0 4 0 2 4 1 2 4 1 2 3]\n","ans :  [1 3 2 1 4 1 1 2 0 0 4 0 2 2 1 2 2 1 4 3]\n","step 710, training accuracy 0.6000\n","pred:  [3 1 0 4 2 4 3 3 3 3 1 3 1 3 4 3 1 1 0 0]\n","ans :  [0 1 0 4 2 2 0 3 3 3 1 4 1 3 2 2 1 1 3 1]\n","step 720, training accuracy 0.6000\n","pred:  [3 4 0 1 4 1 3 4 0 2 2 0 4 0 1 2 4 3 0 4]\n","ans :  [0 4 3 1 3 1 3 4 0 2 2 3 4 0 1 4 3 0 0 2]\n","step 730, training accuracy 0.9000\n","pred:  [1 4 4 0 2 0 1 2 1 0 3 4 3 2 2 3 2 2 0 1]\n","ans :  [3 4 4 0 2 0 1 2 1 0 3 4 3 2 4 3 2 2 0 1]\n","step 740, training accuracy 0.7500\n","pred:  [2 0 4 3 0 4 0 4 3 3 3 3 1 3 4 2 4 1 1 1]\n","ans :  [4 0 3 3 0 2 0 4 3 3 3 3 1 0 2 2 4 1 1 1]\n","step 750, training accuracy 1.0000\n","pred:  [4 0 1 4 1 4 0 4 1 1 0 4 4 1 0 3 3 3 3 2]\n","ans :  [4 0 1 4 1 4 0 4 1 1 0 4 4 1 0 3 3 3 3 2]\n","step 760, training accuracy 0.8500\n","pred:  [2 1 2 2 0 1 2 3 3 3 1 4 3 3 0 3 1 1 1 4]\n","ans :  [2 1 2 2 0 1 2 3 0 3 1 4 3 3 0 4 1 1 1 2]\n","step 770, training accuracy 0.9000\n","pred:  [3 2 0 4 3 3 4 0 2 0 4 3 2 0 1 1 2 4 4 1]\n","ans :  [3 2 0 4 3 0 4 0 2 0 4 3 2 0 1 1 2 4 3 1]\n","step 780, training accuracy 0.9000\n","pred:  [0 4 4 1 0 2 0 4 3 2 4 0 1 2 3 2 4 0 3 0]\n","ans :  [0 4 4 1 3 2 0 4 3 2 2 0 1 2 3 2 4 0 3 0]\n","step 790, training accuracy 0.6500\n","pred:  [2 3 1 4 2 0 4 0 1 1 4 0 1 2 3 3 0 4 1 3]\n","ans :  [2 4 1 4 2 2 4 3 1 2 4 0 1 2 3 2 0 2 1 2]\n","step 800, training accuracy 0.8500\n","pred:  [3 2 4 1 4 0 4 2 0 0 1 4 1 2 2 3 0 2 2 2]\n","ans :  [0 2 4 1 4 0 4 2 0 0 1 4 1 2 2 3 0 2 4 4]\n","step 810, training accuracy 0.8500\n","pred:  [4 2 4 2 1 1 0 4 0 3 0 3 3 3 2 1 0 4 0 1]\n","ans :  [4 2 4 2 1 1 0 4 0 3 0 3 0 2 3 1 0 4 0 1]\n","step 820, training accuracy 0.7500\n","pred:  [3 1 0 0 4 1 4 3 4 2 0 2 1 1 4 3 0 3 4 4]\n","ans :  [3 2 0 0 4 2 4 3 3 4 0 2 3 1 4 3 0 3 4 4]\n","step 830, training accuracy 0.6500\n","pred:  [1 4 2 1 4 3 1 3 2 3 1 2 3 4 0 2 3 0 2 0]\n","ans :  [1 2 2 1 4 3 1 4 2 0 1 4 0 2 0 2 4 0 2 0]\n","step 840, training accuracy 0.9000\n","pred:  [2 3 0 2 2 4 0 1 2 4 0 1 1 2 2 3 0 3 2 3]\n","ans :  [2 3 0 2 4 4 0 1 2 4 3 1 1 2 2 3 0 3 2 3]\n","step 850, training accuracy 0.8500\n","pred:  [3 4 0 4 1 1 2 2 1 0 3 2 3 3 2 2 4 3 1 4]\n","ans :  [3 4 0 3 1 1 2 2 1 0 3 4 3 3 2 2 4 3 1 3]\n","step 860, training accuracy 0.8000\n","pred:  [3 1 2 4 1 1 0 4 3 3 3 1 0 4 3 1 4 4 0 4]\n","ans :  [3 1 2 4 1 1 3 4 3 4 3 1 0 3 3 1 3 4 0 4]\n","step 870, training accuracy 0.6500\n","pred:  [4 3 2 4 4 1 2 0 0 2 1 2 3 1 2 3 2 1 0 2]\n","ans :  [3 3 4 2 4 1 2 0 0 2 1 2 3 3 4 3 2 1 3 4]\n","step 880, training accuracy 0.9000\n","pred:  [3 0 2 4 2 0 2 2 1 1 1 2 4 3 0 0 1 3 3 1]\n","ans :  [3 0 2 4 2 0 2 2 1 1 1 2 4 3 0 0 1 0 0 1]\n","step 890, training accuracy 0.9000\n","pred:  [1 0 0 0 0 1 1 3 3 2 3 4 1 2 3 2 3 4 1 1]\n","ans :  [1 0 0 0 0 1 1 3 3 4 3 4 1 2 3 2 3 2 1 1]\n","step 900, training accuracy 0.7000\n","pred:  [0 1 2 4 3 0 2 1 4 1 4 1 0 1 3 4 1 2 4 1]\n","ans :  [0 1 2 4 3 3 2 1 2 1 2 1 3 1 4 4 1 2 2 1]\n","step 910, training accuracy 0.9000\n","pred:  [2 3 0 1 4 0 0 4 1 0 1 3 2 3 2 2 0 0 0 0]\n","ans :  [2 3 0 1 4 0 0 2 1 0 1 3 2 3 2 2 0 0 3 0]\n","step 920, training accuracy 0.6500\n","pred:  [4 3 4 2 0 4 1 3 0 2 1 3 2 0 3 4 4 1 4 2]\n","ans :  [2 3 4 2 0 4 1 4 1 3 1 4 2 3 3 4 4 1 2 2]\n","step 930, training accuracy 0.7500\n","pred:  [1 0 4 3 2 1 1 4 0 3 3 1 3 1 0 0 4 3 3 4]\n","ans :  [1 3 4 3 4 1 1 4 0 4 0 2 3 1 0 0 4 3 3 4]\n","step 940, training accuracy 0.8000\n","pred:  [4 1 4 4 2 0 3 3 4 1 1 0 0 1 2 2 1 3 2 3]\n","ans :  [3 1 4 4 2 0 3 3 3 1 1 0 3 1 4 2 1 3 2 3]\n","step 950, training accuracy 0.8500\n","pred:  [1 1 2 3 4 2 1 2 0 3 3 2 0 3 3 3 1 0 3 1]\n","ans :  [1 1 2 3 2 2 1 2 0 3 3 2 0 3 0 2 1 0 3 1]\n","step 960, training accuracy 0.8000\n","pred:  [2 3 1 3 1 4 2 4 0 0 0 3 4 0 2 2 1 0 2 3]\n","ans :  [2 3 2 3 2 4 2 4 0 3 0 3 4 0 2 2 0 0 2 3]\n","step 970, training accuracy 0.9000\n","pred:  [2 4 4 1 3 2 0 2 0 0 1 4 2 2 4 4 3 1 0 1]\n","ans :  [2 4 4 1 3 2 0 2 0 0 1 4 2 2 2 2 3 1 0 1]\n","step 980, training accuracy 0.7000\n","pred:  [0 2 2 3 1 0 2 4 4 2 2 0 3 1 4 2 0 3 3 1]\n","ans :  [0 4 2 3 1 0 4 4 3 2 4 3 0 1 4 2 0 3 3 1]\n","step 990, training accuracy 0.8000\n","pred:  [0 3 2 1 1 2 4 0 1 3 2 1 1 3 2 3 4 4 3 2]\n","ans :  [0 4 2 1 1 2 4 0 1 3 2 1 1 4 2 4 4 4 2 2]\n","step 1000, training accuracy 0.9500\n","pred:  [4 2 0 4 0 3 1 1 2 0 1 2 3 0 1 0 1 2 4 4]\n","ans :  [4 2 0 4 0 3 1 1 2 0 1 2 3 0 1 0 1 2 2 4]\n","step 1010, training accuracy 0.7500\n","pred:  [2 4 1 3 3 2 4 1 4 1 0 3 4 1 2 0 2 1 2 3]\n","ans :  [4 4 1 3 3 2 2 1 4 2 0 3 4 2 2 3 2 1 2 3]\n","step 1020, training accuracy 0.8000\n","pred:  [0 3 3 3 3 0 2 0 4 3 2 1 2 4 3 3 1 4 0 1]\n","ans :  [0 0 3 3 0 0 2 0 4 0 2 1 2 4 3 3 1 3 0 1]\n","step 1030, training accuracy 0.8500\n","pred:  [2 1 0 2 2 0 2 2 0 0 3 2 1 4 3 2 4 3 4 1]\n","ans :  [2 1 0 4 2 3 2 2 0 0 3 2 1 4 3 2 4 3 2 1]\n","step 1040, training accuracy 0.8500\n","pred:  [4 1 2 4 4 2 2 0 3 0 1 0 3 2 2 2 0 3 1 2]\n","ans :  [3 1 2 4 4 2 2 0 3 0 1 0 3 4 2 2 1 3 1 2]\n","step 1050, training accuracy 0.9000\n","pred:  [4 2 1 0 1 3 2 2 1 2 3 1 2 3 4 0 0 4 0 2]\n","ans :  [4 2 1 0 1 3 2 2 1 4 0 1 2 3 4 0 0 4 0 2]\n","step 1060, training accuracy 0.8500\n","pred:  [2 1 4 0 1 0 2 2 3 1 3 4 3 4 2 1 4 4 3 3]\n","ans :  [2 1 4 0 1 0 2 2 0 1 3 4 3 2 4 1 4 4 3 3]\n","step 1070, training accuracy 0.9500\n","pred:  [2 4 1 3 1 3 3 3 0 3 0 3 0 0 0 3 2 2 4 4]\n","ans :  [2 4 1 3 1 3 3 0 0 3 0 3 0 0 0 3 2 2 4 4]\n","step 1080, training accuracy 0.9000\n","pred:  [4 0 1 1 3 1 1 4 3 3 0 4 1 4 0 2 0 1 3 3]\n","ans :  [4 0 1 1 3 1 1 4 3 3 0 4 1 4 3 2 0 1 2 3]\n","step 1090, training accuracy 0.7500\n","pred:  [1 1 1 3 2 3 4 0 4 3 2 1 2 3 4 1 4 0 3 4]\n","ans :  [1 1 1 0 2 3 2 0 4 3 1 1 2 3 2 1 2 0 3 4]\n","step 1100, training accuracy 0.9000\n","pred:  [1 0 0 1 4 4 1 2 0 4 3 0 4 2 4 3 0 4 2 1]\n","ans :  [1 0 0 1 4 4 1 2 0 4 3 0 4 2 4 3 0 2 4 1]\n","step 1110, training accuracy 0.7500\n","pred:  [2 1 3 3 0 1 2 3 2 1 2 3 4 0 2 0 3 3 0 3]\n","ans :  [2 1 3 3 0 1 1 3 2 1 2 3 2 0 2 0 4 0 0 0]\n","step 1120, training accuracy 0.9500\n","pred:  [2 4 3 4 4 3 2 1 2 0 4 3 0 2 0 0 4 1 4 1]\n","ans :  [2 4 3 4 2 3 2 1 2 0 4 3 0 2 0 0 4 1 4 1]\n","step 1130, training accuracy 0.8500\n","pred:  [0 1 2 4 3 3 0 4 2 3 1 0 2 1 3 3 1 0 4 3]\n","ans :  [0 1 2 4 3 3 0 4 4 0 1 0 2 1 0 3 1 0 4 3]\n","step 1140, training accuracy 0.8000\n","pred:  [3 2 3 1 0 0 0 0 1 2 2 4 0 4 2 1 4 3 4 2]\n","ans :  [0 2 4 1 0 0 0 0 1 2 2 2 0 4 2 1 2 3 4 2]\n","step 1150, training accuracy 0.8000\n","pred:  [1 0 3 0 1 4 2 2 4 2 2 0 2 3 4 0 3 1 0 2]\n","ans :  [1 3 3 3 1 4 2 2 4 2 2 0 2 3 4 3 4 1 0 2]\n","step 1160, training accuracy 0.9000\n","pred:  [3 2 3 4 3 3 1 4 2 4 0 1 1 2 0 2 1 4 0 4]\n","ans :  [3 2 3 4 0 3 1 4 2 3 0 1 1 2 0 2 1 4 0 4]\n","step 1170, training accuracy 0.7500\n","pred:  [1 0 2 0 4 4 1 3 2 4 2 1 0 3 3 4 1 3 3 2]\n","ans :  [1 1 2 0 4 3 1 3 2 4 4 1 3 3 3 4 1 3 0 2]\n","step 1180, training accuracy 0.9000\n","pred:  [2 4 4 2 0 1 4 3 4 2 2 1 4 4 3 0 1 2 3 1]\n","ans :  [2 4 4 2 0 1 4 3 4 4 2 1 4 4 0 0 1 2 3 1]\n","step 1190, training accuracy 0.6500\n","pred:  [2 3 0 3 1 2 4 3 2 4 1 3 0 0 4 3 1 3 1 1]\n","ans :  [2 3 3 0 1 2 2 3 4 3 1 3 0 0 4 4 1 3 2 1]\n","step 1200, training accuracy 0.8000\n","pred:  [2 3 4 2 0 4 4 4 1 0 0 0 4 4 2 0 4 1 2 3]\n","ans :  [4 3 4 2 0 4 4 4 1 0 0 0 4 2 2 0 4 2 4 3]\n","step 1210, training accuracy 0.9500\n","pred:  [2 4 2 4 0 0 0 1 1 0 4 3 1 3 2 3 1 1 2 1]\n","ans :  [2 4 2 4 0 0 0 1 1 0 4 3 1 0 2 3 1 1 2 1]\n","step 1220, training accuracy 0.6000\n","pred:  [4 4 4 0 0 4 3 1 0 3 2 3 2 0 1 4 2 1 4 4]\n","ans :  [4 3 3 0 0 3 0 1 0 3 2 3 3 0 1 4 4 1 3 3]\n","step 1230, training accuracy 0.8000\n","pred:  [0 3 0 4 1 1 3 4 2 4 1 1 1 3 1 4 2 1 1 4]\n","ans :  [0 0 0 4 1 1 4 4 2 4 1 1 1 0 1 2 2 1 1 4]\n","step 1240, training accuracy 0.9500\n","pred:  [4 2 3 4 1 4 1 2 0 3 0 4 1 4 3 4 1 1 0 4]\n","ans :  [4 2 3 4 1 4 1 2 0 3 0 4 1 4 3 2 1 1 0 4]\n","step 1250, training accuracy 0.8000\n","pred:  [0 4 0 4 0 2 4 2 2 3 2 0 3 2 4 0 3 0 4 3]\n","ans :  [0 4 0 4 0 2 4 4 2 3 2 0 0 2 4 0 3 3 4 0]\n","step 1260, training accuracy 1.0000\n","pred:  [1 1 3 0 4 1 2 2 1 3 4 1 2 0 0 3 3 0 4 1]\n","ans :  [1 1 3 0 4 1 2 2 1 3 4 1 2 0 0 3 3 0 4 1]\n","step 1270, training accuracy 0.9000\n","pred:  [1 2 0 3 4 1 3 1 4 3 3 4 2 3 2 1 2 1 3 0]\n","ans :  [1 2 0 3 4 1 3 1 2 3 3 4 2 2 2 1 2 1 3 0]\n","step 1280, training accuracy 0.8000\n","pred:  [1 4 1 1 3 3 4 0 4 3 1 4 2 1 4 2 4 1 4 0]\n","ans :  [1 4 1 1 0 0 4 0 4 4 1 2 2 1 4 2 4 1 4 0]\n","step 1290, training accuracy 0.5500\n","pred:  [2 1 3 2 3 4 4 0 3 4 4 1 1 2 0 0 1 0 3 2]\n","ans :  [4 2 4 2 3 4 4 0 3 4 4 1 2 4 3 3 1 3 3 4]\n","step 1300, training accuracy 0.9000\n","pred:  [0 4 2 1 0 0 2 4 2 2 1 4 3 4 2 1 4 4 2 0]\n","ans :  [0 4 2 1 0 0 2 4 2 2 1 3 3 4 2 1 3 4 2 0]\n","step 1310, training accuracy 0.9500\n","pred:  [0 2 3 2 2 3 4 2 1 0 1 3 3 4 3 2 1 1 0 1]\n","ans :  [0 2 3 2 2 3 4 2 1 0 1 3 3 4 3 4 1 1 0 1]\n","step 1320, training accuracy 0.9500\n","pred:  [2 0 2 3 0 4 1 2 0 2 0 0 1 0 3 2 4 3 3 2]\n","ans :  [2 0 2 3 0 4 1 2 0 2 0 0 1 0 3 3 4 3 3 2]\n","step 1330, training accuracy 0.8500\n","pred:  [3 0 2 0 2 2 1 0 2 0 4 2 2 2 1 3 3 3 4 3]\n","ans :  [3 0 4 0 4 2 1 0 2 0 4 2 2 2 1 3 3 3 3 3]\n","step 1340, training accuracy 0.8000\n","pred:  [4 1 1 3 4 4 4 2 3 0 2 0 1 0 3 1 4 0 0 4]\n","ans :  [4 1 1 3 4 3 3 2 3 0 4 0 3 0 3 1 4 0 0 4]\n","step 1350, training accuracy 0.6500\n","pred:  [2 4 4 2 2 1 3 0 4 3 0 1 3 3 1 2 0 4 0 1]\n","ans :  [3 4 3 2 2 0 3 0 4 3 0 1 0 0 1 1 0 2 0 1]\n","step 1360, training accuracy 0.9000\n","pred:  [0 2 1 2 3 4 4 0 1 0 1 4 1 4 1 4 4 3 0 2]\n","ans :  [3 2 1 2 3 2 4 0 1 0 1 4 1 4 1 4 4 3 0 2]\n","step 1370, training accuracy 0.9500\n","pred:  [2 3 0 3 2 4 0 2 3 0 4 0 1 2 2 3 3 4 4 1]\n","ans :  [2 3 0 3 2 4 0 2 3 0 4 0 2 2 2 3 3 4 4 1]\n","step 1380, training accuracy 0.8500\n","pred:  [4 2 3 1 4 2 4 3 3 3 1 1 1 1 0 4 3 0 1 1]\n","ans :  [4 2 0 1 4 2 2 0 3 3 1 1 1 1 0 4 3 0 1 1]\n","step 1390, training accuracy 0.8000\n","pred:  [2 4 4 1 0 0 4 0 1 1 4 2 2 0 2 3 2 3 0 0]\n","ans :  [4 4 4 1 0 3 3 0 1 1 4 4 2 0 2 3 2 3 0 0]\n","step 1400, training accuracy 0.8500\n","pred:  [1 3 2 2 0 0 3 1 3 4 3 2 1 2 3 2 4 2 1 4]\n","ans :  [1 3 2 2 0 0 3 1 4 4 3 2 1 3 3 2 4 2 1 3]\n","step 1410, training accuracy 0.9000\n","pred:  [1 1 4 4 4 0 3 2 1 1 2 0 2 3 4 0 0 0 1 1]\n","ans :  [1 1 4 2 4 0 3 2 1 1 2 0 2 0 4 0 0 0 1 1]\n","step 1420, training accuracy 1.0000\n","pred:  [3 0 3 4 2 3 4 4 4 0 1 1 1 1 2 1 1 3 3 3]\n","ans :  [3 0 3 4 2 3 4 4 4 0 1 1 1 1 2 1 1 3 3 3]\n","step 1430, training accuracy 0.8500\n","pred:  [1 0 2 3 1 3 3 3 3 1 3 0 4 0 2 4 2 2 4 3]\n","ans :  [1 0 2 0 1 3 3 3 3 1 3 0 4 0 2 2 2 2 4 0]\n","step 1440, training accuracy 0.8500\n","pred:  [1 0 0 2 2 3 1 4 3 3 1 4 2 0 0 4 4 3 4 1]\n","ans :  [1 0 0 2 2 3 0 4 3 3 1 2 2 0 0 4 4 3 4 0]\n","step 1450, training accuracy 0.8500\n","pred:  [4 4 0 4 3 1 3 0 4 0 1 4 3 1 4 4 1 0 3 1]\n","ans :  [4 4 0 4 0 1 3 0 2 0 1 4 3 1 4 3 1 0 3 1]\n","step 1460, training accuracy 0.8500\n","pred:  [1 2 0 1 3 4 2 0 2 4 2 1 2 1 2 4 3 3 1 4]\n","ans :  [1 2 0 1 3 3 2 0 2 3 2 1 2 1 2 4 0 3 1 4]\n","step 1470, training accuracy 0.9000\n","pred:  [4 1 2 1 0 4 0 1 1 0 3 3 2 4 0 4 2 2 2 4]\n","ans :  [4 1 2 1 0 4 0 1 1 0 3 3 2 2 0 2 2 2 2 4]\n","step 1480, training accuracy 0.8000\n","pred:  [1 3 2 1 2 0 0 2 0 2 4 3 4 1 1 0 4 4 3 3]\n","ans :  [1 3 2 1 4 0 0 4 0 2 4 3 4 1 1 0 3 3 3 3]\n","step 1490, training accuracy 0.9000\n","pred:  [3 2 1 4 3 1 0 4 0 4 3 2 0 3 1 0 0 1 2 4]\n","ans :  [3 2 1 4 3 1 0 4 0 4 3 2 0 0 1 0 0 1 1 4]\n","step 1500, training accuracy 0.9000\n","pred:  [2 1 0 4 4 4 2 3 3 4 4 1 2 1 0 0 0 2 2 0]\n","ans :  [2 2 3 4 4 4 2 3 3 4 4 1 2 1 0 0 0 2 2 0]\n","step 1510, training accuracy 0.8500\n","pred:  [0 3 4 4 4 0 3 2 0 4 0 1 2 3 1 3 1 0 1 4]\n","ans :  [3 4 4 4 4 0 3 2 0 4 0 1 2 3 1 3 1 3 1 4]\n","step 1520, training accuracy 0.7000\n","pred:  [1 4 0 4 2 1 2 3 2 3 2 0 4 4 1 0 4 0 0 4]\n","ans :  [1 4 0 3 4 2 2 3 2 4 2 0 4 4 1 3 4 1 0 4]\n","step 1530, training accuracy 0.9000\n","pred:  [2 1 3 3 0 0 3 3 4 3 4 4 0 1 2 2 1 4 2 3]\n","ans :  [2 1 3 3 0 0 3 3 4 3 4 2 0 1 2 2 1 3 2 3]\n","step 1540, training accuracy 0.8500\n","pred:  [3 3 0 2 1 3 2 3 3 1 3 4 1 4 1 2 2 0 1 0]\n","ans :  [0 3 0 2 1 0 2 3 3 1 3 4 1 2 1 2 2 0 1 0]\n","step 1550, training accuracy 0.8500\n","pred:  [2 4 4 1 1 4 0 0 4 0 2 3 4 0 2 3 2 4 1 1]\n","ans :  [4 4 4 1 1 2 0 0 4 0 2 3 4 0 2 3 2 3 1 1]\n","step 1560, training accuracy 0.9000\n","pred:  [0 4 3 3 1 4 4 3 1 2 3 1 2 0 0 1 4 1 3 0]\n","ans :  [0 4 4 3 1 4 4 3 1 2 3 1 2 0 0 1 2 1 3 0]\n","step 1570, training accuracy 0.9500\n","pred:  [1 4 0 4 1 4 3 3 0 1 2 3 2 0 4 0 1 4 2 1]\n","ans :  [1 4 0 4 1 4 0 3 0 1 2 3 2 0 4 0 1 4 2 1]\n","step 1580, training accuracy 0.9000\n","pred:  [1 3 2 4 0 1 3 0 1 1 4 3 3 1 4 0 1 0 1 3]\n","ans :  [1 3 2 4 0 1 3 0 1 1 4 0 3 1 4 0 1 0 1 0]\n","step 1590, training accuracy 0.8000\n","pred:  [1 4 2 2 3 4 4 1 3 1 4 0 0 2 1 4 2 3 0 3]\n","ans :  [1 4 2 2 3 3 4 1 3 3 4 0 0 2 1 4 4 3 3 3]\n","step 1600, training accuracy 0.7500\n","pred:  [0 2 2 4 0 1 3 1 0 4 2 2 4 4 4 3 4 0 0 0]\n","ans :  [0 2 2 4 3 1 3 2 3 4 2 2 3 4 4 3 4 0 3 0]\n","step 1610, training accuracy 0.8500\n","pred:  [1 1 3 4 4 0 3 2 4 2 1 4 0 0 4 4 0 1 3 1]\n","ans :  [1 1 3 4 4 0 4 2 2 2 1 4 0 0 4 2 0 1 3 1]\n","step 1620, training accuracy 0.8500\n","pred:  [4 0 0 1 3 1 1 4 1 2 0 2 3 0 4 4 4 1 3 0]\n","ans :  [4 0 0 1 3 1 1 4 1 2 0 2 3 0 2 3 4 2 3 0]\n","step 1630, training accuracy 0.8000\n","pred:  [3 2 1 1 2 3 3 2 0 1 1 4 0 3 0 4 2 2 0 0]\n","ans :  [3 2 1 1 2 0 3 4 0 1 1 4 0 3 0 4 1 3 0 0]\n","step 1640, training accuracy 0.8000\n","pred:  [1 4 0 3 2 2 4 1 0 2 4 0 1 3 2 0 3 1 3 4]\n","ans :  [1 4 0 3 4 2 4 1 3 4 4 0 1 3 4 0 3 1 3 4]\n","step 1650, training accuracy 0.8000\n","pred:  [1 4 3 2 4 4 0 2 1 0 1 1 1 3 2 2 4 4 0 3]\n","ans :  [0 4 4 2 4 4 2 2 1 0 1 1 1 3 2 2 4 3 0 3]\n","step 1660, training accuracy 0.8500\n","pred:  [3 1 4 0 0 3 4 4 0 3 1 4 0 3 0 1 4 4 2 2]\n","ans :  [0 1 4 0 0 3 4 2 0 3 1 4 3 3 0 1 4 4 2 2]\n","step 1670, training accuracy 0.8000\n","pred:  [0 1 3 2 0 4 4 1 3 0 2 3 2 3 3 0 2 1 4 4]\n","ans :  [0 0 3 2 0 3 4 1 3 3 2 3 2 3 0 0 2 1 4 4]\n","step 1680, training accuracy 0.8500\n","pred:  [0 2 1 1 0 3 2 1 4 4 2 4 0 0 3 2 1 4 3 4]\n","ans :  [0 2 1 0 0 3 2 1 4 4 2 3 0 3 3 2 1 4 3 4]\n","step 1690, training accuracy 0.9500\n","pred:  [1 0 3 3 1 3 0 2 4 2 2 0 2 3 2 4 0 4 4 1]\n","ans :  [1 0 3 3 1 3 0 2 4 2 2 0 2 3 2 4 0 4 2 1]\n","step 1700, training accuracy 0.9000\n","pred:  [0 4 1 4 4 4 4 2 1 2 3 3 0 4 2 1 1 0 0 2]\n","ans :  [0 4 1 4 4 4 2 2 1 2 3 3 3 4 2 1 1 0 0 2]\n","step 1710, training accuracy 0.9000\n","pred:  [4 3 3 2 3 3 1 1 1 3 4 4 0 1 3 0 1 0 0 0]\n","ans :  [4 3 3 1 3 3 1 1 1 3 4 4 0 1 0 0 1 0 0 0]\n","step 1720, training accuracy 0.8000\n","pred:  [4 0 2 2 0 2 0 4 1 3 3 0 3 3 4 2 3 1 3 0]\n","ans :  [4 3 2 2 3 2 0 3 1 3 3 0 3 3 3 2 3 1 3 0]\n","step 1730, training accuracy 0.9000\n","pred:  [0 1 2 3 2 2 1 4 0 0 3 3 1 3 3 2 1 3 3 0]\n","ans :  [3 1 2 4 2 2 1 4 0 0 3 3 1 3 3 2 1 3 3 0]\n","step 1740, training accuracy 0.9000\n","pred:  [4 0 1 2 4 4 4 3 0 3 1 0 1 2 0 2 2 2 0 1]\n","ans :  [4 0 1 2 2 4 4 3 0 3 1 0 1 2 0 1 2 2 0 1]\n","step 1750, training accuracy 0.9500\n","pred:  [0 0 4 3 1 2 2 3 0 1 0 1 2 2 1 4 3 4 3 3]\n","ans :  [0 0 4 3 1 2 2 0 0 1 0 1 2 2 1 4 3 4 3 3]\n","step 1760, training accuracy 0.7500\n","pred:  [4 4 1 4 2 2 1 4 4 1 0 4 3 4 0 2 0 4 1 3]\n","ans :  [3 4 1 4 2 3 1 2 3 1 0 3 3 4 0 2 0 4 1 3]\n","step 1770, training accuracy 0.8500\n","pred:  [1 3 4 0 1 3 4 3 3 1 1 0 0 2 4 0 4 1 1 2]\n","ans :  [1 2 4 0 1 4 4 3 3 1 1 0 0 2 2 0 4 1 1 2]\n","step 1780, training accuracy 0.7000\n","pred:  [4 4 4 2 2 0 4 2 3 2 3 2 1 1 0 0 0 3 4 3]\n","ans :  [4 4 3 2 3 0 3 2 0 2 0 2 1 1 0 0 0 0 4 3]\n","step 1790, training accuracy 0.9000\n","pred:  [1 3 2 1 4 0 2 4 2 2 4 4 2 0 0 4 1 4 1 2]\n","ans :  [1 3 2 1 4 0 2 4 2 2 2 4 2 3 0 4 1 4 1 2]\n","step 1800, training accuracy 0.9500\n","pred:  [2 3 4 0 0 0 0 3 2 1 1 3 4 0 0 2 0 4 3 2]\n","ans :  [2 3 4 0 0 0 3 3 2 1 1 3 4 0 0 2 0 4 3 2]\n","step 1810, training accuracy 0.9000\n","pred:  [2 4 2 3 1 0 1 4 0 0 4 3 4 0 4 1 1 4 3 3]\n","ans :  [2 4 2 3 1 0 1 4 0 0 4 3 2 0 2 1 1 4 3 3]\n","step 1820, training accuracy 0.9000\n","pred:  [4 4 0 3 0 1 4 0 0 4 1 0 4 4 1 2 4 1 2 3]\n","ans :  [4 4 0 3 0 1 4 0 0 4 1 0 4 3 1 2 3 1 2 3]\n","step 1830, training accuracy 0.8000\n","pred:  [1 2 4 3 2 0 3 3 0 2 4 1 1 2 3 4 4 2 1 3]\n","ans :  [1 2 4 3 2 0 3 3 0 2 4 1 2 2 3 2 4 4 1 4]\n","step 1840, training accuracy 0.8500\n","pred:  [0 3 1 0 4 0 1 1 2 4 3 3 1 4 3 4 3 3 0 4]\n","ans :  [0 3 1 0 2 0 1 1 2 4 3 3 1 4 0 4 3 3 0 3]\n","step 1850, training accuracy 0.7500\n","pred:  [1 4 2 0 2 2 3 3 1 0 0 4 4 3 4 1 1 0 4 3]\n","ans :  [1 4 2 0 2 2 3 0 1 0 0 4 2 4 2 1 1 0 3 3]\n","step 1860, training accuracy 0.9500\n","pred:  [1 1 2 1 2 4 2 3 3 3 3 1 4 0 0 3 3 4 1 4]\n","ans :  [1 1 2 1 2 2 2 3 3 3 3 1 4 0 0 3 3 4 1 4]\n","step 1870, training accuracy 0.9500\n","pred:  [0 4 4 2 2 2 3 1 3 2 2 0 2 1 4 3 4 0 1 0]\n","ans :  [0 4 4 2 2 2 0 1 3 2 2 0 2 1 4 3 4 0 1 0]\n","step 1880, training accuracy 0.9000\n","pred:  [0 0 4 0 4 1 0 4 4 3 4 3 2 1 3 1 1 2 3 3]\n","ans :  [0 0 4 0 4 1 0 4 4 3 4 4 2 1 0 1 1 2 3 3]\n","step 1890, training accuracy 0.8000\n","pred:  [4 1 3 4 4 2 2 2 3 4 3 0 4 3 1 2 1 0 0 0]\n","ans :  [4 1 0 4 3 4 2 2 3 4 3 0 4 3 1 2 1 0 3 0]\n","step 1900, training accuracy 0.7000\n","pred:  [4 3 1 1 0 4 2 1 4 4 0 3 3 1 3 3 1 0 2 1]\n","ans :  [4 0 1 1 3 4 2 1 2 4 1 3 3 1 0 3 1 1 2 1]\n","step 1910, training accuracy 0.9000\n","pred:  [2 0 1 2 1 2 0 2 4 1 1 4 0 4 2 3 3 3 3 0]\n","ans :  [2 0 1 2 1 2 0 2 4 1 1 4 0 4 2 2 3 0 3 0]\n","step 1920, training accuracy 0.8500\n","pred:  [4 4 1 3 1 2 1 1 3 2 2 3 3 1 0 3 3 1 1 2]\n","ans :  [4 4 1 3 1 2 1 1 3 2 2 3 2 1 3 2 3 1 1 2]\n","step 1930, training accuracy 0.7000\n","pred:  [1 0 4 3 0 1 0 2 0 4 1 1 4 1 4 4 0 4 0 3]\n","ans :  [1 3 4 4 3 1 0 2 0 2 1 1 4 1 4 4 3 2 0 3]\n","step 1940, training accuracy 0.9000\n","pred:  [2 4 1 2 2 2 3 0 3 0 3 2 2 4 4 0 0 4 1 0]\n","ans :  [2 4 1 2 2 2 3 0 3 0 3 2 4 4 3 0 0 4 1 0]\n","step 1950, training accuracy 0.7000\n","pred:  [0 3 3 0 1 4 1 4 0 4 3 3 1 1 2 4 4 0 4 1]\n","ans :  [0 4 3 3 1 4 1 2 3 3 3 3 1 2 2 4 4 0 4 1]\n","step 1960, training accuracy 0.8500\n","pred:  [1 4 0 4 1 3 4 0 2 0 3 4 0 1 2 2 0 3 0 3]\n","ans :  [1 4 0 4 1 0 4 0 2 0 0 4 0 1 2 2 0 0 0 3]\n","step 1970, training accuracy 0.8000\n","pred:  [1 2 1 1 4 3 2 3 4 0 2 2 2 3 3 3 3 0 2 0]\n","ans :  [1 2 1 1 3 3 2 3 4 0 2 2 4 3 3 3 0 0 4 0]\n","step 1980, training accuracy 0.9000\n","pred:  [4 4 3 0 1 3 1 3 2 2 4 0 1 1 3 2 3 1 2 0]\n","ans :  [4 4 3 3 1 3 1 3 2 2 4 3 1 1 3 2 3 1 2 0]\n","step 1990, training accuracy 0.7500\n","pred:  [3 0 1 4 2 3 0 0 4 2 4 2 0 4 4 3 4 2 1 4]\n","ans :  [4 3 2 4 2 3 3 0 4 3 4 2 0 4 4 3 4 2 1 4]\n","step 2000, training accuracy 0.9000\n","pred:  [4 4 4 1 3 1 0 3 0 4 2 1 1 0 0 0 2 0 0 0]\n","ans :  [4 3 4 1 0 1 0 3 0 4 2 1 1 0 0 0 2 0 0 0]\n","step 2010, training accuracy 0.8500\n","pred:  [3 0 3 0 3 2 4 4 1 3 3 2 1 1 2 1 2 1 0 4]\n","ans :  [3 3 3 0 3 2 2 2 1 3 3 2 1 1 2 1 2 1 0 4]\n","step 2020, training accuracy 0.9000\n","pred:  [1 2 3 3 0 4 1 1 1 3 3 4 4 0 3 1 2 1 0 3]\n","ans :  [1 2 3 3 0 2 1 1 1 3 3 4 3 0 3 1 2 1 0 3]\n","step 2030, training accuracy 0.9000\n","pred:  [0 1 3 1 0 0 4 1 0 1 2 3 2 3 4 1 1 4 2 3]\n","ans :  [0 1 0 1 0 0 4 1 0 2 2 3 2 3 4 1 1 4 2 3]\n","step 2040, training accuracy 0.9500\n","pred:  [2 1 0 3 2 1 2 2 3 3 2 1 4 3 4 0 3 0 3 1]\n","ans :  [2 1 0 3 2 1 2 2 3 3 2 1 4 3 4 0 0 0 3 1]\n","step 2050, training accuracy 0.9500\n","pred:  [4 4 3 1 0 2 3 3 3 0 2 3 4 1 0 0 1 3 0 1]\n","ans :  [4 4 3 1 0 2 3 3 0 0 2 3 4 1 0 0 1 3 0 1]\n","step 2060, training accuracy 0.9000\n","pred:  [1 4 0 4 3 3 2 2 1 0 1 0 3 1 4 1 4 2 0 2]\n","ans :  [1 3 0 4 3 0 2 2 1 0 1 0 3 1 4 1 4 2 0 2]\n","step 2070, training accuracy 0.8500\n","pred:  [2 3 2 0 4 4 1 0 2 2 2 4 0 1 3 2 0 4 2 1]\n","ans :  [2 3 2 0 2 4 1 0 2 2 2 4 0 1 4 2 0 2 2 1]\n","step 2080, training accuracy 0.8500\n","pred:  [0 3 1 1 2 4 4 2 0 4 3 4 0 0 3 2 2 3 3 2]\n","ans :  [0 3 1 1 2 4 4 2 0 4 3 3 0 0 3 4 2 3 0 2]\n","step 2090, training accuracy 0.8500\n","pred:  [1 3 4 2 1 3 0 4 0 4 2 3 3 1 1 0 3 1 2 0]\n","ans :  [1 4 4 2 1 3 0 4 0 2 2 0 3 1 1 0 3 1 2 0]\n","step 2100, training accuracy 0.8000\n","pred:  [1 2 2 1 0 4 0 4 2 3 2 4 3 2 0 2 4 0 1 2]\n","ans :  [1 2 2 1 3 4 0 4 2 3 2 4 3 4 0 4 2 0 1 2]\n","step 2110, training accuracy 0.8500\n","pred:  [0 4 3 2 3 1 1 0 2 3 1 1 3 0 3 3 4 1 4 4]\n","ans :  [0 4 3 2 4 1 1 0 2 0 1 1 3 3 3 3 4 1 4 4]\n","step 2120, training accuracy 1.0000\n","pred:  [3 0 3 4 1 3 1 4 1 0 3 4 2 0 2 4 0 3 2 2]\n","ans :  [3 0 3 4 1 3 1 4 1 0 3 4 2 0 2 4 0 3 2 2]\n","step 2130, training accuracy 0.9000\n","pred:  [0 4 4 1 2 2 4 4 3 0 2 3 1 2 0 4 2 4 2 2]\n","ans :  [0 4 2 1 2 2 4 2 3 0 2 3 1 2 0 4 2 4 2 2]\n","step 2140, training accuracy 0.8500\n","pred:  [4 4 1 3 3 2 1 2 3 2 0 3 3 0 1 2 1 0 2 0]\n","ans :  [2 4 1 3 3 2 1 2 3 2 0 4 3 3 1 2 1 0 2 0]\n","step 2150, training accuracy 0.7500\n","pred:  [0 2 4 3 3 2 2 3 0 3 4 1 0 0 1 2 4 1 0 3]\n","ans :  [0 2 4 0 3 4 2 3 0 3 3 1 0 0 1 2 2 1 3 3]\n","step 2160, training accuracy 0.8000\n","pred:  [1 1 4 2 0 1 2 2 2 4 1 4 0 4 1 4 4 2 0 1]\n","ans :  [1 1 4 3 0 1 2 2 4 4 1 3 0 4 1 4 4 2 3 1]\n","step 2170, training accuracy 0.9500\n","pred:  [2 1 0 0 3 3 4 0 4 3 2 2 2 4 4 1 1 1 0 4]\n","ans :  [2 1 0 0 0 3 4 0 4 3 2 2 2 4 4 1 1 1 0 4]\n","step 2180, training accuracy 0.6500\n","pred:  [4 4 2 0 3 2 1 3 2 0 4 3 1 1 0 2 2 1 4 0]\n","ans :  [4 4 2 0 0 2 1 0 2 0 2 2 1 1 0 1 1 1 3 0]\n","step 2190, training accuracy 0.8500\n","pred:  [1 3 1 0 4 4 1 2 2 2 3 3 4 3 3 4 0 0 2 1]\n","ans :  [2 3 1 0 4 4 1 2 2 2 3 3 4 0 3 4 0 0 2 0]\n","step 2200, training accuracy 0.8500\n","pred:  [2 2 1 4 3 3 4 1 2 1 3 3 1 4 0 0 0 3 3 4]\n","ans :  [2 2 1 4 0 3 4 1 2 1 0 3 1 3 0 0 0 3 3 4]\n","step 2210, training accuracy 0.9000\n","pred:  [2 3 0 4 0 0 1 3 1 1 3 4 4 0 2 2 0 2 3 0]\n","ans :  [2 3 0 2 0 0 1 0 1 1 3 4 4 0 2 2 0 2 3 0]\n","step 2220, training accuracy 0.9500\n","pred:  [4 3 2 3 4 4 0 1 4 0 3 2 0 0 1 2 3 2 2 1]\n","ans :  [4 0 2 3 4 4 0 1 4 0 3 2 0 0 1 2 3 2 2 1]\n","step 2230, training accuracy 0.9500\n","pred:  [4 3 0 1 3 0 2 0 4 4 0 2 4 1 4 3 4 3 4 0]\n","ans :  [4 3 0 1 3 0 2 0 2 4 0 2 4 1 4 3 4 3 4 0]\n","step 2240, training accuracy 0.9500\n","pred:  [1 3 4 4 0 4 3 3 2 1 0 3 0 2 4 1 3 1 1 4]\n","ans :  [1 0 4 4 0 4 3 3 2 1 0 3 0 2 4 1 3 1 1 4]\n","step 2250, training accuracy 0.7500\n","pred:  [0 1 2 3 0 1 0 0 0 4 1 4 3 2 1 4 4 1 1 4]\n","ans :  [0 1 3 3 0 1 0 0 0 4 1 2 3 1 1 2 2 1 1 4]\n","step 2260, training accuracy 0.8500\n","pred:  [3 0 2 2 2 2 2 0 3 0 4 4 0 1 4 0 4 3 1 4]\n","ans :  [3 0 2 2 4 2 2 0 0 0 4 4 0 1 4 0 3 3 1 4]\n","step 2270, training accuracy 0.9500\n","pred:  [2 1 4 4 0 2 3 1 3 0 4 3 0 1 1 2 0 0 3 4]\n","ans :  [2 1 4 4 0 2 3 1 3 0 4 3 3 1 1 2 0 0 3 4]\n","step 2280, training accuracy 0.8500\n","pred:  [4 3 0 1 4 1 3 3 0 1 3 2 1 4 0 1 3 2 4 2]\n","ans :  [4 3 0 1 4 1 3 3 0 1 3 2 1 4 3 1 3 2 2 4]\n","step 2290, training accuracy 0.8000\n","pred:  [4 1 1 1 0 3 4 4 4 3 4 4 0 0 2 1 3 1 0 0]\n","ans :  [4 1 1 1 0 3 4 2 4 3 4 2 0 3 4 1 3 1 0 0]\n","step 2300, training accuracy 1.0000\n","pred:  [2 4 3 4 0 1 3 4 1 1 4 2 0 3 1 4 3 0 2 1]\n","ans :  [2 4 3 4 0 1 3 4 1 1 4 2 0 3 1 4 3 0 2 1]\n","step 2310, training accuracy 0.9500\n","pred:  [4 2 0 4 4 0 2 4 1 4 0 2 2 1 1 1 0 3 1 0]\n","ans :  [4 2 0 4 4 0 2 4 1 2 0 2 2 1 1 1 0 3 1 0]\n","step 2320, training accuracy 0.9000\n","pred:  [0 0 4 3 2 0 1 0 4 2 3 1 3 3 1 1 1 2 3 4]\n","ans :  [0 0 2 3 2 0 1 0 4 2 3 1 0 3 1 1 1 2 3 4]\n","step 2330, training accuracy 0.9000\n","pred:  [1 0 2 3 2 2 0 4 2 3 1 4 3 0 3 1 1 0 3 4]\n","ans :  [1 0 2 3 2 2 0 4 2 3 1 4 3 0 4 1 1 0 0 4]\n","step 2340, training accuracy 0.9000\n","pred:  [0 2 0 1 3 1 2 3 1 0 0 4 3 4 0 1 3 0 4 4]\n","ans :  [0 2 0 1 3 1 2 3 1 0 0 4 3 4 0 1 0 0 3 4]\n","step 2350, training accuracy 0.9000\n","pred:  [2 2 2 0 1 4 0 0 2 0 3 0 3 1 2 3 2 1 1 3]\n","ans :  [2 2 2 0 1 4 0 0 2 0 3 0 3 1 3 3 4 1 1 3]\n","step 2360, training accuracy 0.9500\n","pred:  [2 0 2 3 1 0 2 4 0 2 1 0 4 2 3 4 0 3 2 0]\n","ans :  [2 0 2 3 1 0 2 4 0 2 1 0 4 2 3 4 0 3 2 3]\n","step 2370, training accuracy 0.8500\n","pred:  [1 1 3 4 0 4 3 1 0 0 3 3 1 2 2 3 4 2 0 3]\n","ans :  [1 0 3 3 0 4 3 1 0 0 3 3 1 2 2 3 3 2 0 3]\n","step 2380, training accuracy 0.8500\n","pred:  [4 3 0 1 1 1 3 0 0 4 3 1 4 3 1 1 0 3 0 4]\n","ans :  [2 3 1 1 1 1 3 0 0 4 3 1 4 0 1 1 0 3 0 4]\n","step 2390, training accuracy 0.8500\n","pred:  [1 0 3 0 3 0 2 3 3 3 2 3 0 4 1 1 2 4 1 3]\n","ans :  [1 0 3 3 3 3 2 3 3 3 2 3 0 4 1 1 4 4 1 3]\n","step 2400, training accuracy 0.8000\n","pred:  [3 2 3 4 0 2 0 4 0 1 3 4 2 2 4 0 4 1 3 0]\n","ans :  [3 2 3 4 0 2 0 3 0 1 0 3 2 2 4 0 3 1 3 0]\n","step 2410, training accuracy 0.9500\n","pred:  [4 0 4 2 3 1 4 2 4 1 0 0 2 0 4 1 4 3 2 3]\n","ans :  [4 0 4 2 3 1 3 2 4 1 0 0 2 0 4 1 4 3 2 3]\n","step 2420, training accuracy 0.8000\n","pred:  [2 2 2 2 0 2 4 4 2 4 4 2 0 0 0 0 4 1 0 4]\n","ans :  [2 3 2 2 0 2 4 4 0 4 3 2 0 0 0 0 3 1 0 4]\n","step 2430, training accuracy 0.8500\n","pred:  [1 4 0 2 2 0 4 0 2 3 2 3 1 0 3 4 3 3 4 3]\n","ans :  [1 4 3 2 2 0 4 0 2 3 2 3 1 3 3 0 3 3 4 3]\n","step 2440, training accuracy 0.8500\n","pred:  [4 3 2 3 2 0 0 0 1 3 4 4 0 3 3 0 1 4 3 1]\n","ans :  [4 0 2 3 2 0 0 0 1 3 4 4 0 3 3 0 1 2 0 1]\n","step 2450, training accuracy 0.9000\n","pred:  [0 0 1 1 4 0 1 1 0 3 2 2 4 2 2 3 2 0 2 4]\n","ans :  [0 0 1 1 4 0 1 1 0 3 2 2 2 3 2 3 2 0 2 4]\n","step 2460, training accuracy 1.0000\n","pred:  [1 4 4 1 3 1 1 3 4 4 1 0 2 0 4 2 2 0 2 4]\n","ans :  [1 4 4 1 3 1 1 3 4 4 1 0 2 0 4 2 2 0 2 4]\n","step 2470, training accuracy 0.8000\n","pred:  [4 0 1 4 2 0 0 1 4 3 3 4 0 2 2 2 2 0 0 0]\n","ans :  [4 0 1 4 2 0 0 1 4 0 0 3 0 2 3 2 2 0 0 0]\n","step 2480, training accuracy 0.9500\n","pred:  [0 4 4 2 1 2 1 0 4 3 1 0 0 2 1 3 4 3 4 4]\n","ans :  [0 4 4 2 1 2 1 0 4 3 1 0 0 2 1 3 2 3 4 4]\n","step 2490, training accuracy 0.8500\n","pred:  [0 2 2 3 1 3 4 1 1 4 4 1 0 3 2 4 0 1 3 3]\n","ans :  [0 1 2 3 1 3 4 1 1 4 4 1 0 3 2 2 0 1 3 0]\n","step 2500, training accuracy 0.8500\n","pred:  [2 1 4 2 0 3 4 0 2 3 4 4 2 0 3 4 0 0 2 1]\n","ans :  [2 1 2 3 0 3 4 0 4 3 4 4 2 0 3 4 0 0 2 1]\n","step 2510, training accuracy 0.9500\n","pred:  [1 1 3 0 4 3 2 4 1 1 3 4 2 1 1 1 1 4 2 2]\n","ans :  [1 1 3 0 4 3 2 2 1 1 3 4 2 1 1 1 1 4 2 2]\n","step 2520, training accuracy 0.9000\n","pred:  [1 2 3 0 2 1 0 1 4 0 2 2 4 3 3 3 0 0 4 0]\n","ans :  [1 2 0 0 2 1 0 1 4 0 2 2 2 3 3 3 0 0 4 0]\n","step 2530, training accuracy 0.8500\n","pred:  [2 4 1 3 3 3 4 1 4 1 1 4 4 2 0 0 3 2 0 0]\n","ans :  [2 4 1 3 3 3 4 1 4 1 1 2 2 2 0 0 0 2 0 0]\n","step 2540, training accuracy 0.9000\n","pred:  [3 4 0 1 4 4 1 2 0 2 1 0 0 3 4 1 2 0 0 4]\n","ans :  [3 4 0 1 4 3 1 2 0 2 1 0 0 3 3 1 2 0 0 4]\n","step 2550, training accuracy 0.9000\n","pred:  [2 0 4 2 0 2 4 2 1 3 1 1 1 0 3 4 2 1 2 0]\n","ans :  [2 0 4 2 0 2 2 2 1 3 1 1 1 0 0 4 2 1 2 0]\n","step 2560, training accuracy 0.9500\n","pred:  [4 3 1 1 3 1 4 0 1 4 2 2 4 2 0 0 4 4 2 1]\n","ans :  [4 3 1 1 3 2 4 0 1 4 2 2 4 2 0 0 4 4 2 1]\n","step 2570, training accuracy 0.9000\n","pred:  [0 2 1 1 1 4 0 1 0 4 3 4 1 3 3 4 0 4 4 1]\n","ans :  [0 2 1 0 1 4 0 1 0 4 3 4 1 3 0 4 0 4 4 1]\n","step 2580, training accuracy 0.7500\n","pred:  [4 2 4 1 0 3 0 4 0 4 0 2 3 0 2 2 2 4 2 4]\n","ans :  [4 2 3 1 0 2 0 4 0 4 1 2 3 1 2 2 2 3 2 4]\n","step 2590, training accuracy 1.0000\n","pred:  [1 2 2 3 0 4 3 4 1 2 1 0 2 0 1 0 0 3 3 0]\n","ans :  [1 2 2 3 0 4 3 4 1 2 1 0 2 0 1 0 0 3 3 0]\n","step 2600, training accuracy 0.8000\n","pred:  [3 4 0 1 4 1 0 2 3 1 2 4 3 2 2 1 0 3 3 2]\n","ans :  [3 4 0 1 3 1 0 2 3 1 2 3 4 4 2 1 0 3 3 2]\n","step 2610, training accuracy 0.9500\n","pred:  [3 3 1 2 1 0 0 0 0 3 1 3 4 2 1 4 3 0 4 1]\n","ans :  [3 3 1 2 1 0 0 0 3 3 1 3 4 2 1 4 3 0 4 1]\n","step 2620, training accuracy 0.9000\n","pred:  [2 1 3 4 4 0 3 0 2 2 2 0 3 2 3 1 0 2 4 4]\n","ans :  [2 1 4 4 4 0 3 3 2 2 2 0 3 2 3 1 0 2 4 4]\n","step 2630, training accuracy 0.9000\n","pred:  [4 0 4 1 3 1 0 3 0 2 2 3 2 4 2 4 2 0 2 4]\n","ans :  [4 0 4 1 3 1 0 4 0 2 2 3 2 4 2 2 2 0 2 4]\n","step 2640, training accuracy 0.9500\n","pred:  [4 3 0 1 1 1 4 2 2 2 3 3 4 0 1 1 4 3 3 1]\n","ans :  [4 0 0 1 1 1 4 2 2 2 3 3 4 0 1 1 4 3 3 1]\n","step 2650, training accuracy 0.7000\n","pred:  [1 0 4 2 0 4 2 2 3 2 0 2 2 2 0 4 2 0 1 0]\n","ans :  [1 0 3 2 0 4 2 2 3 4 3 4 4 2 0 4 4 0 1 0]\n","step 2660, training accuracy 0.8500\n","pred:  [0 3 3 1 4 4 4 0 4 1 0 2 0 2 3 0 2 2 2 4]\n","ans :  [0 3 3 1 4 4 2 0 4 1 0 4 0 2 3 3 2 2 2 4]\n","step 2670, training accuracy 0.9000\n","pred:  [4 1 4 0 0 2 4 3 2 4 4 1 2 0 3 4 3 2 1 0]\n","ans :  [4 1 4 0 3 2 4 3 4 4 4 1 2 0 3 4 3 2 1 0]\n","step 2680, training accuracy 0.9000\n","pred:  [0 1 1 4 0 1 3 3 0 4 3 4 1 0 3 2 4 1 4 1]\n","ans :  [0 1 1 4 0 1 0 3 0 4 3 3 1 0 3 2 4 1 4 1]\n","step 2690, training accuracy 0.9000\n","pred:  [3 4 2 0 0 1 2 2 1 3 4 3 0 4 3 3 2 4 0 0]\n","ans :  [3 4 2 3 3 1 2 2 1 3 4 3 0 4 3 3 2 4 0 0]\n","step 2700, training accuracy 0.9000\n","pred:  [3 4 2 2 3 3 0 1 2 1 3 0 4 2 4 0 1 1 3 0]\n","ans :  [3 4 2 2 3 3 0 1 4 1 3 0 4 2 2 0 1 1 3 0]\n","step 2710, training accuracy 0.7500\n","pred:  [2 3 4 0 1 1 3 3 4 2 4 3 4 1 2 2 2 1 3 0]\n","ans :  [2 3 4 0 3 1 4 4 4 2 4 3 4 2 2 4 2 1 3 0]\n","step 2720, training accuracy 0.9000\n","pred:  [4 0 2 0 0 2 4 2 0 4 3 0 2 4 4 3 2 2 1 1]\n","ans :  [4 0 4 0 2 2 4 2 0 4 3 0 2 4 4 3 2 2 1 1]\n","step 2730, training accuracy 0.9000\n","pred:  [0 2 0 3 4 2 1 1 3 4 4 4 2 1 0 4 4 2 1 4]\n","ans :  [0 2 0 3 4 2 1 1 3 3 3 4 2 1 0 4 4 2 1 4]\n","step 2740, training accuracy 0.9500\n","pred:  [2 1 4 1 0 3 4 2 4 1 3 0 1 1 0 3 1 3 4 2]\n","ans :  [2 1 4 1 0 0 4 2 4 1 3 0 1 1 0 3 1 3 4 2]\n","step 2750, training accuracy 0.8000\n","pred:  [0 0 0 0 0 4 2 3 1 1 4 4 3 4 3 4 2 3 1 1]\n","ans :  [0 0 0 3 0 4 2 0 1 1 4 3 3 3 3 4 2 3 1 1]\n","step 2760, training accuracy 0.6500\n","pred:  [4 3 0 4 1 3 0 4 3 2 0 2 4 2 2 0 0 2 2 2]\n","ans :  [4 4 0 4 1 3 0 4 3 2 3 4 3 2 4 0 0 2 4 4]\n","step 2770, training accuracy 0.9500\n","pred:  [3 4 2 3 0 1 1 4 3 3 3 3 0 3 1 4 1 1 0 2]\n","ans :  [3 4 2 3 0 1 1 4 3 0 3 3 0 3 1 4 1 1 0 2]\n","step 2780, training accuracy 0.9500\n","pred:  [1 3 4 2 1 3 2 0 1 3 2 0 2 3 2 0 1 0 3 2]\n","ans :  [1 3 4 2 1 3 2 0 1 3 2 0 2 3 2 0 1 0 4 2]\n","step 2790, training accuracy 0.9500\n","pred:  [4 3 2 0 0 2 0 3 1 1 4 0 3 1 2 1 1 1 4 3]\n","ans :  [4 3 2 0 0 2 0 3 1 1 4 0 3 1 2 1 1 1 4 0]\n","step 2800, training accuracy 0.9500\n","pred:  [2 0 1 2 4 3 1 0 0 3 2 3 4 0 1 2 2 0 2 4]\n","ans :  [2 0 1 2 4 3 1 0 0 3 2 3 4 0 1 2 2 0 2 3]\n","step 2810, training accuracy 1.0000\n","pred:  [4 3 2 4 4 0 1 4 0 1 1 1 4 2 1 1 2 0 1 2]\n","ans :  [4 3 2 4 4 0 1 4 0 1 1 1 4 2 1 1 2 0 1 2]\n","step 2820, training accuracy 0.9000\n","pred:  [4 4 0 3 1 0 2 1 1 0 4 0 2 4 4 0 4 3 1 2]\n","ans :  [4 4 0 3 1 0 2 1 1 0 4 0 2 4 4 3 2 3 1 2]\n","step 2830, training accuracy 0.9000\n","pred:  [4 4 2 3 4 1 1 0 1 2 0 3 1 0 0 1 4 4 2 4]\n","ans :  [2 2 2 3 4 1 1 0 1 2 0 3 1 0 0 1 4 4 2 4]\n","step 2840, training accuracy 0.8000\n","pred:  [1 4 1 3 0 4 3 4 3 1 1 0 4 2 4 2 3 0 3 3]\n","ans :  [1 4 1 3 0 2 3 4 3 1 1 0 4 0 2 2 0 0 3 3]\n","step 2850, training accuracy 1.0000\n","pred:  [4 2 2 3 4 4 3 1 3 3 4 1 1 1 4 0 2 4 4 0]\n","ans :  [4 2 2 3 4 4 3 1 3 3 4 1 1 1 4 0 2 4 4 0]\n","step 2860, training accuracy 0.9500\n","pred:  [0 0 3 3 3 1 2 1 2 1 0 1 4 1 4 3 0 3 0 1]\n","ans :  [0 0 0 3 3 1 2 1 2 1 0 1 4 1 4 3 0 3 0 1]\n","step 2870, training accuracy 0.9000\n","pred:  [3 2 1 2 0 0 0 0 1 2 4 1 2 4 2 0 0 1 0 3]\n","ans :  [3 2 1 2 0 0 3 0 1 2 4 1 2 4 2 0 3 1 0 3]\n","step 2880, training accuracy 0.6500\n","pred:  [0 2 2 0 0 3 0 2 4 3 1 4 2 2 1 2 4 3 3 3]\n","ans :  [3 2 2 3 0 3 0 4 4 0 1 3 2 2 2 1 4 3 3 3]\n","step 2890, training accuracy 0.9000\n","pred:  [3 3 0 0 4 2 1 1 4 2 0 0 1 3 2 4 2 0 0 2]\n","ans :  [3 3 0 0 4 2 1 1 2 2 0 0 1 3 2 4 2 0 0 4]\n","step 2900, training accuracy 0.9500\n","pred:  [1 4 2 0 3 2 1 1 0 2 0 1 0 3 2 1 4 4 2 0]\n","ans :  [1 4 2 0 0 2 1 1 0 2 0 1 0 3 2 1 4 4 2 0]\n","step 2910, training accuracy 0.9500\n","pred:  [3 4 1 3 2 0 2 3 0 0 0 1 4 4 2 3 4 1 0 3]\n","ans :  [3 3 1 3 2 0 2 3 0 0 0 1 4 4 2 3 4 1 0 3]\n","step 2920, training accuracy 1.0000\n","pred:  [1 0 1 3 4 3 4 1 0 0 3 3 3 0 2 1 4 3 1 2]\n","ans :  [1 0 1 3 4 3 4 1 0 0 3 3 3 0 2 1 4 3 1 2]\n","step 2930, training accuracy 1.0000\n","pred:  [1 1 0 0 0 3 1 3 1 1 2 4 3 3 3 4 0 4 3 2]\n","ans :  [1 1 0 0 0 3 1 3 1 1 2 4 3 3 3 4 0 4 3 2]\n","step 2940, training accuracy 0.9500\n","pred:  [2 0 3 4 0 1 4 2 1 4 1 2 4 1 3 2 3 1 3 0]\n","ans :  [2 0 3 2 0 1 4 2 1 4 1 2 4 1 3 2 3 1 3 0]\n","step 2950, training accuracy 0.9000\n","pred:  [0 0 3 4 2 2 3 0 3 3 1 3 4 3 2 4 3 1 2 0]\n","ans :  [0 0 3 4 2 2 0 0 3 0 1 3 4 3 2 4 3 1 2 0]\n","step 2960, training accuracy 1.0000\n","pred:  [0 0 4 0 4 1 2 4 3 3 0 4 1 1 1 1 1 4 2 1]\n","ans :  [0 0 4 0 4 1 2 4 3 3 0 4 1 1 1 1 1 4 2 1]\n","step 2970, training accuracy 0.7000\n","pred:  [2 3 3 1 0 3 3 4 3 0 2 1 4 0 0 2 3 1 1 3]\n","ans :  [2 0 0 1 0 0 3 2 0 0 2 1 4 0 0 2 0 1 1 3]\n","step 2980, training accuracy 0.8000\n","pred:  [0 2 3 4 1 3 1 3 1 0 1 0 2 4 2 3 1 0 2 2]\n","ans :  [0 2 3 4 1 0 0 3 1 0 1 0 2 2 2 0 1 0 2 2]\n","step 2990, training accuracy 0.9000\n","pred:  [3 0 0 2 0 2 3 0 3 4 3 3 4 3 2 4 1 2 2 0]\n","ans :  [3 0 0 2 0 2 3 0 3 4 0 3 4 3 2 3 1 2 2 0]\n","step 3000, training accuracy 0.9000\n","pred:  [2 4 1 0 4 2 3 0 4 2 3 4 4 1 1 2 4 1 4 2]\n","ans :  [2 4 1 0 4 2 3 0 4 2 3 4 4 1 1 2 4 1 2 4]\n","step 3010, training accuracy 0.9500\n","pred:  [4 2 1 3 1 2 1 3 0 2 3 4 2 4 1 0 0 3 2 1]\n","ans :  [2 2 1 3 1 2 1 3 0 2 3 4 2 4 1 0 0 3 2 1]\n","step 3020, training accuracy 0.9500\n","pred:  [3 1 0 2 2 3 0 1 1 2 4 4 2 3 0 0 1 3 4 0]\n","ans :  [3 1 0 2 2 3 0 1 1 2 4 4 2 3 0 0 1 3 2 0]\n","step 3030, training accuracy 0.9500\n","pred:  [0 2 3 3 3 0 4 3 1 1 1 1 2 1 4 0 4 4 2 4]\n","ans :  [0 2 3 3 3 0 4 3 1 1 1 1 2 1 4 0 2 4 2 4]\n","step 3040, training accuracy 0.8500\n","pred:  [0 0 3 3 4 3 3 3 0 0 4 1 0 2 1 1 2 1 1 2]\n","ans :  [0 0 0 3 2 3 0 3 0 0 4 1 0 2 1 1 2 1 1 2]\n","step 3050, training accuracy 0.9500\n","pred:  [4 3 4 0 2 2 4 3 0 1 4 4 0 3 2 2 3 2 4 1]\n","ans :  [4 0 4 0 2 2 4 3 0 1 4 4 0 3 2 2 3 2 4 1]\n","step 3060, training accuracy 0.9500\n","pred:  [1 2 2 2 3 4 4 0 0 4 1 1 0 1 4 1 4 0 4 2]\n","ans :  [1 2 2 2 3 2 4 0 0 4 1 1 0 1 4 1 4 0 4 2]\n","step 3070, training accuracy 1.0000\n","pred:  [0 1 0 4 2 3 2 4 0 2 1 3 2 4 1 0 3 2 0 4]\n","ans :  [0 1 0 4 2 3 2 4 0 2 1 3 2 4 1 0 3 2 0 4]\n","step 3080, training accuracy 0.9500\n","pred:  [3 1 3 0 3 0 2 1 4 4 4 4 3 1 4 2 1 1 1 0]\n","ans :  [3 1 3 0 3 0 2 1 4 4 2 4 3 1 4 2 1 1 1 0]\n","step 3090, training accuracy 0.8000\n","pred:  [4 3 3 1 4 4 3 2 0 1 2 4 1 4 0 0 4 0 2 1]\n","ans :  [4 3 3 2 4 4 3 4 3 1 4 4 1 4 0 0 4 0 2 1]\n","step 3100, training accuracy 0.9500\n","pred:  [3 2 1 4 2 3 3 2 3 1 0 4 3 0 2 0 0 4 4 2]\n","ans :  [3 2 1 4 4 3 3 2 3 1 0 4 3 0 2 0 0 4 4 2]\n","step 3110, training accuracy 0.7000\n","pred:  [2 0 0 1 2 0 3 2 1 1 3 2 4 3 1 3 2 4 0 2]\n","ans :  [4 0 0 1 4 0 3 4 1 1 3 2 3 3 0 3 2 4 0 3]\n","step 3120, training accuracy 0.9500\n","pred:  [2 1 3 2 0 2 1 1 4 0 1 4 3 4 0 3 3 4 2 1]\n","ans :  [2 1 3 2 0 2 1 1 4 0 1 4 3 4 0 0 3 4 2 1]\n","step 3130, training accuracy 0.9000\n","pred:  [2 1 4 0 2 3 3 3 0 1 2 0 3 1 3 3 2 1 3 2]\n","ans :  [2 1 4 0 2 3 3 3 3 1 4 0 3 1 3 3 2 1 3 2]\n","step 3140, training accuracy 0.9500\n","pred:  [4 1 1 0 4 4 1 4 0 2 3 2 0 2 0 2 3 2 4 2]\n","ans :  [4 1 1 0 4 4 1 4 0 2 0 2 0 2 0 2 3 2 4 2]\n","step 3150, training accuracy 0.9500\n","pred:  [1 2 0 0 3 1 2 3 4 1 0 0 4 2 2 1 1 4 1 3]\n","ans :  [1 2 0 0 0 1 2 3 4 1 0 0 4 2 2 1 1 4 1 3]\n","step 3160, training accuracy 0.9000\n","pred:  [1 2 1 3 2 3 0 3 3 3 4 0 3 4 0 0 4 0 1 2]\n","ans :  [1 2 1 3 4 3 0 3 3 3 4 0 4 4 0 0 4 0 1 2]\n","step 3170, training accuracy 0.7000\n","pred:  [0 0 1 4 4 4 2 1 0 4 1 4 3 2 0 2 0 3 4 1]\n","ans :  [0 0 1 3 4 3 4 1 0 3 1 4 0 2 0 3 0 3 4 1]\n","step 3180, training accuracy 0.8000\n","pred:  [4 4 1 3 0 0 1 4 0 2 0 4 0 2 3 4 2 3 0 2]\n","ans :  [2 4 1 3 0 0 1 4 0 3 0 4 0 2 3 4 2 0 0 0]\n","step 3190, training accuracy 0.9500\n","pred:  [1 3 2 2 3 2 1 3 0 4 0 4 1 2 1 4 3 2 0 3]\n","ans :  [1 3 2 2 3 2 1 3 0 4 0 4 1 2 1 2 3 2 0 3]\n","step 3200, training accuracy 0.9500\n","pred:  [4 1 0 0 3 2 4 0 2 3 1 3 1 4 3 3 3 0 1 3]\n","ans :  [4 1 0 0 3 2 4 0 2 3 1 3 1 4 0 3 3 0 1 3]\n","step 3210, training accuracy 1.0000\n","pred:  [1 1 4 4 1 4 0 4 3 0 4 1 1 0 3 0 0 2 0 4]\n","ans :  [1 1 4 4 1 4 0 4 3 0 4 1 1 0 3 0 0 2 0 4]\n","step 3220, training accuracy 0.9000\n","pred:  [2 0 2 1 0 0 2 3 1 2 0 4 4 0 4 4 3 3 0 1]\n","ans :  [2 3 2 1 0 0 2 4 1 2 0 4 4 0 4 4 3 3 0 1]\n","step 3230, training accuracy 0.9000\n","pred:  [0 1 3 4 1 3 2 2 1 1 2 3 1 2 3 2 4 3 0 2]\n","ans :  [0 1 3 2 1 3 2 2 1 1 2 3 1 2 0 2 4 3 0 2]\n","step 3240, training accuracy 0.9000\n","pred:  [4 2 2 2 0 1 1 3 0 1 2 4 3 3 1 3 1 1 0 4]\n","ans :  [2 2 2 2 0 1 1 3 0 1 2 4 3 3 1 0 1 1 0 4]\n","step 3250, training accuracy 0.9000\n","pred:  [2 4 2 1 3 0 3 4 2 2 3 3 2 2 4 4 2 0 0 1]\n","ans :  [2 4 2 1 3 0 3 4 2 2 3 3 4 2 4 4 2 0 0 2]\n","step 3260, training accuracy 0.7500\n","pred:  [4 0 3 3 3 0 3 3 1 1 2 4 0 3 2 0 4 4 0 0]\n","ans :  [4 3 3 3 3 0 3 3 1 1 2 4 0 4 0 3 4 3 0 0]\n","step 3270, training accuracy 0.8500\n","pred:  [2 1 4 2 3 0 2 1 2 4 3 4 2 3 2 4 3 4 4 0]\n","ans :  [2 1 2 2 3 3 2 1 2 4 3 3 2 3 2 4 3 4 4 0]\n","step 3280, training accuracy 1.0000\n","pred:  [1 3 0 3 2 2 4 4 0 1 1 3 3 3 1 2 0 4 4 1]\n","ans :  [1 3 0 3 2 2 4 4 0 1 1 3 3 3 1 2 0 4 4 1]\n","step 3290, training accuracy 0.9000\n","pred:  [1 4 3 2 1 2 1 1 2 0 3 1 3 0 3 4 1 3 0 4]\n","ans :  [1 4 3 2 1 2 1 1 0 0 3 1 3 0 3 4 1 0 0 4]\n","step 3300, training accuracy 0.8500\n","pred:  [0 3 2 2 1 1 2 4 3 3 4 0 1 1 2 4 0 0 3 3]\n","ans :  [0 3 2 2 1 1 4 4 3 3 2 0 1 1 2 4 0 0 0 3]\n","step 3310, training accuracy 0.9500\n","pred:  [4 4 4 4 0 1 1 4 1 3 4 3 0 1 2 4 1 2 4 2]\n","ans :  [4 4 4 4 0 1 1 4 1 3 4 3 0 1 2 4 1 2 3 2]\n","step 3320, training accuracy 0.9000\n","pred:  [1 3 2 2 1 4 2 1 3 1 2 2 4 0 3 3 3 4 4 0]\n","ans :  [1 4 2 2 1 4 2 1 3 1 4 2 4 0 3 3 3 4 4 0]\n","step 3330, training accuracy 0.9500\n","pred:  [3 0 3 4 2 3 0 3 2 2 1 2 3 3 2 1 1 4 3 2]\n","ans :  [3 0 3 4 2 3 0 3 2 2 1 2 0 3 2 1 1 4 3 2]\n","step 3340, training accuracy 0.9500\n","pred:  [3 4 1 0 1 2 1 0 2 3 2 0 1 3 2 4 2 4 1 2]\n","ans :  [3 4 1 1 1 2 1 0 2 3 2 0 1 3 2 4 2 4 1 2]\n","step 3350, training accuracy 0.7500\n","pred:  [4 4 4 0 1 0 4 2 2 0 1 4 4 3 2 4 1 1 0 0]\n","ans :  [4 4 4 3 1 3 4 2 2 3 1 3 4 3 2 4 2 1 0 0]\n","step 3360, training accuracy 0.9500\n","pred:  [0 1 2 4 0 3 1 1 3 4 3 0 4 4 1 2 1 3 2 3]\n","ans :  [0 1 2 4 0 3 1 1 3 4 3 0 4 2 1 2 1 3 2 3]\n","step 3370, training accuracy 0.9000\n","pred:  [2 4 1 4 3 4 4 1 0 2 3 4 0 2 0 3 0 1 4 2]\n","ans :  [2 4 1 4 3 4 4 1 0 2 3 4 0 2 0 0 0 1 2 2]\n","step 3380, training accuracy 0.9500\n","pred:  [4 4 0 3 1 2 1 4 2 2 3 3 1 0 1 4 2 2 1 2]\n","ans :  [4 4 0 3 1 2 1 4 2 2 3 3 1 3 1 4 2 2 1 2]\n","step 3390, training accuracy 0.8500\n","pred:  [0 1 1 4 0 0 1 4 4 1 0 1 1 1 3 4 3 1 3 0]\n","ans :  [1 1 1 4 0 0 1 2 2 1 0 1 1 1 3 4 3 1 3 0]\n","step 3400, training accuracy 0.8500\n","pred:  [3 3 4 1 1 3 1 0 0 1 2 1 3 3 3 4 2 0 4 4]\n","ans :  [0 0 4 1 1 3 1 0 0 1 2 1 0 3 3 4 2 0 4 4]\n","step 3410, training accuracy 0.9500\n","pred:  [0 4 4 2 4 4 1 2 1 3 3 2 4 3 3 2 3 1 0 1]\n","ans :  [0 4 4 2 4 4 1 4 1 3 3 2 4 3 3 2 3 1 0 1]\n","step 3420, training accuracy 0.9500\n","pred:  [1 4 4 4 1 2 4 2 0 4 3 1 1 2 4 1 4 1 0 0]\n","ans :  [1 4 4 4 1 2 4 2 0 2 3 1 1 2 4 1 4 1 0 0]\n","step 3430, training accuracy 0.8000\n","pred:  [2 2 0 0 4 4 1 0 4 3 1 0 4 4 0 0 2 4 4 2]\n","ans :  [2 4 0 0 2 4 1 3 4 3 1 3 4 4 0 0 2 4 4 2]\n","step 3440, training accuracy 0.8000\n","pred:  [0 0 3 3 1 0 2 2 3 1 1 3 2 4 0 2 4 1 3 0]\n","ans :  [0 0 3 0 1 0 2 1 0 1 1 3 2 4 0 2 2 1 3 0]\n","step 3450, training accuracy 0.8500\n","pred:  [0 4 4 2 3 4 4 3 3 4 1 3 0 2 0 0 0 4 1 1]\n","ans :  [0 3 4 2 3 4 4 3 3 2 1 3 0 2 0 0 0 4 1 0]\n","step 3460, training accuracy 0.8500\n","pred:  [2 4 0 2 2 1 4 4 0 4 4 4 1 4 4 1 4 1 0 1]\n","ans :  [2 4 0 2 2 1 4 4 0 4 4 4 1 2 2 1 4 1 3 1]\n","step 3470, training accuracy 0.8000\n","pred:  [0 4 1 0 0 3 2 3 3 4 4 3 2 0 1 3 0 3 0 2]\n","ans :  [0 3 1 0 0 3 2 0 3 2 4 3 2 0 1 4 0 3 0 2]\n","step 3480, training accuracy 0.7500\n","pred:  [1 3 0 1 2 3 2 0 1 4 4 3 4 1 3 3 4 0 1 4]\n","ans :  [1 0 0 1 2 0 2 0 1 4 4 3 3 1 0 3 3 0 1 4]\n","step 3490, training accuracy 1.0000\n","pred:  [3 2 4 0 2 1 1 4 3 3 1 2 4 4 0 3 1 0 2 3]\n","ans :  [3 2 4 0 2 1 1 4 3 3 1 2 4 4 0 3 1 0 2 3]\n","step 3500, training accuracy 0.9500\n","pred:  [1 1 4 3 3 2 2 2 0 0 1 3 4 0 1 3 4 0 0 1]\n","ans :  [1 1 4 3 3 2 2 2 0 0 1 3 2 0 1 3 4 0 0 1]\n","step 3510, training accuracy 0.9000\n","pred:  [1 3 3 3 4 2 0 4 0 0 3 1 4 1 2 1 0 2 1 1]\n","ans :  [1 3 3 3 2 2 0 4 0 0 0 1 4 1 2 1 0 2 1 1]\n","step 3520, training accuracy 0.8500\n","pred:  [1 3 2 1 4 0 2 2 2 4 0 3 0 4 1 2 4 3 1 2]\n","ans :  [1 3 2 1 4 3 2 2 2 4 0 3 0 4 1 4 4 0 1 2]\n","step 3530, training accuracy 0.9000\n","pred:  [0 4 2 1 0 4 0 1 4 4 4 0 3 0 4 0 1 4 0 2]\n","ans :  [0 4 2 1 0 4 3 1 4 4 4 0 3 0 4 0 1 2 0 2]\n","step 3540, training accuracy 1.0000\n","pred:  [0 2 0 3 0 3 3 0 1 4 2 3 4 4 2 1 3 1 4 1]\n","ans :  [0 2 0 3 0 3 3 0 1 4 2 3 4 4 2 1 3 1 4 1]\n","step 3550, training accuracy 0.9500\n","pred:  [1 3 1 0 0 2 4 2 4 3 0 2 1 2 1 3 3 4 4 2]\n","ans :  [1 3 1 0 3 2 4 2 4 3 0 2 1 2 1 3 3 4 4 2]\n","step 3560, training accuracy 0.8500\n","pred:  [1 1 3 3 4 4 0 4 2 1 3 3 4 1 0 0 3 0 4 1]\n","ans :  [1 1 0 0 4 4 0 4 2 1 3 3 4 1 0 0 3 0 2 1]\n","step 3570, training accuracy 0.9000\n","pred:  [3 4 4 3 0 0 0 0 4 4 1 4 3 1 1 1 3 4 0 1]\n","ans :  [3 4 2 3 0 0 0 0 2 4 1 4 3 1 1 1 3 4 0 1]\n","step 3580, training accuracy 0.9500\n","pred:  [3 4 3 3 1 1 4 1 0 1 4 2 0 0 4 1 3 3 4 0]\n","ans :  [3 4 3 3 1 1 4 1 0 1 4 2 0 0 4 1 4 3 4 0]\n","step 3590, training accuracy 0.8500\n","pred:  [4 4 2 4 3 1 0 4 4 2 0 0 0 1 2 0 1 3 1 4]\n","ans :  [4 4 2 3 3 1 0 3 4 2 0 0 3 1 2 0 1 3 1 4]\n","step 3600, training accuracy 0.9500\n","pred:  [3 1 3 2 3 3 2 1 4 2 0 0 3 1 0 0 2 3 2 1]\n","ans :  [3 1 3 2 0 3 2 1 4 2 0 0 3 1 0 0 2 3 2 1]\n","step 3610, training accuracy 0.9500\n","pred:  [1 1 0 0 3 4 2 0 1 4 0 2 1 1 3 4 0 2 1 3]\n","ans :  [1 1 0 0 3 4 2 3 1 4 0 2 1 1 3 4 0 2 1 3]\n","step 3620, training accuracy 0.9500\n","pred:  [1 1 3 4 2 3 0 4 1 3 0 1 4 2 4 3 3 0 0 4]\n","ans :  [1 1 3 3 2 3 0 4 1 3 0 1 4 2 4 3 3 0 0 4]\n","step 3630, training accuracy 0.9000\n","pred:  [0 3 3 1 4 3 2 2 1 3 0 4 2 2 1 0 0 4 0 4]\n","ans :  [0 3 3 1 4 3 2 2 1 3 0 4 2 2 1 0 0 2 0 3]\n","step 3640, training accuracy 0.9000\n","pred:  [2 1 4 2 0 3 0 4 4 0 4 1 3 2 2 1 3 1 2 0]\n","ans :  [2 1 4 2 0 3 0 4 4 0 4 1 0 2 2 1 4 1 2 0]\n","step 3650, training accuracy 0.8500\n","pred:  [0 1 2 0 0 3 4 4 0 2 1 2 1 2 3 3 3 3 3 3]\n","ans :  [0 1 2 0 0 3 4 4 0 2 1 2 0 2 0 3 3 3 0 3]\n","step 3660, training accuracy 0.8500\n","pred:  [3 4 1 2 4 0 4 3 2 0 1 3 1 1 1 3 3 2 0 1]\n","ans :  [3 2 1 2 4 3 4 3 2 0 1 4 1 1 1 3 3 2 0 1]\n","step 3670, training accuracy 1.0000\n","pred:  [3 1 1 2 1 3 0 0 3 0 3 1 2 2 0 0 1 2 2 2]\n","ans :  [3 1 1 2 1 3 0 0 3 0 3 1 2 2 0 0 1 2 2 2]\n","step 3680, training accuracy 0.9000\n","pred:  [4 2 3 2 0 0 1 2 0 1 3 0 3 2 4 4 2 1 3 4]\n","ans :  [4 2 3 2 0 3 1 2 3 1 3 0 3 2 4 4 2 1 3 4]\n","step 3690, training accuracy 0.9500\n","pred:  [2 0 3 1 4 1 1 3 0 2 0 4 4 3 0 0 0 2 3 4]\n","ans :  [2 0 3 1 4 1 1 3 0 2 0 4 4 3 0 0 0 2 4 4]\n","step 3700, training accuracy 0.9500\n","pred:  [4 0 1 0 1 4 4 4 2 4 3 1 3 1 3 4 4 0 1 3]\n","ans :  [4 0 1 0 1 4 4 4 4 4 3 1 3 1 3 4 4 0 1 3]\n","step 3710, training accuracy 0.9000\n","pred:  [1 1 4 0 3 3 3 4 0 3 0 1 1 4 2 4 1 2 2 4]\n","ans :  [1 1 4 0 3 3 3 4 0 3 3 1 1 4 1 4 1 2 2 4]\n","step 3720, training accuracy 0.8500\n","pred:  [3 0 4 4 0 1 3 1 4 2 0 4 0 2 1 3 1 1 2 1]\n","ans :  [3 3 2 4 0 1 3 1 2 2 0 4 0 2 1 3 1 1 2 1]\n","step 3730, training accuracy 0.9000\n","pred:  [2 2 3 0 3 3 0 3 4 0 0 1 1 3 3 3 1 3 4 4]\n","ans :  [2 2 3 0 3 3 0 3 4 0 0 1 1 0 3 3 1 3 2 4]\n","step 3740, training accuracy 0.9000\n","pred:  [2 2 2 2 3 2 1 0 4 3 2 1 1 0 0 3 4 0 2 4]\n","ans :  [2 2 2 2 3 2 1 0 4 3 2 1 1 0 1 3 4 0 1 4]\n","step 3750, training accuracy 0.9000\n","pred:  [1 1 4 1 4 1 1 0 0 1 3 2 2 0 1 4 3 1 2 2]\n","ans :  [1 1 4 1 4 1 1 1 0 1 4 2 2 0 1 4 3 1 2 2]\n","step 3760, training accuracy 0.8500\n","pred:  [0 3 2 1 0 1 3 3 1 1 4 0 4 2 4 3 1 3 3 2]\n","ans :  [0 3 2 1 0 1 3 3 1 1 3 0 4 2 4 3 1 0 3 4]\n","step 3770, training accuracy 0.9500\n","pred:  [3 0 3 4 4 4 4 3 4 0 3 3 1 3 0 3 4 1 2 1]\n","ans :  [3 0 3 4 4 4 4 3 3 0 3 3 1 3 0 3 4 1 2 1]\n","step 3780, training accuracy 0.9000\n","pred:  [1 2 4 0 0 4 1 0 2 3 3 4 2 3 2 1 3 3 1 4]\n","ans :  [1 2 4 0 0 4 1 3 2 3 3 4 2 3 2 1 0 3 1 4]\n","step 3790, training accuracy 0.9000\n","pred:  [4 0 0 3 2 4 2 1 3 1 2 0 4 0 4 1 1 2 4 3]\n","ans :  [4 0 0 3 2 4 2 1 3 1 4 3 4 0 4 1 1 2 4 3]\n","step 3800, training accuracy 0.9000\n","pred:  [1 2 0 3 3 2 2 0 4 0 4 2 0 4 0 1 4 4 3 1]\n","ans :  [1 2 0 3 3 2 2 0 3 0 4 2 0 4 0 1 2 4 3 1]\n","step 3810, training accuracy 0.9000\n","pred:  [4 1 0 4 3 4 3 4 1 4 2 0 1 3 2 2 4 2 0 3]\n","ans :  [4 1 3 4 3 4 0 4 1 4 2 0 1 3 2 2 4 2 0 3]\n","step 3820, training accuracy 0.9500\n","pred:  [1 2 4 1 2 2 3 0 0 2 4 4 1 0 3 4 0 1 1 0]\n","ans :  [1 2 4 1 2 2 3 0 0 2 4 4 1 0 0 4 0 1 1 0]\n","step 3830, training accuracy 0.9500\n","pred:  [2 4 0 2 2 0 3 2 3 0 4 0 1 0 4 2 1 1 3 1]\n","ans :  [2 4 0 4 2 0 3 2 3 0 4 0 1 0 4 2 1 1 3 1]\n","step 3840, training accuracy 0.9500\n","pred:  [1 2 4 1 0 2 1 1 2 0 1 4 4 4 4 3 0 1 3 1]\n","ans :  [1 2 4 1 0 2 1 1 2 0 1 4 4 2 4 3 0 1 3 1]\n","step 3850, training accuracy 0.8500\n","pred:  [2 4 1 1 0 0 3 4 4 0 1 1 1 3 4 4 4 1 2 2]\n","ans :  [2 4 1 1 0 0 0 4 2 0 1 1 1 3 4 4 2 1 2 2]\n","step 3860, training accuracy 0.9000\n","pred:  [0 3 4 2 0 4 0 2 2 1 0 0 1 1 2 4 3 2 2 4]\n","ans :  [0 3 4 2 0 2 0 2 2 1 0 0 1 1 2 4 0 2 2 4]\n","step 3870, training accuracy 0.9000\n","pred:  [0 4 1 1 4 0 1 1 2 2 1 0 4 1 0 4 2 0 3 1]\n","ans :  [0 4 1 1 4 0 1 1 1 2 1 0 2 1 0 4 2 0 3 1]\n","step 3880, training accuracy 0.7000\n","pred:  [2 4 4 2 4 0 1 4 2 2 2 3 0 0 0 3 2 1 1 0]\n","ans :  [2 4 3 3 4 0 1 3 2 3 2 3 0 3 0 3 4 1 1 0]\n","step 3890, training accuracy 0.9500\n","pred:  [3 4 4 0 0 0 1 1 3 0 4 2 2 0 2 1 3 3 3 1]\n","ans :  [3 4 4 0 0 0 1 1 3 0 4 4 2 0 2 1 3 3 3 1]\n","step 3900, training accuracy 0.8500\n","pred:  [1 3 1 4 2 3 2 0 1 3 0 3 2 0 2 4 3 2 4 2]\n","ans :  [1 0 1 4 2 3 2 1 1 3 0 3 2 0 4 4 3 2 4 2]\n","step 3910, training accuracy 0.9000\n","pred:  [3 2 1 1 4 3 3 1 0 0 0 4 3 3 0 4 4 1 2 4]\n","ans :  [3 2 1 1 4 3 3 1 0 0 3 4 3 3 3 4 4 1 2 4]\n","step 3920, training accuracy 1.0000\n","pred:  [2 3 2 0 2 3 4 3 2 4 3 0 0 0 3 3 0 1 1 1]\n","ans :  [2 3 2 0 2 3 4 3 2 4 3 0 0 0 3 3 0 1 1 1]\n","step 3930, training accuracy 0.9000\n","pred:  [4 4 4 0 2 1 3 3 3 4 4 1 1 2 0 4 0 4 1 1]\n","ans :  [4 4 4 3 2 1 3 3 3 4 4 1 1 2 0 4 3 4 1 1]\n","step 3940, training accuracy 0.8500\n","pred:  [2 4 2 4 1 1 2 0 3 3 4 3 1 2 4 1 0 3 2 1]\n","ans :  [2 4 2 2 1 1 2 0 3 4 4 3 1 2 4 1 0 4 2 1]\n","step 3950, training accuracy 0.8500\n","pred:  [3 4 2 2 0 3 1 4 3 0 0 1 1 3 1 4 3 4 1 2]\n","ans :  [3 4 2 2 1 3 1 4 3 0 0 1 1 4 1 4 0 4 1 2]\n","step 3960, training accuracy 0.8500\n","pred:  [1 2 3 0 2 2 4 4 0 2 3 3 0 0 2 4 4 4 2 0]\n","ans :  [1 2 3 0 2 4 4 4 0 2 3 3 0 3 2 4 4 4 4 0]\n","step 3970, training accuracy 0.9500\n","pred:  [4 1 0 1 0 4 0 0 2 0 2 3 3 1 2 3 1 3 3 1]\n","ans :  [4 1 0 1 0 4 0 0 2 0 2 3 0 1 2 3 1 3 3 1]\n","step 3980, training accuracy 0.8500\n","pred:  [4 4 4 3 0 4 2 2 4 3 3 3 2 1 1 2 1 1 3 2]\n","ans :  [4 4 2 3 0 2 2 2 4 3 3 3 2 1 1 4 1 1 3 2]\n","step 3990, training accuracy 0.9500\n","pred:  [0 2 4 1 3 4 2 0 0 1 4 1 3 2 3 3 4 3 0 4]\n","ans :  [0 2 4 1 3 4 2 0 0 1 4 1 3 2 4 3 4 3 0 4]\n","step 4000, training accuracy 0.8500\n","pred:  [1 3 2 1 2 2 1 2 4 0 3 0 3 4 1 3 3 0 4 0]\n","ans :  [1 3 2 1 1 2 1 2 4 0 3 0 0 2 1 3 3 0 4 0]\n","step 4010, training accuracy 1.0000\n","pred:  [4 2 1 4 1 0 4 1 3 2 2 2 0 1 4 3 3 0 0 0]\n","ans :  [4 2 1 4 1 0 4 1 3 2 2 2 0 1 4 3 3 0 0 0]\n","step 4020, training accuracy 1.0000\n","pred:  [0 3 2 2 0 0 4 4 3 3 4 0 1 4 0 4 1 4 2 4]\n","ans :  [0 3 2 2 0 0 4 4 3 3 4 0 1 4 0 4 1 4 2 4]\n","step 4030, training accuracy 0.9500\n","pred:  [2 0 4 2 3 2 1 2 3 0 1 2 1 0 3 1 3 1 1 0]\n","ans :  [2 0 4 2 0 2 1 2 3 0 1 2 1 0 3 1 3 1 1 0]\n","step 4040, training accuracy 0.9000\n","pred:  [4 0 1 3 2 3 1 3 2 0 3 3 2 1 2 0 0 1 4 2]\n","ans :  [4 0 1 3 2 3 1 3 2 0 3 0 2 1 2 0 0 1 2 2]\n","step 4050, training accuracy 1.0000\n","pred:  [4 3 4 2 4 4 2 0 3 0 1 2 2 2 1 0 3 3 2 3]\n","ans :  [4 3 4 2 4 4 2 0 3 0 1 2 2 2 1 0 3 3 2 3]\n","step 4060, training accuracy 0.9500\n","pred:  [4 4 0 3 2 1 0 2 4 4 1 3 2 2 4 1 1 1 2 4]\n","ans :  [2 4 0 3 2 1 0 2 4 4 1 3 2 2 4 1 1 1 2 4]\n","step 4070, training accuracy 0.9000\n","pred:  [3 1 4 3 1 1 2 2 1 4 3 2 4 0 1 1 4 0 4 4]\n","ans :  [3 1 2 3 1 1 2 4 1 4 3 2 4 0 1 1 4 0 4 4]\n","step 4080, training accuracy 1.0000\n","pred:  [0 4 1 4 1 4 4 0 0 3 1 1 4 2 1 0 2 0 2 2]\n","ans :  [0 4 1 4 1 4 4 0 0 3 1 1 4 2 1 0 2 0 2 2]\n","step 4090, training accuracy 0.7000\n","pred:  [4 1 3 1 1 4 3 4 1 2 2 0 0 2 0 2 4 0 1 0]\n","ans :  [4 1 3 1 1 4 1 2 1 2 1 0 0 1 0 1 2 0 1 0]\n","step 4100, training accuracy 0.9000\n","pred:  [2 3 1 4 4 2 2 2 4 0 1 2 2 1 1 1 0 3 0 1]\n","ans :  [2 3 1 2 4 2 2 2 4 0 1 2 2 1 1 1 3 3 0 1]\n","step 4110, training accuracy 0.8500\n","pred:  [2 1 4 0 0 1 2 2 2 3 1 0 1 4 0 3 1 3 0 3]\n","ans :  [2 1 4 0 0 1 2 2 1 3 1 0 1 4 0 3 1 0 0 2]\n","step 4120, training accuracy 0.9500\n","pred:  [3 0 2 3 1 3 1 4 2 0 4 1 2 0 3 1 0 0 2 0]\n","ans :  [0 0 2 3 1 3 1 4 2 0 4 1 2 0 3 1 0 0 2 0]\n","step 4130, training accuracy 1.0000\n","pred:  [3 1 3 0 0 3 4 0 0 1 4 2 0 4 3 4 1 3 2 4]\n","ans :  [3 1 3 0 0 3 4 0 0 1 4 2 0 4 3 4 1 3 2 4]\n","step 4140, training accuracy 0.9500\n","pred:  [4 0 0 1 0 2 3 1 2 4 2 0 1 4 4 0 2 4 1 1]\n","ans :  [4 0 0 1 0 2 3 1 2 4 2 0 1 2 4 0 2 4 1 1]\n","step 4150, training accuracy 0.8500\n","pred:  [0 4 3 3 1 1 2 4 0 4 0 0 1 4 2 1 3 1 4 3]\n","ans :  [0 4 3 3 1 1 4 2 0 4 3 0 1 4 2 1 3 1 4 3]\n","step 4160, training accuracy 0.9000\n","pred:  [1 1 2 4 4 3 0 4 3 1 2 4 4 1 1 2 4 1 4 4]\n","ans :  [1 1 2 4 4 4 0 4 3 1 2 4 2 1 1 2 4 1 4 4]\n","step 4170, training accuracy 1.0000\n","pred:  [4 4 3 4 0 3 0 1 0 1 0 4 1 0 2 2 0 3 4 3]\n","ans :  [4 4 3 4 0 3 0 1 0 1 0 4 1 0 2 2 0 3 4 3]\n","step 4180, training accuracy 0.9000\n","pred:  [0 3 2 1 4 1 3 2 2 3 2 2 4 0 0 1 0 4 4 0]\n","ans :  [0 3 2 1 4 1 3 2 2 3 2 2 3 0 0 3 0 4 4 0]\n","step 4190, training accuracy 0.9000\n","pred:  [2 1 2 2 3 2 4 0 4 3 4 3 0 2 4 2 3 0 4 0]\n","ans :  [2 1 2 2 3 4 4 0 4 3 4 3 0 2 4 2 3 0 4 3]\n","step 4200, training accuracy 1.0000\n","pred:  [3 2 3 1 1 2 0 2 3 1 1 0 2 4 4 3 3 4 4 0]\n","ans :  [3 2 3 1 1 2 0 2 3 1 1 0 2 4 4 3 3 4 4 0]\n","step 4210, training accuracy 0.8500\n","pred:  [3 1 2 3 0 1 3 1 4 1 1 4 4 3 1 1 4 0 0 3]\n","ans :  [0 1 2 3 0 1 0 1 4 1 1 4 2 3 1 1 4 0 0 3]\n","step 4220, training accuracy 1.0000\n","pred:  [2 4 1 1 1 1 3 0 2 0 0 4 4 0 3 4 4 4 1 3]\n","ans :  [2 4 1 1 1 1 3 0 2 0 0 4 4 0 3 4 4 4 1 3]\n","step 4230, training accuracy 0.8500\n","pred:  [1 2 2 1 1 2 3 0 3 3 2 0 4 4 4 2 4 1 4 2]\n","ans :  [2 2 4 1 1 2 3 0 3 3 2 0 3 4 4 2 4 1 4 2]\n","step 4240, training accuracy 0.9000\n","pred:  [1 2 3 0 3 0 4 4 2 4 3 2 1 3 4 4 0 3 0 4]\n","ans :  [1 2 3 0 0 0 3 4 2 4 3 2 1 3 4 4 0 3 0 4]\n","step 4250, training accuracy 0.9500\n","pred:  [3 1 4 0 0 3 0 1 2 0 3 2 4 2 1 4 2 3 4 1]\n","ans :  [3 1 4 0 0 3 0 1 2 0 4 2 4 2 1 4 2 3 4 1]\n","step 4260, training accuracy 0.9500\n","pred:  [0 4 1 4 4 1 0 3 3 0 1 4 1 1 3 2 3 0 0 2]\n","ans :  [0 4 1 4 4 1 0 3 0 0 1 4 1 1 3 2 3 0 0 2]\n","step 4270, training accuracy 0.8500\n","pred:  [4 2 0 2 1 2 4 3 2 3 4 4 4 0 0 3 2 0 2 0]\n","ans :  [4 2 0 2 1 2 4 3 4 3 3 4 4 0 0 3 2 0 4 0]\n","step 4280, training accuracy 0.8500\n","pred:  [4 3 4 1 2 2 4 2 2 1 0 1 0 2 3 1 3 0 3 1]\n","ans :  [2 3 4 1 2 2 2 2 2 1 0 1 0 2 3 1 3 0 0 1]\n","step 4290, training accuracy 0.8500\n","pred:  [1 0 3 4 0 4 4 2 3 0 4 0 1 0 2 2 1 1 2 2]\n","ans :  [1 0 3 4 0 2 4 2 0 0 4 0 1 0 2 2 1 1 1 2]\n","step 4300, training accuracy 1.0000\n","pred:  [1 0 4 0 4 2 1 4 2 2 3 3 2 1 4 3 3 2 0 4]\n","ans :  [1 0 4 0 4 2 1 4 2 2 3 3 2 1 4 3 3 2 0 4]\n","step 4310, training accuracy 0.9500\n","pred:  [1 3 2 3 0 1 4 1 4 4 2 1 0 4 1 3 4 0 3 2]\n","ans :  [1 3 2 3 0 1 3 1 4 4 2 1 0 4 1 3 4 0 3 2]\n","step 4320, training accuracy 0.8500\n","pred:  [2 2 3 2 0 2 0 4 1 0 1 4 4 0 4 3 3 3 1 2]\n","ans :  [2 2 3 2 0 2 0 4 1 0 1 4 4 3 4 3 3 0 1 1]\n","step 4330, training accuracy 1.0000\n","pred:  [4 3 2 0 1 4 3 0 0 2 4 4 3 1 2 1 1 0 1 0]\n","ans :  [4 3 2 0 1 4 3 0 0 2 4 4 3 1 2 1 1 0 1 0]\n","step 4340, training accuracy 0.9000\n","pred:  [4 4 0 0 1 3 3 1 1 4 0 0 1 4 3 3 1 4 1 4]\n","ans :  [4 4 0 0 1 3 3 1 1 4 0 0 1 2 3 3 1 4 1 2]\n","step 4350, training accuracy 1.0000\n","pred:  [1 0 2 1 0 3 2 4 4 3 1 2 1 3 0 1 4 4 1 3]\n","ans :  [1 0 2 1 0 3 2 4 4 3 1 2 1 3 0 1 4 4 1 3]\n","step 4360, training accuracy 1.0000\n","pred:  [0 0 2 2 0 2 1 3 4 1 2 2 3 4 1 0 3 1 0 4]\n","ans :  [0 0 2 2 0 2 1 3 4 1 2 2 3 4 1 0 3 1 0 4]\n","step 4370, training accuracy 0.9500\n","pred:  [4 4 1 4 3 4 0 2 2 1 3 2 1 1 3 2 1 0 1 0]\n","ans :  [4 4 1 3 3 4 0 2 2 1 3 2 1 1 3 2 1 0 1 0]\n","step 4380, training accuracy 1.0000\n","pred:  [1 3 2 4 3 4 0 4 4 2 0 4 0 4 2 1 4 0 0 0]\n","ans :  [1 3 2 4 3 4 0 4 4 2 0 4 0 4 2 1 4 0 0 0]\n","step 4390, training accuracy 0.8000\n","pred:  [1 4 0 3 2 2 0 2 2 3 3 3 3 0 1 4 0 3 1 0]\n","ans :  [1 4 0 0 2 2 0 2 2 3 3 3 0 0 1 3 0 0 1 0]\n","step 4400, training accuracy 0.9500\n","pred:  [4 4 3 2 1 2 2 0 1 0 1 4 3 0 1 2 4 4 4 3]\n","ans :  [4 4 3 2 1 2 4 0 1 0 1 4 3 0 1 2 4 4 4 3]\n","step 4410, training accuracy 0.9500\n","pred:  [3 4 2 4 2 4 1 2 2 1 4 3 0 0 1 4 3 4 1 0]\n","ans :  [4 4 2 4 2 4 1 2 2 1 4 3 0 0 1 4 3 4 1 0]\n","step 4420, training accuracy 0.8000\n","pred:  [3 2 3 4 2 0 4 4 4 1 2 1 0 3 4 3 2 4 0 2]\n","ans :  [0 2 3 4 2 0 3 4 4 1 4 1 1 3 4 3 2 4 0 2]\n","step 4430, training accuracy 0.9500\n","pred:  [2 3 4 2 1 0 3 4 1 3 0 1 0 4 1 0 0 1 4 3]\n","ans :  [2 3 3 2 1 0 3 4 1 3 0 1 0 4 1 0 0 1 4 3]\n","step 4440, training accuracy 0.9500\n","pred:  [4 1 1 4 3 1 3 2 0 1 4 3 2 1 3 4 2 4 2 3]\n","ans :  [4 1 1 4 0 1 3 2 0 1 4 3 2 1 3 4 2 4 2 3]\n","step 4450, training accuracy 1.0000\n","pred:  [0 3 0 1 4 4 1 4 2 4 4 3 3 4 4 0 2 1 1 0]\n","ans :  [0 3 0 1 4 4 1 4 2 4 4 3 3 4 4 0 2 1 1 0]\n","step 4460, training accuracy 0.8500\n","pred:  [4 4 3 4 1 1 2 0 3 4 4 2 3 3 2 1 4 2 3 1]\n","ans :  [2 4 3 2 1 1 2 0 3 4 4 2 3 3 2 1 2 2 3 1]\n","step 4470, training accuracy 0.9500\n","pred:  [3 1 0 4 0 0 4 3 2 0 2 4 2 1 1 3 0 4 4 3]\n","ans :  [3 1 0 4 0 0 4 3 2 0 2 4 2 1 1 3 0 4 3 3]\n","step 4480, training accuracy 1.0000\n","pred:  [3 1 1 0 4 4 1 1 0 3 2 0 2 1 4 3 1 4 2 0]\n","ans :  [3 1 1 0 4 4 1 1 0 3 2 0 2 1 4 3 1 4 2 0]\n","step 4490, training accuracy 1.0000\n","pred:  [0 4 1 1 0 3 4 0 0 2 2 0 2 2 3 0 4 1 2 4]\n","ans :  [0 4 1 1 0 3 4 0 0 2 2 0 2 2 3 0 4 1 2 4]\n","step 4500, training accuracy 0.8500\n","pred:  [2 0 3 0 0 2 1 4 0 3 0 3 2 4 2 0 2 4 3 4]\n","ans :  [2 0 3 0 0 4 1 4 0 3 0 3 3 4 4 0 2 4 3 4]\n","step 4510, training accuracy 0.9000\n","pred:  [0 3 2 0 0 1 4 3 1 4 4 2 1 3 3 0 0 1 4 2]\n","ans :  [0 3 2 0 0 1 4 3 1 3 4 2 1 3 3 0 0 1 4 4]\n","step 4520, training accuracy 0.8500\n","pred:  [4 0 3 2 0 1 4 2 1 0 3 1 1 4 1 0 3 4 4 3]\n","ans :  [4 0 3 1 0 1 2 2 1 0 3 1 1 2 1 0 3 4 4 3]\n","step 4530, training accuracy 0.9000\n","pred:  [0 3 0 0 0 1 0 2 4 2 3 3 2 4 1 1 3 4 2 3]\n","ans :  [0 3 0 0 0 1 0 2 3 2 3 3 2 4 1 1 3 4 4 3]\n","step 4540, training accuracy 0.9500\n","pred:  [4 4 3 1 4 2 2 1 0 3 2 3 2 0 3 3 3 0 1 1]\n","ans :  [4 4 3 1 4 2 2 1 0 3 2 3 2 0 3 3 0 0 1 1]\n","step 4550, training accuracy 0.9000\n","pred:  [1 2 1 4 0 0 1 0 1 1 0 1 3 2 4 3 0 4 2 0]\n","ans :  [1 2 1 3 0 0 1 0 1 1 0 1 3 2 4 3 0 2 2 0]\n","step 4560, training accuracy 0.9500\n","pred:  [1 0 3 4 3 2 4 0 4 1 0 3 3 1 0 1 3 0 4 3]\n","ans :  [1 0 3 4 3 2 4 0 4 1 0 3 3 1 0 1 0 0 4 3]\n","step 4570, training accuracy 0.8500\n","pred:  [0 3 2 3 1 3 0 4 2 3 4 4 2 1 3 1 4 4 0 4]\n","ans :  [1 3 2 3 1 3 0 4 2 3 4 4 2 1 2 1 2 4 0 4]\n","step 4580, training accuracy 0.8500\n","pred:  [4 4 1 0 0 0 4 4 0 1 2 0 1 4 3 1 1 2 3 3]\n","ans :  [2 4 1 3 0 0 4 4 0 1 2 0 1 4 3 1 1 2 3 0]\n","step 4590, training accuracy 0.8000\n","pred:  [2 2 3 4 4 3 2 3 3 2 0 1 4 4 0 0 2 1 3 0]\n","ans :  [2 2 3 2 4 3 2 3 3 2 0 2 3 4 0 0 2 2 3 0]\n","step 4600, training accuracy 0.9000\n","pred:  [1 2 2 4 0 3 2 2 3 0 1 3 0 2 0 2 0 4 2 4]\n","ans :  [1 2 2 4 0 3 2 1 3 0 1 3 0 2 0 3 0 4 2 4]\n","step 4610, training accuracy 0.8500\n","pred:  [1 4 3 0 3 2 1 4 0 3 3 3 0 1 0 1 0 4 4 1]\n","ans :  [1 4 3 0 0 1 1 4 0 3 0 3 0 1 0 1 0 4 4 1]\n","step 4620, training accuracy 0.9500\n","pred:  [0 0 2 2 0 3 4 4 1 0 4 1 4 3 4 1 4 0 2 2]\n","ans :  [3 0 2 2 0 3 4 4 1 0 4 1 4 3 4 1 4 0 2 2]\n","step 4630, training accuracy 0.9500\n","pred:  [0 0 1 3 1 0 4 0 4 2 3 2 2 2 4 2 4 0 3 0]\n","ans :  [0 0 1 4 1 0 4 0 4 2 3 2 2 2 4 2 4 0 3 0]\n","step 4640, training accuracy 0.8500\n","pred:  [1 2 0 4 4 4 1 2 2 0 3 0 2 0 0 3 3 3 3 1]\n","ans :  [1 2 0 4 4 4 1 2 2 0 3 3 2 3 3 3 3 3 3 1]\n","step 4650, training accuracy 0.8500\n","pred:  [3 0 2 0 1 4 1 3 1 1 4 0 4 3 3 1 3 3 3 4]\n","ans :  [0 0 2 0 1 4 1 3 1 1 3 0 4 3 3 1 3 3 2 4]\n","step 4660, training accuracy 0.9000\n","pred:  [3 4 0 1 0 3 1 2 3 1 3 3 2 2 4 1 1 4 1 1]\n","ans :  [0 3 0 1 0 3 1 2 3 1 3 3 2 2 4 1 1 4 1 1]\n","step 4670, training accuracy 0.9500\n","pred:  [4 1 4 2 0 2 3 1 0 1 0 4 2 3 1 4 4 1 1 1]\n","ans :  [2 1 4 2 0 2 3 1 0 1 0 4 2 3 1 4 4 1 1 1]\n","step 4680, training accuracy 0.9500\n","pred:  [2 3 4 1 2 2 3 3 4 0 1 3 4 0 3 0 4 2 1 1]\n","ans :  [2 3 4 1 2 2 3 3 4 0 1 3 4 0 4 0 4 2 1 1]\n","step 4690, training accuracy 0.9500\n","pred:  [4 4 2 1 3 0 0 1 2 4 1 0 3 3 3 1 0 4 4 3]\n","ans :  [4 4 2 1 3 3 0 1 2 4 1 0 3 3 3 1 0 4 4 3]\n","step 4700, training accuracy 0.8500\n","pred:  [3 1 3 3 0 1 2 0 4 4 3 4 2 1 1 2 4 2 0 0]\n","ans :  [3 1 3 0 0 1 1 0 4 4 3 4 2 1 1 2 3 2 0 0]\n","step 4710, training accuracy 0.9000\n","pred:  [2 1 1 3 0 3 2 2 2 4 0 2 3 1 1 1 0 4 3 3]\n","ans :  [2 1 1 4 0 3 2 2 2 2 0 2 3 1 1 1 0 4 3 3]\n","step 4720, training accuracy 0.9000\n","pred:  [0 3 4 2 3 2 1 3 3 1 0 3 4 4 0 3 1 0 4 2]\n","ans :  [0 3 4 2 3 4 1 3 3 1 0 3 2 4 0 3 1 0 4 2]\n","step 4730, training accuracy 0.9500\n","pred:  [3 0 3 3 1 1 1 4 3 4 2 1 2 0 1 2 0 4 4 1]\n","ans :  [3 0 3 3 1 1 1 4 3 4 2 1 2 0 1 2 0 2 4 1]\n","step 4740, training accuracy 1.0000\n","pred:  [3 3 3 0 2 1 3 4 2 1 1 3 2 3 3 3 3 3 0 4]\n","ans :  [3 3 3 0 2 1 3 4 2 1 1 3 2 3 3 3 3 3 0 4]\n","step 4750, training accuracy 0.9500\n","pred:  [3 2 3 4 1 0 1 4 0 1 1 1 0 3 1 4 4 2 3 4]\n","ans :  [3 2 3 4 1 0 1 4 0 1 1 1 0 3 1 4 4 2 0 4]\n","step 4760, training accuracy 1.0000\n","pred:  [0 2 3 1 3 4 1 3 4 1 1 1 0 3 4 4 4 1 4 1]\n","ans :  [0 2 3 1 3 4 1 3 4 1 1 1 0 3 4 4 4 1 4 1]\n","step 4770, training accuracy 0.9000\n","pred:  [4 1 1 0 1 3 4 3 3 4 0 4 3 0 1 4 1 3 3 0]\n","ans :  [4 1 1 0 1 0 4 3 3 4 0 4 3 0 1 4 1 3 0 0]\n","step 4780, training accuracy 0.8500\n","pred:  [1 2 3 1 1 2 4 3 4 4 3 1 3 3 3 2 3 4 0 2]\n","ans :  [1 2 3 1 1 2 4 3 4 4 4 1 3 3 3 2 3 3 1 2]\n","step 4790, training accuracy 1.0000\n","pred:  [1 1 3 3 2 1 2 4 2 0 4 1 4 3 3 2 1 3 3 1]\n","ans :  [1 1 3 3 2 1 2 4 2 0 4 1 4 3 3 2 1 3 3 1]\n","step 4800, training accuracy 0.8500\n","pred:  [1 2 3 2 2 4 3 0 1 0 3 3 4 1 4 1 3 0 3 2]\n","ans :  [1 2 3 2 2 4 3 0 1 3 4 3 2 1 4 1 3 0 3 2]\n","step 4810, training accuracy 0.9500\n","pred:  [4 0 1 3 1 3 0 4 1 1 2 0 4 3 0 4 1 4 4 2]\n","ans :  [4 0 1 3 1 3 0 4 1 1 2 0 4 3 0 4 0 4 4 2]\n","step 4820, training accuracy 0.9000\n","pred:  [1 2 2 3 4 0 3 0 1 4 1 2 0 2 3 1 4 2 4 3]\n","ans :  [1 2 2 3 4 1 3 0 1 4 1 2 0 2 3 1 4 2 3 3]\n","step 4830, training accuracy 0.9000\n","pred:  [2 2 0 3 0 2 4 2 2 4 0 4 2 1 4 3 1 3 0 2]\n","ans :  [2 2 0 3 0 2 4 2 2 4 0 4 2 3 4 3 3 3 0 2]\n","step 4840, training accuracy 0.8500\n","pred:  [4 0 2 1 3 2 0 1 3 4 0 4 3 2 1 2 3 1 3 4]\n","ans :  [3 0 4 1 3 4 0 1 3 4 0 4 3 2 1 2 3 1 3 4]\n","step 4850, training accuracy 0.9500\n","pred:  [0 3 2 0 3 3 0 2 2 1 2 3 3 0 4 4 2 1 1 2]\n","ans :  [0 3 2 0 4 3 0 2 2 1 2 3 3 0 4 4 2 1 1 2]\n","step 4860, training accuracy 0.8000\n","pred:  [0 4 0 1 3 2 1 3 2 4 0 2 4 4 0 1 2 4 3 0]\n","ans :  [0 4 0 1 0 2 1 3 4 4 0 2 4 4 0 0 2 4 4 0]\n","step 4870, training accuracy 1.0000\n","pred:  [0 2 4 1 4 4 0 3 1 0 0 4 2 0 4 3 2 1 1 2]\n","ans :  [0 2 4 1 4 4 0 3 1 0 0 4 2 0 4 3 2 1 1 2]\n","step 4880, training accuracy 0.9000\n","pred:  [0 3 2 2 4 1 0 3 0 1 3 0 4 2 3 3 4 1 4 1]\n","ans :  [0 0 2 2 4 1 0 3 0 0 3 0 4 2 3 3 4 1 4 1]\n","step 4890, training accuracy 0.9000\n","pred:  [1 3 1 3 3 4 3 0 4 3 3 0 1 0 3 1 2 2 0 0]\n","ans :  [0 3 1 0 3 4 3 0 4 3 3 0 1 0 3 1 2 2 0 0]\n","step 4900, training accuracy 1.0000\n","pred:  [2 4 3 3 1 1 2 4 0 0 1 4 3 2 1 2 0 1 4 4]\n","ans :  [2 4 3 3 1 1 2 4 0 0 1 4 3 2 1 2 0 1 4 4]\n","step 4910, training accuracy 0.9000\n","pred:  [4 4 4 0 0 4 4 4 0 4 2 0 1 4 3 0 0 0 1 1]\n","ans :  [2 4 4 0 0 4 4 2 0 4 2 0 1 4 3 0 0 0 1 1]\n","step 4920, training accuracy 0.8500\n","pred:  [4 0 4 0 4 1 3 4 0 4 0 3 2 1 4 2 0 0 3 1]\n","ans :  [4 0 4 0 4 2 3 4 3 2 0 3 2 1 4 2 0 0 3 1]\n","step 4930, training accuracy 0.9000\n","pred:  [1 2 1 1 0 0 1 0 0 4 4 4 4 4 2 4 4 1 3 2]\n","ans :  [1 2 1 2 3 0 1 0 0 4 4 4 4 4 2 4 4 1 3 2]\n","step 4940, training accuracy 0.9500\n","pred:  [1 1 3 0 3 2 3 0 1 1 0 3 4 3 2 2 0 2 0 2]\n","ans :  [1 1 3 0 3 2 3 0 1 1 0 0 4 3 2 2 0 2 0 2]\n","step 4950, training accuracy 1.0000\n","pred:  [1 1 2 0 1 4 0 1 1 1 0 4 3 3 3 4 4 2 2 4]\n","ans :  [1 1 2 0 1 4 0 1 1 1 0 4 3 3 3 4 4 2 2 4]\n","step 4960, training accuracy 0.9000\n","pred:  [2 0 3 0 0 2 3 3 4 1 1 4 4 3 0 2 2 2 0 3]\n","ans :  [2 0 3 0 3 2 3 3 4 1 1 4 4 3 0 4 2 2 0 3]\n","step 4970, training accuracy 0.9500\n","pred:  [1 0 0 1 2 2 1 4 4 1 3 1 3 0 4 4 1 0 4 4]\n","ans :  [1 0 0 1 2 2 1 4 4 1 3 1 3 0 4 4 1 0 2 4]\n","step 4980, training accuracy 0.9500\n","pred:  [4 0 2 2 4 3 1 0 3 1 3 3 3 4 1 4 1 0 0 2]\n","ans :  [4 0 2 2 4 3 1 0 3 1 3 3 3 4 1 3 1 0 0 2]\n","step 4990, training accuracy 0.9000\n","pred:  [4 0 1 4 3 4 1 4 3 1 2 0 0 3 1 3 3 4 3 4]\n","ans :  [4 3 1 4 3 2 1 4 3 1 2 0 0 3 1 3 3 4 3 4]\n","step 5000, training accuracy 0.8500\n","pred:  [4 4 3 0 1 2 1 2 3 3 1 0 3 4 0 0 1 0 3 2]\n","ans :  [3 4 3 0 1 2 1 2 3 0 1 0 3 3 0 0 1 0 3 2]\n","End Of Program-Train\n","SavePath: drive/My Drive/AWD/res.ckpt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Cnct3a-PYDmq","colab_type":"code","outputId":"dbace725-8a80-4913-aa29-37b566b5eaa4","executionInfo":{"status":"ok","timestamp":1565565803802,"user_tz":-540,"elapsed":17889,"user":{"displayName":"서민성","photoUrl":"","userId":"06770200570856547998"}},"colab":{"base_uri":"https://localhost:8080/","height":73}},"source":["# 모든 데이터에 대해 xy 좌표값 계산하기\n","xy_list = []\n","argm_list = []\n","cnt = 0\n","\n","bef = 0\n","aft = 20\n","for i in range(116):\n","    xy, am = model.get_xy(images[bef:aft])\n","    \n","    for i in range(len(xy)):\n","        xy_list.append(xy[i])\n","        argm_list.append(am[i])\n","    \n","    bef = aft\n","    aft += 20\n","    \n","xy, am = model.get_xy(images[bef:2323])\n","for i in range(len(xy)):\n","    xy_list.append(xy[i])\n","    argm_list.append(am[i])\n","\n","\n","print(xy_list)\n","print(argm_list)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[array([-2.0319548, 13.368402 ], dtype=float32), array([ 4.5675993, -0.2152335], dtype=float32), array([ 6.9440336, -3.8514557], dtype=float32), array([-5.341563 ,  5.7516336], dtype=float32), array([-2.2859292,  3.3337543], dtype=float32), array([-0.80394447, -2.634318  ], dtype=float32), array([-0.13491404,  2.9426222 ], dtype=float32), array([4.5314283 , 0.20819587], dtype=float32), array([ 3.827728 , -1.9811578], dtype=float32), array([ 7.414772 , -2.5482676], dtype=float32), array([ 3.6467707, -5.3131104], dtype=float32), array([ 3.4961827, -4.441802 ], dtype=float32), array([ 4.0637035 , -0.46021897], dtype=float32), array([1.1927269, 2.1048985], dtype=float32), array([-0.82043564,  3.2437072 ], dtype=float32), array([ 1.3370295, -3.9365597], dtype=float32), array([0.51790047, 0.02098447], dtype=float32), array([ 3.7661984, -3.8240457], dtype=float32), array([-2.448271  ,  0.27232057], dtype=float32), array([-0.22112948, -0.97984105], dtype=float32), array([ 0.34421575, -1.1924937 ], dtype=float32), array([3.320607 , 1.5980926], dtype=float32), array([ 3.9635947, -4.0158253], dtype=float32), array([ 2.1200306, -1.3594434], dtype=float32), array([ 1.9434249, -1.9324937], dtype=float32), array([-2.6040955,  2.4446146], dtype=float32), array([-6.1408434,  7.4381304], dtype=float32), array([ 4.7229137, -2.6543548], dtype=float32), array([-3.1412237,  5.518932 ], dtype=float32), array([ 2.6735075, -1.7505257], dtype=float32), array([1.4277134, 6.8553996], dtype=float32), array([-0.37723714, -2.6192837 ], dtype=float32), array([ 3.0430367, -2.3888347], dtype=float32), array([-0.6329879 ,  0.12473077], dtype=float32), array([ 5.6372185, -3.1338391], dtype=float32), array([ 6.955929 , -3.0868216], dtype=float32), array([ 4.280031 , -1.5130742], dtype=float32), array([ 1.4972858, -1.0217495], dtype=float32), array([-1.1230563,  3.503643 ], dtype=float32), array([ 1.3272662, -1.7112641], dtype=float32), array([-0.37972885,  2.090417  ], dtype=float32), array([ 1.8959901 , -0.18282062], dtype=float32), array([-1.6593605 , -0.39222938], dtype=float32), array([ 3.0984862 , -0.04164976], dtype=float32), array([ 3.0884995, -1.3114347], dtype=float32), array([ 3.9621732, -3.154243 ], dtype=float32), array([1.2689068, 1.6857874], dtype=float32), array([ 6.520233 , -3.6164045], dtype=float32), array([-0.21064562,  0.845069  ], dtype=float32), array([-0.50744176,  9.623237  ], dtype=float32), array([ 0.43422413, -1.6085331 ], dtype=float32), array([-0.12587965,  1.0905676 ], dtype=float32), array([-5.754055, 15.341273], dtype=float32), array([-2.7233784,  0.7704949], dtype=float32), array([ 2.8751633, -4.7905912], dtype=float32), array([-0.12439692, -1.5221925 ], dtype=float32), array([ 3.035868, -3.632946], dtype=float32), array([ 2.6973474, -2.780098 ], dtype=float32), array([ 5.2255898, -4.0435057], dtype=float32), array([ 6.172868, -3.848999], dtype=float32), array([ 1.5512569 , -0.88727945], dtype=float32), array([-6.4623194,  8.000132 ], dtype=float32), array([-0.5700036,  4.7394724], dtype=float32), array([ 0.81656337, -0.9766794 ], dtype=float32), array([-0.28753728,  2.9546247 ], dtype=float32), array([ 3.5864995, -2.9992313], dtype=float32), array([ 4.5177298, -2.8800087], dtype=float32), array([ 7.2547817, -3.0855167], dtype=float32), array([-7.561907, 10.305859], dtype=float32), array([ 2.908325 , -3.7115078], dtype=float32), array([-1.3867157, -2.1724339], dtype=float32), array([ 0.9427073 , -0.54541117], dtype=float32), array([ 3.7110074, -0.646637 ], dtype=float32), array([2.0910125, 3.2820954], dtype=float32), array([-1.2494628, -1.3741479], dtype=float32), array([ 6.0798445, -3.6247911], dtype=float32), array([ 0.24702716, -2.6801922 ], dtype=float32), array([3.3382466 , 0.14390606], dtype=float32), array([ 6.7921143, -2.4952288], dtype=float32), array([ 4.103771 , -1.4089246], dtype=float32), array([ 1.6435809, 11.364644 ], dtype=float32), array([-0.4403872, -0.419357 ], dtype=float32), array([5.6145735, 1.1253695], dtype=float32), array([-0.16816938, -0.1336537 ], dtype=float32), array([ 2.0408268, -3.4073339], dtype=float32), array([ 3.4489248, -4.144933 ], dtype=float32), array([ 2.8169   , -2.6549373], dtype=float32), array([-4.020008 ,  1.2135301], dtype=float32), array([0.31898654, 0.23207265], dtype=float32), array([-0.8812368, -2.095607 ], dtype=float32), array([ 3.1629107, -2.141133 ], dtype=float32), array([ 3.42051   , -0.06911975], dtype=float32), array([-5.7672234,  7.5691824], dtype=float32), array([-2.880944 ,  1.8126631], dtype=float32), array([ 5.8694086, -1.1739554], dtype=float32), array([ 0.7426493, -2.396987 ], dtype=float32), array([ 2.8828948, -2.239045 ], dtype=float32), array([ 4.5235977, -3.4837747], dtype=float32), array([-1.6143687,  2.8999643], dtype=float32), array([ 6.84111  , -2.6853902], dtype=float32), array([ 4.661579 , -2.1757112], dtype=float32), array([-4.436199 ,  2.5804572], dtype=float32), array([ 3.8227694, -2.9034872], dtype=float32), array([ 3.229224 , -2.5544176], dtype=float32), array([-2.0490851 , -0.96355206], dtype=float32), array([-0.9008148,  0.625797 ], dtype=float32), array([-0.24769211, -4.3410316 ], dtype=float32), array([4.296488  , 0.73809844], dtype=float32), array([ 1.4312  , -3.587564], dtype=float32), array([ 0.60205936, -0.1870814 ], dtype=float32), array([ 6.2856026, -1.5609207], dtype=float32), array([ 6.7829227, -2.3347476], dtype=float32), array([-4.3803596, 10.844269 ], dtype=float32), array([-3.1015625, 14.788072 ], dtype=float32), array([ 1.7800138, -2.042133 ], dtype=float32), array([-1.2652788,  1.6273079], dtype=float32), array([ 2.8643382, -3.854661 ], dtype=float32), array([0.64001846, 0.81847566], dtype=float32), array([ 4.3813744, -1.261869 ], dtype=float32), array([ 5.7676888, -2.8498113], dtype=float32), array([ 5.3414354, -1.4487393], dtype=float32), array([ 4.5123625, -1.936451 ], dtype=float32), array([-0.19614995, -1.2017918 ], dtype=float32), array([-3.139714, 11.834276], dtype=float32), array([ 0.26830995, -0.8287596 ], dtype=float32), array([ 3.859598, -2.10648 ], dtype=float32), array([ 1.614847  , -0.87989706], dtype=float32), array([ 3.029907 , -3.1905396], dtype=float32), array([ 4.757757 , -2.4455013], dtype=float32), array([ 3.1269667, -3.1453261], dtype=float32), array([ 4.502967 , -3.0821106], dtype=float32), array([-4.187416,  4.6676  ], dtype=float32), array([ 2.9157221, -1.6658049], dtype=float32), array([ 6.0736265, -3.293785 ], dtype=float32), array([-6.8512106,  8.682654 ], dtype=float32), array([ 1.0226893, -3.0980868], dtype=float32), array([ 5.314932 , -3.3252275], dtype=float32), array([-2.330746 ,  1.5131094], dtype=float32), array([-1.6423404,  5.351916 ], dtype=float32), array([2.7156003 , 0.03470582], dtype=float32), array([4.6936426 , 0.63482684], dtype=float32), array([-0.583269 , -1.5090196], dtype=float32), array([ 2.3145428, -2.4717748], dtype=float32), array([ 4.660488 , -3.8852544], dtype=float32), array([0.7239784, 8.709818 ], dtype=float32), array([ 4.958477 , -3.0618594], dtype=float32), array([-6.231905 ,  2.4759877], dtype=float32), array([ 2.904552, -2.278706], dtype=float32), array([ 2.4161706, -2.3480344], dtype=float32), array([ 3.0442889, -3.3423548], dtype=float32), array([-1.8143601,  2.3846445], dtype=float32), array([ 2.9065108 , -0.18500823], dtype=float32), array([0.89436567, 0.5884238 ], dtype=float32), array([ 3.3760707, -2.8799098], dtype=float32), array([ 4.903863, -3.127657], dtype=float32), array([-1.0322607, 13.0896   ], dtype=float32), array([-3.229382 ,  0.9034753], dtype=float32), array([ 3.751922 , -4.2223315], dtype=float32), array([ 1.7886033, -0.9874039], dtype=float32), array([-0.7612451,  2.9924784], dtype=float32), array([ 6.1696205, -3.1363325], dtype=float32), array([ 3.9103205, -2.8461943], dtype=float32), array([ 1.3629277, -3.0729947], dtype=float32), array([-4.021311, 15.316855], dtype=float32), array([ 1.6948102, -1.1410155], dtype=float32), array([ 4.1337395, -1.9280639], dtype=float32), array([-0.8063215,  2.0686266], dtype=float32), array([ 2.4831514, -1.1189604], dtype=float32), array([3.7971342, 1.9527104], dtype=float32), array([-2.9393997 , -0.93705064], dtype=float32), array([-5.0670652,  8.954263 ], dtype=float32), array([0.94419134, 0.07039851], dtype=float32), array([ 6.5241985, -2.6289644], dtype=float32), array([-1.1508988 , -0.07786268], dtype=float32), array([-0.98181015, -0.33871114], dtype=float32), array([ 0.01445299, -0.32325453], dtype=float32), array([ 3.2881663, -4.791706 ], dtype=float32), array([ 5.7653675, -1.738713 ], dtype=float32), array([ 1.3871858, -0.3147077], dtype=float32), array([ 4.1172466, -4.1395035], dtype=float32), array([2.9469645, 0.9955875], dtype=float32), array([-9.166625,  7.923806], dtype=float32), array([ 4.7276087, -2.8351443], dtype=float32), array([-0.33484328, -0.6911251 ], dtype=float32), array([ 1.981581 , -1.7232461], dtype=float32), array([ 0.02275324, -1.502821  ], dtype=float32), array([ 3.5168025, -2.1092846], dtype=float32), array([ 0.808844 , -2.4568832], dtype=float32), array([ 3.553682, -1.615654], dtype=float32), array([ 3.8980372, -3.1687875], dtype=float32), array([ 4.5077343, -3.3605304], dtype=float32), array([ 1.5666621, -2.4916062], dtype=float32), array([-3.375482,  4.621922], dtype=float32), array([1.1305931, 8.717048 ], dtype=float32), array([-0.97170407,  9.906904  ], dtype=float32), array([ 5.3278217, -4.1183352], dtype=float32), array([ 3.950187 , -0.4202066], dtype=float32), array([ 1.6990931, -0.3319555], dtype=float32), array([ 1.6211741, -1.3677337], dtype=float32), array([ 3.3612115, -3.5584097], dtype=float32), array([-2.1530447,  1.4893394], dtype=float32), array([ 0.5702294, -1.1965892], dtype=float32), array([-1.2530115, 11.804057 ], dtype=float32), array([ 4.1879234, -2.52628  ], dtype=float32), array([ 6.1497316, -2.9160213], dtype=float32), array([-0.00582087,  1.1064017 ], dtype=float32), array([ 2.5180242 , -0.12414783], dtype=float32), array([ 0.52576804, -3.038382  ], dtype=float32), array([-2.2634368,  2.5463555], dtype=float32), array([ 2.7881925, -3.5070558], dtype=float32), array([-3.9759872,  1.0450575], dtype=float32), array([ 4.9037676, -2.259979 ], dtype=float32), array([5.0513296, 1.8603215], dtype=float32), array([-1.8000922,  2.3222864], dtype=float32), array([ 5.847535 , -3.5285716], dtype=float32), array([-5.4316826,  3.293797 ], dtype=float32), array([ 6.7729673, -1.7482033], dtype=float32), array([-0.47468638,  0.3504997 ], dtype=float32), array([ 2.310889 , -2.0367634], dtype=float32), array([ 4.5930414, -3.0590022], dtype=float32), array([ 1.4508028, -3.2900205], dtype=float32), array([0.572291  , 0.77878445], dtype=float32), array([-3.8079443 , -0.10129148], dtype=float32), array([-1.5184703, -3.0405016], dtype=float32), array([ 4.264081, -1.924541], dtype=float32), array([ 0.73747826, -4.4049587 ], dtype=float32), array([ 2.8272598, -0.5626747], dtype=float32), array([-1.4772502,  1.2308517], dtype=float32), array([ 4.594985, -3.231618], dtype=float32), array([-0.08020723,  5.9605904 ], dtype=float32), array([0.58084345, 0.91719514], dtype=float32), array([ 4.367942 , -2.5981867], dtype=float32), array([ 6.776021  , -0.96124536], dtype=float32), array([-2.767807,  4.077058], dtype=float32), array([5.1425085, 1.4621274], dtype=float32), array([ 3.3214452 , -0.28805465], dtype=float32), array([-6.5726595, 14.937667 ], dtype=float32), array([ 2.4765885, -3.5552406], dtype=float32), array([ 3.752343 , -2.7385192], dtype=float32), array([ 4.862093 , -2.7966046], dtype=float32), array([ 5.144022  , -0.43257362], dtype=float32), array([ 4.484598 , -1.3721266], dtype=float32), array([-7.8096094, 15.355887 ], dtype=float32), array([ 2.970023 , -3.6473231], dtype=float32), array([-1.1334989, -2.975438 ], dtype=float32), array([ 5.4793863, -2.6865687], dtype=float32), array([ 4.1061096, -3.2679913], dtype=float32), array([1.2170718 , 0.22407681], dtype=float32), array([ 6.1312866, -3.620729 ], dtype=float32), array([ 1.2177188, -2.448257 ], dtype=float32), array([ 2.0114293, -2.3903112], dtype=float32), array([ 2.3152363, -1.1843126], dtype=float32), array([-0.2725824, 11.557395 ], dtype=float32), array([ 0.8772894, -1.5785792], dtype=float32), array([ 1.4436555, -2.157876 ], dtype=float32), array([0.9058088, 3.002059 ], dtype=float32), array([-1.1999713 ,  0.22353864], dtype=float32), array([ 1.3069274, -1.7239485], dtype=float32), array([-1.5025539 ,  0.26274604], dtype=float32), array([ 1.9033625, -1.0362067], dtype=float32), array([-2.3425565,  6.7027693], dtype=float32), array([ 6.032955 , -2.6303315], dtype=float32), array([ 2.4194157, -4.275317 ], dtype=float32), array([2.7026968, 1.0304115], dtype=float32), array([-3.802276 ,  2.4027941], dtype=float32), array([ 3.2352536, -0.5771256], dtype=float32), array([0.62642086, 8.23233   ], dtype=float32), array([ 6.788394 , -2.0437825], dtype=float32), array([ 8.052984 , -2.7313712], dtype=float32), array([0.5596825, 7.0913877], dtype=float32), array([ 2.5041115, -1.8429894], dtype=float32), array([ 2.1050746, -4.8394394], dtype=float32), array([ 2.4250004, -4.1981134], dtype=float32), array([-2.244167 ,  1.9478815], dtype=float32), array([ 2.3102257, -4.0494556], dtype=float32), array([-2.2538295 ,  0.94621336], dtype=float32), array([ 2.4775007, -3.2240708], dtype=float32), array([-5.7356663,  4.395775 ], dtype=float32), array([-0.20403689, -3.3174255 ], dtype=float32), array([3.6623237, 1.2598522], dtype=float32), array([ 3.2030275, -2.3065486], dtype=float32), array([-0.6973982 , -0.24376363], dtype=float32), array([ 3.4413502, -3.3567252], dtype=float32), array([-0.127384 , -2.8689077], dtype=float32), array([ 1.6605756, -3.8079424], dtype=float32), array([ 0.08036631, -0.80591255], dtype=float32), array([ 4.583663 , -1.8310502], dtype=float32), array([ 5.698021  , -0.36234927], dtype=float32), array([-3.5716422 ,  0.43018574], dtype=float32), array([ 0.27281117, -0.5323549 ], dtype=float32), array([ 2.3495953, -2.866942 ], dtype=float32), array([-3.6852236, 24.054533 ], dtype=float32), array([ 3.9380639, -0.9310221], dtype=float32), array([ 1.3177183, -1.5893316], dtype=float32), array([ 3.0507123, -2.1549568], dtype=float32), array([ 4.7083244, -3.190923 ], dtype=float32), array([-0.2518766,  3.2891126], dtype=float32), array([-2.65191   , -0.44238752], dtype=float32), array([ 5.988803 , -2.6689699], dtype=float32), array([-1.1925858,  0.9195517], dtype=float32), array([ 2.0966666, -1.0988166], dtype=float32), array([ 6.3377113, -2.539689 ], dtype=float32), array([1.0730498, 1.1520188], dtype=float32), array([-2.1824362,  4.885027 ], dtype=float32), array([-0.36956125,  2.5130577 ], dtype=float32), array([ 0.9024402, -3.129159 ], dtype=float32), array([ 0.78698087, -1.4952712 ], dtype=float32), array([-0.27255088, -2.7903056 ], dtype=float32), array([ 0.7072885, -2.1153455], dtype=float32), array([ 1.9182656, -0.9860807], dtype=float32), array([3.2393095, 0.3603807], dtype=float32), array([ 7.368017 , -2.8663518], dtype=float32), array([ 2.7652638, -2.1479096], dtype=float32), array([0.3416891, 1.4758065], dtype=float32), array([-4.718563,  3.02558 ], dtype=float32), array([2.7193587, 2.544727 ], dtype=float32), array([ 4.9335966, -3.4208841], dtype=float32), array([-0.64839995,  3.347949  ], dtype=float32), array([-3.793464 ,  3.9825315], dtype=float32), array([ 4.8521986, -0.902023 ], dtype=float32), array([2.5901191, 0.6335557], dtype=float32), array([ 3.0784914, -3.3707628], dtype=float32), array([-0.5541869,  1.923594 ], dtype=float32), array([-3.9162147,  7.839076 ], dtype=float32), array([-5.6478996,  3.3182755], dtype=float32), array([ 2.6206896, -2.024638 ], dtype=float32), array([-0.4546563,  0.2995513], dtype=float32), array([ 4.4513617, -4.0729566], dtype=float32), array([3.8917563 , 0.39911085], dtype=float32), array([-2.316435 ,  2.0469007], dtype=float32), array([2.1936114, 3.6886294], dtype=float32), array([ 0.7991067, -0.963981 ], dtype=float32), array([ 8.0304985, -1.880491 ], dtype=float32), array([ 3.8630679, -2.410049 ], dtype=float32), array([-3.2616103, -0.8696391], dtype=float32), array([ 1.4114106 , -0.95274645], dtype=float32), array([ 5.58992, -4.08703], dtype=float32), array([ 4.035446 , -3.1508083], dtype=float32), array([-0.6388947,  4.244789 ], dtype=float32), array([ 3.140474  , -0.90013176], dtype=float32), array([ 3.3173897, -2.3031762], dtype=float32), array([ 7.1047792, -2.2758884], dtype=float32), array([ 5.914977, -2.303163], dtype=float32), array([ 3.060045 , -3.2444143], dtype=float32), array([ 4.2342424, -2.0944543], dtype=float32), array([-5.775606 , -1.5090954], dtype=float32), array([ 5.3916674, -2.5184405], dtype=float32), array([ 2.3353493, -1.9911852], dtype=float32), array([-1.0405209 , -0.01522917], dtype=float32), array([ 3.3330147, -1.137589 ], dtype=float32), array([1.32957   , 0.00658995], dtype=float32), array([ 0.5571363, -0.7138422], dtype=float32), array([ 0.72872996, -6.483324  ], dtype=float32), array([0.02087688, 2.9560306 ], dtype=float32), array([-1.6179559,  2.3458197], dtype=float32), array([1.369374 , 5.0493393], dtype=float32), array([ 0.8833852, 13.416238 ], dtype=float32), array([-5.059537 ,  3.9196317], dtype=float32), array([1.4863913, 3.3975585], dtype=float32), array([ 4.2997217, -3.7488637], dtype=float32), array([ 1.9048908, -0.4988206], dtype=float32), array([0.68899155, 3.1396217 ], dtype=float32), array([-1.5110501, -0.522389 ], dtype=float32), array([-0.4305427, -2.1896627], dtype=float32), array([ 4.1050787, -2.6985414], dtype=float32), array([3.9511588 , 0.70331293], dtype=float32), array([-1.0294658, -2.9654722], dtype=float32), array([-2.7296877,  8.971076 ], dtype=float32), array([ 4.355401 , -1.2638323], dtype=float32), array([4.500388 , 2.8664732], dtype=float32), array([ 4.157627 , -2.8701048], dtype=float32), array([ 7.8253736, -3.0985172], dtype=float32), array([ 1.2011945, -3.6231947], dtype=float32), array([-0.5368887 ,  0.03182822], dtype=float32), array([ 3.9519165, -3.6790614], dtype=float32), array([ 2.1458602, -0.8850203], dtype=float32), array([ 1.7022846 , -0.09786946], dtype=float32), array([ 4.56293  , -2.7490673], dtype=float32), array([-1.0587974,  4.8072886], dtype=float32), array([-9.152221,  4.539789], dtype=float32), array([-1.643845  ,  0.52172786], dtype=float32), array([ 7.4453955, -2.7574456], dtype=float32), array([0.47657907, 0.9172452 ], dtype=float32), array([ 2.784006, -2.32663 ], dtype=float32), array([ 7.8053856, -2.7590065], dtype=float32), array([-1.2464937,  3.432703 ], dtype=float32), array([ 3.9248645, -4.432104 ], dtype=float32), array([1.3766501, 9.313809 ], dtype=float32), array([ 1.0494895, -1.9266508], dtype=float32), array([ 3.6634934 , -0.45591372], dtype=float32), array([-5.972993 ,  7.2178164], dtype=float32), array([ 1.3126466, -2.943263 ], dtype=float32), array([ 2.4898489, -2.4973986], dtype=float32), array([ 2.8988707, -4.0984077], dtype=float32), array([ 3.300618 , -2.7827501], dtype=float32), array([ 2.9505956, -3.0007865], dtype=float32), array([-7.2211676, 10.562575 ], dtype=float32), array([2.9543073, 0.6068955], dtype=float32), array([ 3.0904968, -2.527481 ], dtype=float32), array([0.03893638, 0.6698083 ], dtype=float32), array([ 3.3618262, -0.7695969], dtype=float32), array([-5.895383 ,  5.2150536], dtype=float32), array([ 0.46404898, -0.48032314], dtype=float32), array([-0.03556538, -1.2091665 ], dtype=float32), array([-0.9909886,  1.4243968], dtype=float32), array([ 0.0921334, -2.5729113], dtype=float32), array([ 2.2395673, -1.059648 ], dtype=float32), array([ 1.3562915, -3.006496 ], dtype=float32), array([-2.401948 ,  4.7422223], dtype=float32), array([2.1022365 , 0.68794113], dtype=float32), array([-1.1293395, 10.829818 ], dtype=float32), array([ 3.585603 , -3.2198853], dtype=float32), array([ 4.0688934, -1.4639013], dtype=float32), array([ 0.8160031, -3.0818534], dtype=float32), array([ 1.4676948 , -0.60454255], dtype=float32), array([ 0.19779444, -0.7259305 ], dtype=float32), array([ 5.3195477, -2.510634 ], dtype=float32), array([ 3.4632638, -1.8135815], dtype=float32), array([3.3012679 , 0.72213805], dtype=float32), array([ 7.3841286, -2.3328834], dtype=float32), array([ 4.914321, -2.217638], dtype=float32), array([ 6.0740833, -1.8023217], dtype=float32), array([-0.8509775,  2.653085 ], dtype=float32), array([-1.7635765,  7.378293 ], dtype=float32), array([-0.9438591,  0.7698229], dtype=float32), array([0.66813517, 2.4457104 ], dtype=float32), array([ 4.5947857, -3.518587 ], dtype=float32), array([ 2.8235939, -3.0760622], dtype=float32), array([ 2.0881712, -1.790252 ], dtype=float32), array([-2.0891204,  1.8966968], dtype=float32), array([-1.9457016,  5.4767103], dtype=float32), array([-0.6150644, -1.8449891], dtype=float32), array([ 1.8698852, -0.62339  ], dtype=float32), array([ 0.4052229, -2.0251393], dtype=float32), array([ 3.5736258, -2.2182891], dtype=float32), array([ 5.7138977, -2.5846944], dtype=float32), array([ 5.3800974, -4.6074486], dtype=float32), array([1.0376116, 2.9531524], dtype=float32), array([-3.0383487,  3.843507 ], dtype=float32), array([ 1.9891009, -1.939193 ], dtype=float32), array([7.021228 , 0.5987255], dtype=float32), array([5.967964 , 2.2903192], dtype=float32), array([ 4.2149267, -1.6840713], dtype=float32), array([2.7781937 , 0.22542316], dtype=float32), array([-3.9742339 ,  0.53268653], dtype=float32), array([-5.3503876,  2.0638368], dtype=float32), array([2.7179687 , 0.14779359], dtype=float32), array([-0.2484864,  2.7323205], dtype=float32), array([ 5.6444826, -2.0673468], dtype=float32), array([ 2.8767195, -2.7527184], dtype=float32), array([1.7137544 , 0.16666108], dtype=float32), array([ 2.2307913, -3.4826975], dtype=float32), array([ 1.170572, -2.582101], dtype=float32), array([-4.7829733,  6.005231 ], dtype=float32), array([ 1.1594393, -0.541899 ], dtype=float32), array([ 2.81457  , -2.8226466], dtype=float32), array([-0.82776314, -0.06751126], dtype=float32), array([ 5.0702696, -2.0790908], dtype=float32), array([0.7817861, 2.7175071], dtype=float32), array([-1.7031858,  0.1534279], dtype=float32), array([ 6.2098064, -2.7835958], dtype=float32), array([3.416731 , 2.3927844], dtype=float32), array([-1.5886102 , -0.75032455], dtype=float32), array([2.319953 , 4.3865047], dtype=float32), array([2.706108 , 1.0672607], dtype=float32), array([-4.7740774, 14.337842 ], dtype=float32), array([-1.5351286, -1.1052172], dtype=float32), array([ 2.5647097, -2.8054698], dtype=float32), array([ 3.129719 , -3.4005208], dtype=float32), array([ 1.60255  , -1.2891507], dtype=float32), array([ 3.1997664 , -0.81967086], dtype=float32), array([ 1.3496273, -3.4318972], dtype=float32), array([ 1.4462402, -0.7165789], dtype=float32), array([ 0.9663062, -1.085932 ], dtype=float32), array([ 1.1110106 , -0.90176636], dtype=float32), array([-0.10976839,  0.40464586], dtype=float32), array([-2.9914265 ,  0.62564343], dtype=float32), array([ 1.2640779, -1.8877583], dtype=float32), array([ 0.89757454, -1.4909029 ], dtype=float32), array([ 6.324748, -2.401379], dtype=float32), array([4.1207275, 2.9004796], dtype=float32), array([ 1.4719868, -1.9422045], dtype=float32), array([ 0.8326011, -1.3273816], dtype=float32), array([1.9890554, 7.3846025], dtype=float32), array([0.524534 , 3.8797045], dtype=float32), array([ 3.30423  , -3.5498257], dtype=float32), array([-0.42565656, -1.3833761 ], dtype=float32), array([-1.3841052, -4.283985 ], dtype=float32), array([-0.7668152,  7.095829 ], dtype=float32), array([-2.9538305 ,  0.12276644], dtype=float32), array([ 1.8252823 , -0.15874416], dtype=float32), array([ 4.579753 , -1.1710446], dtype=float32), array([ 1.0535357, -1.5162578], dtype=float32), array([ 7.5786858, -2.44593  ], dtype=float32), array([-1.240767,  8.069317], dtype=float32), array([ 0.16824341, -1.3685946 ], dtype=float32), array([ 3.6761353, -3.2112942], dtype=float32), array([ 2.0552604, -2.2697194], dtype=float32), array([-0.38052118, -2.6035793 ], dtype=float32), array([ 3.5431035, -4.512593 ], dtype=float32), array([-5.266328,  7.869108], dtype=float32), array([5.052491, 5.139132], dtype=float32), array([ 4.3163433, -2.0770493], dtype=float32), array([-4.7124453,  5.347    ], dtype=float32), array([-1.0585515,  8.993972 ], dtype=float32), array([ 3.4714077, -1.9969988], dtype=float32), array([ 1.9891391, -1.7089343], dtype=float32), array([1.206706  , 0.05742031], dtype=float32), array([ 0.2607354, -0.730995 ], dtype=float32), array([-1.9354827,  2.277123 ], dtype=float32), array([ 4.953289, -3.885818], dtype=float32), array([ 3.4436252, -2.3522496], dtype=float32), array([-1.1884301,  3.5061269], dtype=float32), array([ 6.1658726, -3.3505704], dtype=float32), array([ 4.920306 , -4.3521876], dtype=float32), array([-3.1842723, -1.727418 ], dtype=float32), array([ 3.3698504, -2.3679385], dtype=float32), array([1.2368052, 1.9385407], dtype=float32), array([ 4.512542 , -3.1530995], dtype=float32), array([ 4.1279554, -5.79945  ], dtype=float32), array([ 2.9927862, -2.7747107], dtype=float32), array([3.1394942, 2.5385454], dtype=float32), array([ 2.973124 , -2.8811884], dtype=float32), array([ 6.5067396, -1.9720306], dtype=float32), array([1.9046052, 6.082098 ], dtype=float32), array([-0.9492578,  3.0890474], dtype=float32), array([-3.726095,  5.358374], dtype=float32), array([-2.5887513 ,  0.23556882], dtype=float32), array([ 7.5182934, -2.5841267], dtype=float32), array([0.36642992, 0.51765686], dtype=float32), array([-1.588618, -1.664407], dtype=float32), array([ 3.0284965, -4.516245 ], dtype=float32), array([4.4097986, 1.0081863], dtype=float32), array([1.5714748, 1.2682757], dtype=float32), array([ 4.9254436, -2.439081 ], dtype=float32), array([-1.1135306, -1.3418992], dtype=float32), array([-3.3106844,  4.040945 ], dtype=float32), array([-2.2140365,  4.9652867], dtype=float32), array([ 4.4812365, -3.727057 ], dtype=float32), array([ 3.054671 , -2.9668972], dtype=float32), array([ 1.7725823, -1.224647 ], dtype=float32), array([ 1.2411802, -1.8256929], dtype=float32), array([0.3558743, 7.751871 ], dtype=float32), array([-2.402604,  9.302347], dtype=float32), array([ 1.9464304, -2.1136422], dtype=float32), array([-0.24596685, -3.532793  ], dtype=float32), array([0.3238052, 0.6953303], dtype=float32), array([ 4.1983194, -3.2466106], dtype=float32), array([ 1.3361769, -4.456881 ], dtype=float32), array([-3.668988 ,  3.4695072], dtype=float32), array([ 2.9714582, -3.6581397], dtype=float32), array([0.48753357, 2.6716795 ], dtype=float32), array([1.3270323, 1.0232153], dtype=float32), array([ 2.1104863, -1.7287369], dtype=float32), array([2.375282 , 0.5442905], dtype=float32), array([ 4.3107214 , -0.24720126], dtype=float32), array([ 3.9496567, -2.2805526], dtype=float32), array([-0.7539899,  1.2691782], dtype=float32), array([ 6.015092 , -2.4363403], dtype=float32), array([-0.3665666, -2.0228498], dtype=float32), array([ 1.6595426 , -0.39944416], dtype=float32), array([ 2.4549606, -4.381526 ], dtype=float32), array([ 6.1999083, -3.3672006], dtype=float32), array([-1.8746849,  4.8162737], dtype=float32), array([ 0.6757046, -2.5895033], dtype=float32), array([-5.207238,  4.992799], dtype=float32), array([ 2.2251437, -0.9772392], dtype=float32), array([ 2.1486003, -1.9014807], dtype=float32), array([ 3.1070673, -3.2055027], dtype=float32), array([ 5.359825 , -2.8736763], dtype=float32), array([-1.7029866,  9.236819 ], dtype=float32), array([2.015159, 0.803508], dtype=float32), array([-0.735015 , -0.9350708], dtype=float32), array([3.3520167, 3.9619932], dtype=float32), array([ 2.1016357 , -0.77777773], dtype=float32), array([1.0100715 , 0.27786523], dtype=float32), array([0.3172927, 2.9712517], dtype=float32), array([ 3.0098298, -4.352276 ], dtype=float32), array([ 4.3242884, -2.753457 ], dtype=float32), array([-0.32052386,  1.7512789 ], dtype=float32), array([-2.958191,  2.046467], dtype=float32), array([-1.2370017,  9.646173 ], dtype=float32), array([ 6.4830914, -2.3571784], dtype=float32), array([ 2.6996155, -2.4200072], dtype=float32), array([ 5.799036 , -2.6795413], dtype=float32), array([-1.5864615,  5.025039 ], dtype=float32), array([-3.83305  ,  1.6220767], dtype=float32), array([ 5.9495   , -2.0702703], dtype=float32), array([ 0.18024874, -1.8640637 ], dtype=float32), array([ 5.177041, -4.894984], dtype=float32), array([0.86913526, 3.5139625 ], dtype=float32), array([ 4.4395514, -2.9820356], dtype=float32), array([ 0.91299415, -1.1211421 ], dtype=float32), array([-8.654224,  9.087702], dtype=float32), array([ 3.7435677, -0.524797 ], dtype=float32), array([3.0732148, 1.7530715], dtype=float32), array([ 3.1886146, -4.2064104], dtype=float32), array([ 5.039049 , -3.4166217], dtype=float32), array([ 3.9573247, -3.081049 ], dtype=float32), array([-2.0070193,  0.9258496], dtype=float32), array([ 0.9075464, -1.0579579], dtype=float32), array([0.03152227, 5.498372  ], dtype=float32), array([ 6.031617 , -3.9588308], dtype=float32), array([3.7121499, 2.3355029], dtype=float32), array([ 3.372156 , -3.6053052], dtype=float32), array([ 4.6970396, -3.2783601], dtype=float32), array([-4.296294,  9.926863], dtype=float32), array([ 5.0154896, -3.947671 ], dtype=float32), array([ 1.3908708, 11.343398 ], dtype=float32), array([-2.3285403, -1.373405 ], dtype=float32), array([1.6345832, 4.0324106], dtype=float32), array([ 5.1798   , -3.1911774], dtype=float32), array([2.431064 , 3.6585586], dtype=float32), array([ 2.4756646, -3.3495297], dtype=float32), array([-5.8165417,  7.4437504], dtype=float32), array([ 1.8216074, -2.3684356], dtype=float32), array([-0.34908223, -1.4892697 ], dtype=float32), array([ 4.9411297, -3.0171814], dtype=float32), array([ 3.9653971, -5.9615545], dtype=float32), array([-2.8316479, -2.3987803], dtype=float32), array([-0.4491961, -1.3431666], dtype=float32), array([ 3.858023 , -0.7852046], dtype=float32), array([ 1.4065068 , -0.43728685], dtype=float32), array([2.1626575, 3.494685 ], dtype=float32), array([ 1.9115446, -1.9801395], dtype=float32), array([-1.3385239, -2.0951695], dtype=float32), array([-6.127529,  4.07166 ], dtype=float32), array([3.3900921, 5.670692 ], dtype=float32), array([-4.032239, 12.897359], dtype=float32), array([ 5.5372686, -0.8416955], dtype=float32), array([-6.2453594,  8.106708 ], dtype=float32), array([ 4.89174  , -1.0472612], dtype=float32), array([ 5.5317717, -4.133194 ], dtype=float32), array([ 3.0938337, -3.0615597], dtype=float32), array([ 2.979735 , -5.9475594], dtype=float32), array([ 4.9435673, -0.9905495], dtype=float32), array([ 4.042529  , -0.01429361], dtype=float32), array([ 2.6500375, -2.7015784], dtype=float32), array([ 3.178901, -4.863222], dtype=float32), array([ 1.7911913 , -0.44959813], dtype=float32), array([4.815381 , 0.7167477], dtype=float32), array([0.02138066, 1.6840184 ], dtype=float32), array([-2.633483,  0.583448], dtype=float32), array([ 3.6122773, -4.336862 ], dtype=float32), array([-4.0509214 ,  0.15666407], dtype=float32), array([2.498276 , 5.8083167], dtype=float32), array([4.123499 , 1.5807607], dtype=float32), array([ 4.904769 , -2.1291919], dtype=float32), array([-1.5245873,  3.763754 ], dtype=float32), array([-1.397166 ,  1.1056955], dtype=float32), array([ 4.615205, -3.45296 ], dtype=float32), array([ 2.1402376, -2.3060734], dtype=float32), array([ 6.7618704, -2.4338057], dtype=float32), array([ 7.291071 , -3.4332623], dtype=float32), array([ 1.24014  , -3.5536761], dtype=float32), array([-0.7969301,  3.1391098], dtype=float32), array([-0.8997999, -0.7312196], dtype=float32), array([0.28202987, 1.0770895 ], dtype=float32), array([-4.4973364, 10.997453 ], dtype=float32), array([ 5.9360714, -3.301909 ], dtype=float32), array([-0.5956831,  5.875863 ], dtype=float32), array([ 4.890169 , -3.0205336], dtype=float32), array([3.6537573, 1.7547114], dtype=float32), array([-1.8933812,  0.9561288], dtype=float32), array([1.1323636 , 0.49260765], dtype=float32), array([ 6.378227, -2.574912], dtype=float32), array([-5.7674046,  2.1176414], dtype=float32), array([ 4.303055 , -3.4496894], dtype=float32), array([0.7176113, 1.5457263], dtype=float32), array([ 2.7605927, -4.3680363], dtype=float32), array([ 3.6576254, -3.8207903], dtype=float32), array([-3.2315  , 13.008721], dtype=float32), array([3.0401578, 0.8987473], dtype=float32), array([ 4.4612617, -2.7420568], dtype=float32), array([-3.9224434,  7.5886364], dtype=float32), array([-1.7190778,  1.584974 ], dtype=float32), array([ 2.9746497, -2.6030834], dtype=float32), array([ 6.04595  , -4.0719166], dtype=float32), array([ 6.4641685, -2.7188993], dtype=float32), array([-1.0736661, -1.8576481], dtype=float32), array([ 6.3147087, -3.2927892], dtype=float32), array([ 1.5482512, -2.1083012], dtype=float32), array([-1.5826838,  6.7422113], dtype=float32), array([2.1841831, 1.5239897], dtype=float32), array([ 5.4140615, -2.6694953], dtype=float32), array([ 6.0685225, -2.9945953], dtype=float32), array([ 4.0225897 , -0.71316427], dtype=float32), array([ 0.8709233, -1.4060378], dtype=float32), array([ 3.8188636, -1.9835076], dtype=float32), array([4.2212505, 2.2296326], dtype=float32), array([ 3.536781 , -7.4123974], dtype=float32), array([3.9960144 , 0.02949029], dtype=float32), array([-0.16230488,  2.8826222 ], dtype=float32), array([-7.98005 , 12.801555], dtype=float32), array([-0.23496366, -5.356986  ], dtype=float32), array([1.0301547, 2.374875 ], dtype=float32), array([ 3.3321002, -1.3543098], dtype=float32), array([-1.0828295 ,  0.66019624], dtype=float32), array([4.1591215 , 0.80436045], dtype=float32), array([-7.1179934,  3.4055033], dtype=float32), array([ 3.58479  , -1.0386019], dtype=float32), array([ 2.0213265, -4.077199 ], dtype=float32), array([ 3.6301062, -2.1823497], dtype=float32), array([ 3.1269681, -2.3185935], dtype=float32), array([ 4.5656004, -6.056179 ], dtype=float32), array([ 0.2788689, -0.9871598], dtype=float32), array([-0.25176835, -2.1154275 ], dtype=float32), array([-6.042391, 10.513288], dtype=float32), array([-0.3087479,  1.7678943], dtype=float32), array([-2.2674541,  4.6276903], dtype=float32), array([ 5.5549774, -0.3672694], dtype=float32), array([ 5.8065996, -2.8646767], dtype=float32), array([-0.90035564,  2.3193867 ], dtype=float32), array([-0.871124 ,  1.6505718], dtype=float32), array([1.4088986, 1.5451818], dtype=float32), array([0.01904488, 1.8338513 ], dtype=float32), array([2.4855564, 2.561889 ], dtype=float32), array([ 6.5911713, -3.060625 ], dtype=float32), array([2.5254133, 1.5683224], dtype=float32), array([-0.42572787, -1.0808773 ], dtype=float32), array([ 1.2340882 , -0.39399117], dtype=float32), array([-0.00846672,  4.714904  ], dtype=float32), array([ 0.15044308, -2.1732647 ], dtype=float32), array([-5.789366 ,  4.5057387], dtype=float32), array([ 4.751404, -4.875205], dtype=float32), array([0.30412638, 0.10063213], dtype=float32), array([ 0.7633095, -1.3000104], dtype=float32), array([ 3.6151154, -5.9948654], dtype=float32), array([-2.2998638 ,  0.53459436], dtype=float32), array([2.2329543, 8.525909 ], dtype=float32), array([-7.364109 ,  6.2348027], dtype=float32), array([3.5159028, 6.9500227], dtype=float32), array([ 3.4222295, -2.8198693], dtype=float32), array([ 4.1297855, -2.3310673], dtype=float32), array([0.74106514, 2.0235987 ], dtype=float32), array([ 0.67835665, -0.3466323 ], dtype=float32), array([ 8.98485  , -1.1724613], dtype=float32), array([ 4.8806314, -3.1456428], dtype=float32), array([ 0.8627237, -2.6658692], dtype=float32), array([ 7.58955  , -2.9690413], dtype=float32), array([2.3458025, 0.2053737], dtype=float32), array([ 2.972458  , -0.75698143], dtype=float32), array([ 1.2047937, -2.7391195], dtype=float32), array([2.2437558, 1.3356137], dtype=float32), array([-5.8618526,  1.7428772], dtype=float32), array([-3.7506921, 19.911524 ], dtype=float32), array([ 3.0421505, -4.7276587], dtype=float32), array([ 0.1919682, -1.1856978], dtype=float32), array([ 2.6676855, -5.087615 ], dtype=float32), array([-1.5894858, -1.185509 ], dtype=float32), array([ 5.1800194, -2.2059894], dtype=float32), array([1.5993917, 8.446761 ], dtype=float32), array([ 1.1907294, -2.1158857], dtype=float32), array([1.0623308 , 0.53187495], dtype=float32), array([ 5.9836273, -3.055796 ], dtype=float32), array([ 5.813449 , -2.2971625], dtype=float32), array([-8.979706,  6.017684], dtype=float32), array([ 5.0462923, -3.657918 ], dtype=float32), array([ 4.086458 , -2.5096965], dtype=float32), array([ 6.181411, -2.800529], dtype=float32), array([ 1.98722  , -0.4664063], dtype=float32), array([ 1.2439649, 10.977243 ], dtype=float32), array([-7.4198933,  5.8719506], dtype=float32), array([-0.621209 , -3.6610074], dtype=float32), array([ 5.412133 , -2.3918085], dtype=float32), array([-0.39670938,  0.41955495], dtype=float32), array([ 0.3825333, -1.3782852], dtype=float32), array([ 4.642149 , -3.8286748], dtype=float32), array([2.5387223, 0.7052497], dtype=float32), array([ 1.3514364 , -0.86110073], dtype=float32), array([ 6.1030874, -2.8722043], dtype=float32), array([0.9313525, 5.508557 ], dtype=float32), array([ 3.798096 , -5.1557837], dtype=float32), array([ 2.3864675 , -0.03030604], dtype=float32), array([ 4.73872  , -2.8746865], dtype=float32), array([0.6909739, 1.9301474], dtype=float32), array([4.1336374 , 0.46255928], dtype=float32), array([ 2.0243511, -1.8825488], dtype=float32), array([-4.5406256,  5.9381995], dtype=float32), array([ 2.8496554, -1.9231052], dtype=float32), array([ 3.3226311, -4.264322 ], dtype=float32), array([ 3.9599211, -3.2492657], dtype=float32), array([-1.6124759, -1.4621601], dtype=float32), array([ 3.002988, -4.46137 ], dtype=float32), array([-4.7363095 , -0.16651398], dtype=float32), array([2.4966257, 2.3196774], dtype=float32), array([1.5338719, 0.8908532], dtype=float32), array([ 1.6665757, -2.2806528], dtype=float32), array([ 0.19228148, -1.4303529 ], dtype=float32), array([ 3.7025573, -1.6323526], dtype=float32), array([2.6489289, 9.750569 ], dtype=float32), array([2.073764  , 0.30639023], dtype=float32), array([0.6408069, 9.423133 ], dtype=float32), array([ 0.1457274, -2.5726502], dtype=float32), array([ 1.4984274, -1.2390971], dtype=float32), array([ 3.6007588, -1.7709656], dtype=float32), array([ 6.0523024, -3.0733123], dtype=float32), array([ 3.9113896, -2.4368196], dtype=float32), array([3.521598 , 5.6543407], dtype=float32), array([-4.62021  ,  6.0088797], dtype=float32), array([ 4.565733 , -3.5939827], dtype=float32), array([1.7494714 , 0.86732656], dtype=float32), array([ 1.7554677 , -0.24102467], dtype=float32), array([0.95398283, 3.5400329 ], dtype=float32), array([-1.0955186,  3.3834324], dtype=float32), array([-3.0312145,  3.9895427], dtype=float32), array([ 3.8446858, -2.3073878], dtype=float32), array([ 1.3136418 , -0.07793748], dtype=float32), array([ 5.219406 , -3.3888113], dtype=float32), array([2.1317322, 5.081125 ], dtype=float32), array([ 5.396247 , -2.5558007], dtype=float32), array([2.0447242, 1.623831 ], dtype=float32), array([-0.6511573,  4.648733 ], dtype=float32), array([ 1.9573658, -1.2162569], dtype=float32), array([ 3.7090771, -4.1025724], dtype=float32), array([0.3476497, 1.4762576], dtype=float32), array([ 5.005434, -3.590386], dtype=float32), array([-1.7587152, -0.3014546], dtype=float32), array([ 5.903736 , -3.8607116], dtype=float32), array([-6.1868973,  4.9304414], dtype=float32), array([ 4.759968, -4.024742], dtype=float32), array([1.4121418, 1.7880387], dtype=float32), array([ 2.1939094, -4.771974 ], dtype=float32), array([ 1.0673678, -3.801765 ], dtype=float32), array([-3.6262774,  1.9554789], dtype=float32), array([ 5.0857534, -1.8373423], dtype=float32), array([ 3.6207774, -5.170449 ], dtype=float32), array([-0.4833954,  1.249897 ], dtype=float32), array([3.0360134, 1.4925041], dtype=float32), array([ 5.842947 , -0.4690556], dtype=float32), array([ 5.982667 , -3.1254025], dtype=float32), array([0.40910625, 0.38205022], dtype=float32), array([2.9457614, 3.0221605], dtype=float32), array([ 5.679265 , -2.9644895], dtype=float32), array([-1.1096298,  9.640599 ], dtype=float32), array([-7.558114,  7.754814], dtype=float32), array([-0.50487494, -1.4888625 ], dtype=float32), array([-1.7566152,  3.253682 ], dtype=float32), array([1.3791878, 2.4295115], dtype=float32), array([ 3.406996 , -2.8238597], dtype=float32), array([ 0.42069697, -3.04396   ], dtype=float32), array([ 1.3945286, -3.83498  ], dtype=float32), array([-0.70045984, -2.263804  ], dtype=float32), array([ 0.8395182, -3.455679 ], dtype=float32), array([2.5744286, 4.7883573], dtype=float32), array([ 6.4154015, -2.1470509], dtype=float32), array([5.7074137, 1.5880339], dtype=float32), array([2.3006341, 3.1647973], dtype=float32), array([ 1.9842875, -1.0644767], dtype=float32), array([ 2.646341  , -0.35983366], dtype=float32), array([-2.1383119,  8.433678 ], dtype=float32), array([ 3.8243854, -5.5818877], dtype=float32), array([0.3513701, 7.461026 ], dtype=float32), array([ 3.9314282, -1.4484773], dtype=float32), array([-1.9666691, -2.696386 ], dtype=float32), array([ 4.7245836, -3.7891736], dtype=float32), array([-2.264919 ,  1.5896707], dtype=float32), array([-0.49020725,  0.6798237 ], dtype=float32), array([ 0.1707108, -0.2821433], dtype=float32), array([2.6717923, 5.7518754], dtype=float32), array([ 4.8789215, -2.6761405], dtype=float32), array([ 4.961709 , -1.5767765], dtype=float32), array([-8.611428, 12.267362], dtype=float32), array([2.244363  , 0.86751425], dtype=float32), array([ 1.2561717, -1.8254097], dtype=float32), array([-1.4182642,  6.468297 ], dtype=float32), array([1.3635948, 1.3589692], dtype=float32), array([ 2.6229818, -3.1147816], dtype=float32), array([-1.631069 , -2.7351701], dtype=float32), array([-2.0322742, -0.7044502], dtype=float32), array([ 4.869728 , -3.7411833], dtype=float32), array([ 4.123987, -5.580905], dtype=float32), array([ 3.8335125, -0.6192468], dtype=float32), array([1.6341779 , 0.88084286], dtype=float32), array([ 2.8580768, -2.2368445], dtype=float32), array([ 3.284094 , -2.3387308], dtype=float32), array([2.261602 , 0.5189423], dtype=float32), array([2.2378082, 6.247371 ], dtype=float32), array([0.18446553, 3.2130694 ], dtype=float32), array([ 1.0176712, -1.4252975], dtype=float32), array([-5.8113623,  6.5650463], dtype=float32), array([ 3.5928304, -4.041334 ], dtype=float32), array([ 5.2562494 , -0.42784244], dtype=float32), array([ 4.462737 , -3.7974753], dtype=float32), array([ 1.6825225, -1.4587216], dtype=float32), array([ 3.136161, 11.387498], dtype=float32), array([-4.229312 ,  1.4947817], dtype=float32), array([ 0.8816328, -4.887645 ], dtype=float32), array([ 3.4699957, -4.1873946], dtype=float32), array([4.119358  , 0.67087466], dtype=float32), array([5.54088 , 1.621469], dtype=float32), array([-2.640502 ,  3.3611403], dtype=float32), array([2.404328  , 0.17116576], dtype=float32), array([ 2.5360856, -1.6119115], dtype=float32), array([3.4624884, 3.9315372], dtype=float32), array([ 1.420162 , -4.3395877], dtype=float32), array([ 5.395896 , -3.6402082], dtype=float32), array([-8.816151 ,  5.8526864], dtype=float32), array([1.9274573 , 0.30661494], dtype=float32), array([ 4.1046143, -2.0459807], dtype=float32), array([ 0.87454796, -1.4413028 ], dtype=float32), array([ 1.5829828, -4.2214518], dtype=float32), array([ 0.5965208, 14.613559 ], dtype=float32), array([0.7609451 , 0.43067175], dtype=float32), array([ 1.4516766 , -0.10162795], dtype=float32), array([ 4.6755486, -1.6900947], dtype=float32), array([ 3.1805308, -3.3552654], dtype=float32), array([ 5.881754 , -1.5743942], dtype=float32), array([-4.1467705 , -0.47955495], dtype=float32), array([-0.90919644,  2.6831343 ], dtype=float32), array([ 2.5709946, -0.5025168], dtype=float32), array([ 1.3068185, -3.9305315], dtype=float32), array([-5.4145927,  1.1117492], dtype=float32), array([ 1.2845032 , -0.46144086], dtype=float32), array([ 0.40176058, -2.8557715 ], dtype=float32), array([ 5.9114656, -3.5970335], dtype=float32), array([0.64819765, 5.6681886 ], dtype=float32), array([-0.18012446, -0.07064325], dtype=float32), array([4.778965 , 2.1963396], dtype=float32), array([ 3.1176841, -0.7514792], dtype=float32), array([ 6.3693495, -3.3920627], dtype=float32), array([-1.1957886 ,  0.37764174], dtype=float32), array([ 5.5541677, -1.7902572], dtype=float32), array([ 0.54943895, 10.938827  ], dtype=float32), array([-1.5107579, -1.8789532], dtype=float32), array([-2.4076612, -1.3381884], dtype=float32), array([ 1.6752403, -4.6825504], dtype=float32), array([-0.34838653,  4.6181054 ], dtype=float32), array([-1.0961701,  5.231417 ], dtype=float32), array([ 1.1457973, -2.028716 ], dtype=float32), array([ 4.3310757, -1.2000685], dtype=float32), array([ 4.369379 , -2.6486633], dtype=float32), array([ 3.0241325, -3.8267837], dtype=float32), array([-0.5817189,  3.2992458], dtype=float32), array([ 1.2299986, -2.2132301], dtype=float32), array([ 4.3527145, -5.3251543], dtype=float32), array([ 3.8766143, -0.9523261], dtype=float32), array([-5.2891293,  5.4405336], dtype=float32), array([ 1.6430018 , -0.36015475], dtype=float32), array([3.008046 , 3.8576362], dtype=float32), array([ 3.3615696, -1.9284439], dtype=float32), array([ 0.61846626, -1.1455674 ], dtype=float32), array([ 6.5563583, -3.080331 ], dtype=float32), array([-4.5488997,  7.491352 ], dtype=float32), array([ 4.990678 , -1.4954672], dtype=float32), array([-0.8704278,  6.2530622], dtype=float32), array([3.1061137, 0.7753952], dtype=float32), array([ 2.5871406, -2.504681 ], dtype=float32), array([ 2.6246917, -1.842247 ], dtype=float32), array([ 5.142562 , -4.4219418], dtype=float32), array([ 5.5408964, -2.0616035], dtype=float32), array([0.03737342, 2.57944   ], dtype=float32), array([-2.1172047, -2.3990943], dtype=float32), array([ 5.3043194, -3.935526 ], dtype=float32), array([-3.2318673,  7.427917 ], dtype=float32), array([-1.6824615,  6.391671 ], dtype=float32), array([ 1.8548071 , -0.20626062], dtype=float32), array([ 4.430872 , -4.7536488], dtype=float32), array([-5.745965,  5.315266], dtype=float32), array([ 4.45426  , -2.6018295], dtype=float32), array([3.152343 , 5.2046404], dtype=float32), array([ 5.3338995, -3.354803 ], dtype=float32), array([ 4.070945 , -4.7315493], dtype=float32), array([1.1528211, 2.1203146], dtype=float32), array([-0.13522679, 11.001239  ], dtype=float32), array([-2.0350788, -1.4945216], dtype=float32), array([0.6269865, 5.8208537], dtype=float32), array([ 1.5247936, -4.3911715], dtype=float32), array([-7.861515 ,  3.7727652], dtype=float32), array([0.79173005, 2.0722122 ], dtype=float32), array([ 6.195552 , -1.0804274], dtype=float32), array([-1.22683  ,  4.3977504], dtype=float32), array([2.1938717, 2.9874485], dtype=float32), array([ 1.8475914 , -0.25734967], dtype=float32), array([ 6.216481, -4.755352], dtype=float32), array([ 2.064031 , -3.7689123], dtype=float32), array([ 3.9668782, -3.932281 ], dtype=float32), array([-0.9202106, -2.5957959], dtype=float32), array([ 0.503876 , -0.4363503], dtype=float32), array([ 4.0342627, -3.207952 ], dtype=float32), array([-11.306137,   1.443996], dtype=float32), array([5.7832108, 0.287167 ], dtype=float32), array([ 1.7053497, -1.866921 ], dtype=float32), array([ 6.9170504, -2.7315278], dtype=float32), array([ 5.642744 , -3.1644678], dtype=float32), array([3.5733068, 4.295685 ], dtype=float32), array([ 5.4536657, -3.252317 ], dtype=float32), array([ 5.3609676, -3.155072 ], dtype=float32), array([ 3.222953 , -4.5470047], dtype=float32), array([-2.081387,  5.965932], dtype=float32), array([-1.2717583, 10.797535 ], dtype=float32), array([ 1.7064846, -2.1694322], dtype=float32), array([2.8726177, 1.1337581], dtype=float32), array([0.24172735, 0.9391349 ], dtype=float32), array([ 2.3390858, -3.9067526], dtype=float32), array([-0.67531043, -2.0422406 ], dtype=float32), array([-1.4452605,  7.30449  ], dtype=float32), array([ 1.758172, -1.974195], dtype=float32), array([ 2.240388  , -0.06098717], dtype=float32), array([-2.3746955, -1.2970014], dtype=float32), array([-5.2821484,  1.6655884], dtype=float32), array([ 1.697223 , -2.4204187], dtype=float32), array([ 5.4592943, -2.6295276], dtype=float32), array([ 4.680382 , -3.4636817], dtype=float32), array([-1.6800975, -4.769718 ], dtype=float32), array([-7.7642746,  9.146033 ], dtype=float32), array([ 4.7241917 , -0.81564134], dtype=float32), array([-0.19797695,  4.0017285 ], dtype=float32), array([ 3.6369073 , -0.00611395], dtype=float32), array([ 3.6380522, -3.5426536], dtype=float32), array([ 0.2699101, -0.6641522], dtype=float32), array([ 5.219433 , -4.4019322], dtype=float32), array([ 4.3482647, -1.6481366], dtype=float32), array([ 5.202322 , -2.2586887], dtype=float32), array([3.693723 , 1.5047839], dtype=float32), array([ 3.3141916 , -0.25604075], dtype=float32), array([0.57210386, 1.7724063 ], dtype=float32), array([-4.047576,  5.726184], dtype=float32), array([ 2.5551448, 10.204165 ], dtype=float32), array([ 2.843116 , -2.9813643], dtype=float32), array([2.6244395, 1.511066 ], dtype=float32), array([ 0.05158222, -3.5795875 ], dtype=float32), array([2.5017965, 7.5804396], dtype=float32), array([ 3.4690535, -0.8720816], dtype=float32), array([2.7089522, 5.788294 ], dtype=float32), array([ 2.5753462, -1.8129196], dtype=float32), array([-4.2902193,  1.0153792], dtype=float32), array([ 3.424147 , -5.5895953], dtype=float32), array([ 5.1772165, -3.437602 ], dtype=float32), array([-4.4832296 ,  0.80441445], dtype=float32), array([ 4.3808727, -4.422016 ], dtype=float32), array([-3.1580722,  1.906127 ], dtype=float32), array([ 1.0899057, -2.2490225], dtype=float32), array([ 4.876499 , -1.0018489], dtype=float32), array([ 3.4168174 , -0.52276593], dtype=float32), array([-6.357229,  8.0023  ], dtype=float32), array([ 6.1613026, -1.2202148], dtype=float32), array([-2.7043622,  5.808525 ], dtype=float32), array([ 7.0896254, -3.840221 ], dtype=float32), array([ 2.6211529 , -0.49352962], dtype=float32), array([2.2915127, 4.475938 ], dtype=float32), array([4.31888  , 4.3735676], dtype=float32), array([ 1.5185096, -1.142447 ], dtype=float32), array([ 2.083215 , -3.0387483], dtype=float32), array([ 3.528293 , -1.3159137], dtype=float32), array([ 6.7471266, -6.228719 ], dtype=float32), array([1.3260772 , 0.02193922], dtype=float32), array([0.76962805, 1.9885032 ], dtype=float32), array([-5.884471 , 14.7910795], dtype=float32), array([2.0114992, 2.2712972], dtype=float32), array([ 1.7861173 , -0.40860814], dtype=float32), array([ 5.430645 , -3.8442316], dtype=float32), array([ 1.9628723, -0.7521693], dtype=float32), array([-0.26716155, -1.3919361 ], dtype=float32), array([ 4.6777935, -1.9390554], dtype=float32), array([ 0.45217037, -3.3536797 ], dtype=float32), array([ 2.4049764, -3.266849 ], dtype=float32), array([-10.643285  ,   0.68800944], dtype=float32), array([2.4878318, 0.8948299], dtype=float32), array([ 4.5322104, -0.5672515], dtype=float32), array([ 5.075321 , -3.1602392], dtype=float32), array([ 3.97239, -3.89952], dtype=float32), array([-1.1119176, -1.8057384], dtype=float32), array([ 6.662875 , -3.3533673], dtype=float32), array([ 5.755723 , -3.0885062], dtype=float32), array([ 1.5796885 , -0.18819529], dtype=float32), array([1.4233282, 0.2820838], dtype=float32), array([ 3.3648498, -1.9318333], dtype=float32), array([-4.2400475, -2.000944 ], dtype=float32), array([ 3.2673743, -3.355854 ], dtype=float32), array([-0.5534928, 11.166537 ], dtype=float32), array([ 1.4783959 , -0.09010941], dtype=float32), array([ 1.840092 , -3.2382169], dtype=float32), array([-3.410535,  8.419174], dtype=float32), array([-0.18342447, -0.76549166], dtype=float32), array([ 5.615945 , -3.9230394], dtype=float32), array([-4.4466276,  8.056126 ], dtype=float32), array([2.6237962, 3.1584396], dtype=float32), array([-2.8053598 , -0.40796834], dtype=float32), array([3.1248543, 1.2710469], dtype=float32), array([ 4.592453  , -0.96039945], dtype=float32), array([ 4.3949757 , -0.21794242], dtype=float32), array([-4.308568,  6.318545], dtype=float32), array([2.289304 , 1.5488265], dtype=float32), array([ 3.061664, -3.234185], dtype=float32), array([ 6.218671, -2.612058], dtype=float32), array([ 1.8949504, -3.6644564], dtype=float32), array([ 3.6205566, -1.7575939], dtype=float32), array([-4.801195 , -1.4963088], dtype=float32), array([ 0.16939425, -1.9457245 ], dtype=float32), array([-0.64378417,  6.038658  ], dtype=float32), array([-2.9295251, 12.5509615], dtype=float32), array([ 3.9914443, -2.4901884], dtype=float32), array([3.3224947 , 0.59680456], dtype=float32), array([ 0.8244765, -2.3824797], dtype=float32), array([-0.21102089,  3.3407512 ], dtype=float32), array([ 1.7504363, -1.5804381], dtype=float32), array([ 5.6367464, -3.7633247], dtype=float32), array([1.0594002, 0.9085168], dtype=float32), array([ 2.4681056 , -0.40158272], dtype=float32), array([ 5.042286 , -3.0371022], dtype=float32), array([ 4.070553, -4.531973], dtype=float32), array([-7.4798155,  7.517325 ], dtype=float32), array([ 2.1060674 , -0.35121542], dtype=float32), array([1.5137861, 2.7880259], dtype=float32), array([ 1.3160322, -2.0829995], dtype=float32), array([ 5.612153 , -5.3422112], dtype=float32), array([4.2648916, 5.318519 ], dtype=float32), array([-0.678992, -2.039951], dtype=float32), array([ 6.1222124, -1.3895359], dtype=float32), array([2.1884043, 1.0928555], dtype=float32), array([-4.121668 ,  5.7519073], dtype=float32), array([-3.1564362,  2.443578 ], dtype=float32), array([-0.00244021, -0.3297525 ], dtype=float32), array([ 5.828289 , -1.7410812], dtype=float32), array([ 4.304079 , -2.7816598], dtype=float32), array([1.0286504, 1.6434515], dtype=float32), array([0.626021 , 4.9978633], dtype=float32), array([ 3.101185 , -0.2318663], dtype=float32), array([ 1.9385242, -4.061571 ], dtype=float32), array([ 6.4473257, -5.2341995], dtype=float32), array([5.751089 , 1.1273332], dtype=float32), array([ 3.5020516, -1.9274259], dtype=float32), array([ 0.32909966, -3.632865  ], dtype=float32), array([ 6.027583 , -2.5559223], dtype=float32), array([-0.13436055,  3.404255  ], dtype=float32), array([ 6.2850437, -2.509524 ], dtype=float32), array([-0.65626895,  5.726582  ], dtype=float32), array([-0.4678411, -4.8933463], dtype=float32), array([-0.76307553,  0.11853379], dtype=float32), array([-0.29698122,  1.1996248 ], dtype=float32), array([ 5.5675774, -3.1824486], dtype=float32), array([0.94682014, 6.2890005 ], dtype=float32), array([-1.7049503,  8.558265 ], dtype=float32), array([-1.9623543,  2.8500657], dtype=float32), array([-5.0671663,  3.593792 ], dtype=float32), array([-2.0805924, -0.4922045], dtype=float32), array([ 2.0214036 , -0.83303946], dtype=float32), array([ 5.0836716, -4.3078327], dtype=float32), array([4.1402426, 0.5488239], dtype=float32), array([-9.232872 ,  5.2538395], dtype=float32), array([ 4.683408 , -3.2656116], dtype=float32), array([ 5.9430237, -4.0470676], dtype=float32), array([-1.347181 ,  3.2929015], dtype=float32), array([0.8645563, 5.2942433], dtype=float32), array([ 4.3205843, -0.5112485], dtype=float32), array([ 4.03858  , -4.8611417], dtype=float32), array([ 6.214569 , -3.6140823], dtype=float32), array([3.7968314, 2.6228566], dtype=float32), array([1.3533618, 2.9627948], dtype=float32), array([ 1.3535697, -2.9219592], dtype=float32), array([ 3.1863625 , -0.44247323], dtype=float32), array([-0.30823708, -2.6514292 ], dtype=float32), array([2.5842688, 1.1182494], dtype=float32), array([-3.4804556, 11.693869 ], dtype=float32), array([ 2.388667, -4.602603], dtype=float32), array([-3.6352494,  5.6151867], dtype=float32), array([ 2.5424378, -1.6002924], dtype=float32), array([ 2.9714105, -2.1679084], dtype=float32), array([ 4.263734 , -3.4074702], dtype=float32), array([ 3.298959  , -0.10774744], dtype=float32), array([ 5.753951 , -1.1164927], dtype=float32), array([ 2.5285294 , -0.36573273], dtype=float32), array([ 2.1345565 , -0.11073261], dtype=float32), array([ 2.976565 , -0.7684819], dtype=float32), array([ 4.724573 , -2.4358344], dtype=float32), array([ 1.3736596, -1.9841702], dtype=float32), array([ 6.2811766, -2.356704 ], dtype=float32), array([3.3453324, 1.5744216], dtype=float32), array([ 2.5542176, -2.5867171], dtype=float32), array([ 1.6804943, -2.9092078], dtype=float32), array([-3.4342215,  0.4572652], dtype=float32), array([ 3.4514925, -2.3331847], dtype=float32), array([ 3.7943842, -5.898148 ], dtype=float32), array([0.6482887, 1.4430885], dtype=float32), array([-0.14584184, -0.45286554], dtype=float32), array([-4.1860094,  6.1070766], dtype=float32), array([4.6606064 , 0.14827746], dtype=float32), array([-4.3887525,  6.8145514], dtype=float32), array([-6.20942 ,  8.788451], dtype=float32), array([ 7.3152447, -2.5631542], dtype=float32), array([ 4.932704 , -0.8902387], dtype=float32), array([ 5.714278 , -1.9198477], dtype=float32), array([-0.26013172, -1.448288  ], dtype=float32), array([-7.292697,  3.726214], dtype=float32), array([-7.2493296,  9.177979 ], dtype=float32), array([-1.5182068,  1.3893256], dtype=float32), array([-5.1300716, 10.287236 ], dtype=float32), array([ 4.7645617, -4.6891694], dtype=float32), array([ 1.1118758, -3.2908304], dtype=float32), array([ 4.117796 , -2.0038505], dtype=float32), array([ 5.2897587, -4.851522 ], dtype=float32), array([-0.47792256,  1.229574  ], dtype=float32), array([ 5.720379 , -1.7859998], dtype=float32), array([4.360672 , 0.2996636], dtype=float32), array([-2.2880454,  5.2329755], dtype=float32), array([ 2.9995456, -3.0634756], dtype=float32), array([ 1.9919546, -4.18003  ], dtype=float32), array([-0.07516479,  5.092388  ], dtype=float32), array([ 6.642953, -1.737407], dtype=float32), array([-0.52490115,  8.997594  ], dtype=float32), array([ 0.06634873, -0.15431792], dtype=float32), array([ 0.4664656, -2.4853413], dtype=float32), array([4.71515   , 0.05499077], dtype=float32), array([ 2.7537832, -2.0448005], dtype=float32), array([ 2.5537508, -3.4405816], dtype=float32), array([ 3.7399457, -1.4040232], dtype=float32), array([-2.365499 ,  7.4014525], dtype=float32), array([ 6.0355263, -3.6441355], dtype=float32), array([ 3.7108576, -1.392981 ], dtype=float32), array([ 5.6650667, -2.6390274], dtype=float32), array([-4.0686355,  1.4999282], dtype=float32), array([ 5.069435, -3.059414], dtype=float32), array([ 4.4171906, -3.6567373], dtype=float32), array([ 5.792575 , -2.0203395], dtype=float32), array([-0.29688394, -1.4713874 ], dtype=float32), array([-0.4667157, -3.2546153], dtype=float32), array([-5.3725224, 12.922121 ], dtype=float32), array([0.49731362, 0.34834862], dtype=float32), array([0.8718927, 1.8779533], dtype=float32), array([-4.3727436,  1.2285838], dtype=float32), array([ 6.116643 , -2.7072484], dtype=float32), array([ 2.5363128, -1.4471729], dtype=float32), array([ 4.4087706, -4.748797 ], dtype=float32), array([ 3.982916, -2.781927], dtype=float32), array([ 5.83265 , -3.514227], dtype=float32), array([-3.4032943,  1.7907352], dtype=float32), array([ 4.952258 , -3.9345007], dtype=float32), array([ 2.0462797, -2.6798136], dtype=float32), array([-6.336829 ,  2.7343986], dtype=float32), array([ 1.4432995, -2.1858635], dtype=float32), array([-1.0513463,  6.6659784], dtype=float32), array([ 1.8398285, -2.2733586], dtype=float32), array([ 3.8079045, -1.6975083], dtype=float32), array([3.6129367, 2.4164777], dtype=float32), array([ 3.052721, -1.33953 ], dtype=float32), array([1.5009518, 2.3114262], dtype=float32), array([-2.005844, 10.360026], dtype=float32), array([0.2597258, 2.0646174], dtype=float32), array([3.2610009, 1.2100334], dtype=float32), array([ 2.023907 , -1.8845038], dtype=float32), array([3.999101  , 0.31152695], dtype=float32), array([ 3.8055623, -4.800252 ], dtype=float32), array([ 6.36197  , -3.6526866], dtype=float32), array([ 0.6953893, 10.584933 ], dtype=float32), array([-2.0749753 ,  0.95954484], dtype=float32), array([ 5.6514387, -3.9497833], dtype=float32), array([-3.47481,  4.60683], dtype=float32), array([4.3738184, 0.7482161], dtype=float32), array([-3.4963775,  1.1319616], dtype=float32), array([-5.913212 ,  1.1566441], dtype=float32), array([ 2.1517575, -4.2076497], dtype=float32), array([ 2.1985278, -1.7875814], dtype=float32), array([ 4.345788 , -2.2547853], dtype=float32), array([-4.196327 ,  1.4124994], dtype=float32), array([ 7.127264 , -2.7363706], dtype=float32), array([-4.0550394,  3.3765464], dtype=float32), array([7.072172, 5.818988], dtype=float32), array([ 2.8886812, -2.5454164], dtype=float32), array([ 0.39676714, -1.3654869 ], dtype=float32), array([-6.543849 ,  7.4794507], dtype=float32), array([1.5406992, 1.2896128], dtype=float32), array([-1.3484883, -1.7894096], dtype=float32), array([ 4.376766 , -3.6978683], dtype=float32), array([-4.496516 , 10.9252615], dtype=float32), array([ 5.6106224, -3.2164671], dtype=float32), array([ 2.1422627, -2.985279 ], dtype=float32), array([ 2.4529295, -0.3648612], dtype=float32), array([ 0.3634293, -1.7320333], dtype=float32), array([3.1807725 , 0.06215817], dtype=float32), array([ 3.8171012, -1.84361  ], dtype=float32), array([ 7.6536455, -2.016408 ], dtype=float32), array([-2.810845 ,  1.8545189], dtype=float32), array([-0.7052429,  2.7606874], dtype=float32), array([ 2.9598906, -0.6178054], dtype=float32), array([0.29717505, 0.06032658], dtype=float32), array([ 4.2215624, -4.2094827], dtype=float32), array([2.3639653, 5.398839 ], dtype=float32), array([ 3.4647472, -2.7811503], dtype=float32), array([ 2.8413382, -1.5739493], dtype=float32), array([ 4.864155 , -2.3454509], dtype=float32), array([ 2.8674047, -2.6650207], dtype=float32), array([ 0.2390647 , -0.26426095], dtype=float32), array([-2.75648  ,  5.9211698], dtype=float32), array([ 1.8449869, 11.399967 ], dtype=float32), array([3.1314952 , 0.08512038], dtype=float32), array([ 4.6250143, -2.371226 ], dtype=float32), array([ 3.7434633, -2.4318268], dtype=float32), array([ 5.0449095, -2.9956226], dtype=float32), array([ 4.6574717, -2.305687 ], dtype=float32), array([7.0910826, 4.3062453], dtype=float32), array([ 2.0111072, -1.9795411], dtype=float32), array([ 2.7252128, -2.6108375], dtype=float32), array([-1.9900504, -1.9912369], dtype=float32), array([-4.066902 ,  2.2478774], dtype=float32), array([-6.8568497 ,  0.92131275], dtype=float32), array([ 4.7308283, -3.435054 ], dtype=float32), array([1.0495036, 5.3526382], dtype=float32), array([-0.13300097, -0.04159433], dtype=float32), array([-0.01944745, -1.4118369 ], dtype=float32), array([ 1.941751 , -1.2176535], dtype=float32), array([4.509598 , 1.3263047], dtype=float32), array([-1.1611574, 12.054926 ], dtype=float32), array([-0.7546825 ,  0.63414305], dtype=float32), array([ 3.9386723, -3.045023 ], dtype=float32), array([1.8210509, 5.3160305], dtype=float32), array([-0.41197288,  4.3315744 ], dtype=float32), array([ 4.267806 , -3.3913884], dtype=float32), array([-2.9781184,  2.2229383], dtype=float32), array([ 3.7343562, -1.042124 ], dtype=float32), array([1.0616715 , 0.63244295], dtype=float32), array([-1.788221 , -1.4370596], dtype=float32), array([ 0.9201412, -2.607659 ], dtype=float32), array([0.8834624, 1.763063 ], dtype=float32), array([-1.2627099,  2.1431851], dtype=float32), array([ 5.260771 , -2.6550941], dtype=float32), array([ 5.5697174, -3.4445438], dtype=float32), array([-2.0151777, -2.286372 ], dtype=float32), array([ 3.2494433, -3.5934062], dtype=float32), array([ 5.703142, -3.725119], dtype=float32), array([ 3.2894275, -2.2646265], dtype=float32), array([0.07308835, 6.8397856 ], dtype=float32), array([-3.5460804,  3.8090477], dtype=float32), array([1.0371103, 4.827578 ], dtype=float32), array([-0.09983349, -0.06390017], dtype=float32), array([ 0.47910953, -5.8200293 ], dtype=float32), array([-2.2572083,  3.1469479], dtype=float32), array([ 1.6090915, -2.086597 ], dtype=float32), array([3.3497145, 3.1611   ], dtype=float32), array([ 3.582743 , -2.7634006], dtype=float32), array([0.41140115, 9.543203  ], dtype=float32), array([1.0797398, 7.2530146], dtype=float32), array([ 5.9239063, -5.638728 ], dtype=float32), array([ 2.8681624, -3.4011486], dtype=float32), array([ 2.3247054, -3.2429774], dtype=float32), array([-0.40322644, -0.27649897], dtype=float32), array([ 2.4902754, -3.9442282], dtype=float32), array([ 4.748579 , -1.0875642], dtype=float32), array([ 2.6545653, -4.0721874], dtype=float32), array([ 0.96215546, -0.17207843], dtype=float32), array([-3.823937 ,  2.3425765], dtype=float32), array([ 5.272689 , -2.6088362], dtype=float32), array([ 5.7201023, -1.841713 ], dtype=float32), array([ 4.6728973, -4.256581 ], dtype=float32), array([ 4.960272 , -4.5986586], dtype=float32), array([5.458624 , 4.8778787], dtype=float32), array([ 4.2193747, -4.255527 ], dtype=float32), array([-0.54767555, -0.1420123 ], dtype=float32), array([ 5.0133524, -3.4565992], dtype=float32), array([-1.4183319, -3.1202412], dtype=float32), array([-0.5412332,  6.479684 ], dtype=float32), array([-0.50141555,  0.07245094], dtype=float32), array([0.6019069, 3.6250176], dtype=float32), array([-0.5279376, -1.6270311], dtype=float32), array([ 5.358446, -3.782061], dtype=float32), array([1.4224303, 0.8113622], dtype=float32), array([ 0.8765104, -1.3866432], dtype=float32), array([-0.093086, 15.984391], dtype=float32), array([-0.7988788,  2.1118064], dtype=float32), array([-2.6596842, -0.7377592], dtype=float32), array([ 4.287301 , -2.3106606], dtype=float32), array([-0.3179196,  4.8201675], dtype=float32), array([2.9253   , 1.6279342], dtype=float32), array([ 5.537924 , -3.5121698], dtype=float32), array([ 2.931525, -1.529244], dtype=float32), array([-4.0663533 , -0.70284826], dtype=float32), array([-0.93279654, -0.7078281 ], dtype=float32), array([0.38413024, 6.2208915 ], dtype=float32), array([ 6.015765 , -3.6895666], dtype=float32), array([ 2.9783847, -1.4362078], dtype=float32), array([ 1.8457718, -4.1493187], dtype=float32), array([ 2.2930353, -0.2921523], dtype=float32), array([ 6.121068, -4.829106], dtype=float32), array([ 1.4389343, -1.0056136], dtype=float32), array([2.4487753, 1.0155094], dtype=float32), array([-4.9321012, 12.9378195], dtype=float32), array([-3.487975, 10.127141], dtype=float32), array([1.0617763 , 0.42881507], dtype=float32), array([-0.37724078, -5.5244274 ], dtype=float32), array([ 7.334181 , -2.6478493], dtype=float32), array([ 5.7337627, -1.3140652], dtype=float32), array([-2.2518501, 10.640559 ], dtype=float32), array([-1.0294905, -2.0578773], dtype=float32), array([-0.46942663,  0.4912743 ], dtype=float32), array([ 4.7768126, -1.920903 ], dtype=float32), array([ 4.394988 , -2.9285972], dtype=float32), array([ 4.334594 , -0.6379686], dtype=float32), array([-0.9697996 ,  0.32958406], dtype=float32), array([-1.3147458 , -0.02226168], dtype=float32), array([0.19069707, 8.830102  ], dtype=float32), array([ 2.9459622, -3.2405598], dtype=float32), array([ 2.3742535, -4.6435513], dtype=float32), array([-3.5567539, -2.3721225], dtype=float32), array([ 2.6976092, -1.0806289], dtype=float32), array([ 4.7935953, -2.5062952], dtype=float32), array([3.9000447, 4.5714364], dtype=float32), array([-9.280241,  9.656538], dtype=float32), array([ 3.333545 , -1.4920359], dtype=float32), array([ 3.8292372, -3.3184848], dtype=float32), array([ 5.583436, -3.323811], dtype=float32), array([-1.6965761 , -0.32175046], dtype=float32), array([ 0.9834347, -2.3220177], dtype=float32), array([-5.8852196, 15.379293 ], dtype=float32), array([ 5.007292 , -3.0862856], dtype=float32), array([ 4.46243  , -2.7659483], dtype=float32), array([ 3.719519, -1.712589], dtype=float32), array([ 0.42531097, -2.06186   ], dtype=float32), array([ 3.5111825, -3.6162157], dtype=float32), array([ 4.8830986, -2.751437 ], dtype=float32), array([1.3558707, 1.7738795], dtype=float32), array([ 3.9056542 , -0.65551037], dtype=float32), array([ 5.25183  , -3.1715555], dtype=float32), array([-2.9464147 ,  0.93177634], dtype=float32), array([2.199856, 2.316534], dtype=float32), array([0.8829104 , 0.34928125], dtype=float32), array([-3.3274214, 10.561385 ], dtype=float32), array([-1.9105167,  2.024299 ], dtype=float32), array([ 2.2538793, -4.4725127], dtype=float32), array([ 4.120613 , -0.8135589], dtype=float32), array([ 4.3798647, -1.7526579], dtype=float32), array([ 1.5013947, -3.2743173], dtype=float32), array([-1.9933869,  2.587706 ], dtype=float32), array([-0.8955926,  2.4972303], dtype=float32), array([ 5.755268 , -3.7495904], dtype=float32), array([ 3.852852 , -2.1222925], dtype=float32), array([ 3.9415157, -1.6440401], dtype=float32), array([-3.5093904 ,  0.84162384], dtype=float32), array([-3.62617  , -1.0011363], dtype=float32), array([ 2.987559 , -3.3628848], dtype=float32), array([-1.2894623 , -0.60144144], dtype=float32), array([1.1947451, 6.1437626], dtype=float32), array([2.7576454, 0.5602531], dtype=float32), array([ 1.5139275, 16.015553 ], dtype=float32), array([ 5.987727 , -2.9116693], dtype=float32), array([0.5385573, 2.86283  ], dtype=float32), array([ 4.8591194, -2.1535802], dtype=float32), array([ 5.8971233, -4.13925  ], dtype=float32), array([ 0.02761149, -2.1278794 ], dtype=float32), array([-0.4460444 , -0.76881653], dtype=float32), array([ 1.9498966 , -0.39496064], dtype=float32), array([ 0.55285025, -2.8735209 ], dtype=float32), array([ 4.0073385, -3.6906853], dtype=float32), array([-5.084839, 14.640887], dtype=float32), array([-0.11773849,  2.7941403 ], dtype=float32), array([1.0082943, 2.4833617], dtype=float32), array([ 2.4064715, -2.4231887], dtype=float32), array([1.4681084, 1.2587354], dtype=float32), array([3.9403431 , 0.37091202], dtype=float32), array([ 0.9502249, -3.0903883], dtype=float32), array([ 3.0159504, -3.095211 ], dtype=float32), array([1.1566136, 4.7666373], dtype=float32), array([ 5.473274 , -3.9510508], dtype=float32), array([-2.1607337,  1.1407521], dtype=float32), array([ 4.0095196, -2.519507 ], dtype=float32), array([ 5.6556244, -2.7238884], dtype=float32), array([ 3.0559204, -1.8992093], dtype=float32), array([-0.53172594,  1.6112711 ], dtype=float32), array([-3.3260593,  7.2586765], dtype=float32), array([ 2.2487228, -2.08484  ], dtype=float32), array([ 4.1691856, -2.7073112], dtype=float32), array([0.6651205 , 0.00313932], dtype=float32), array([3.5781329, 7.017856 ], dtype=float32), array([-0.55370575, -3.6436996 ], dtype=float32), array([-1.226896 ,  2.4123778], dtype=float32), array([ 2.3930345, -0.8275104], dtype=float32), array([-0.11450422, 14.9314375 ], dtype=float32), array([ 6.2499924, -1.1419492], dtype=float32), array([-9.321344,  3.386207], dtype=float32), array([ 5.7269497, -3.6234398], dtype=float32), array([2.6357443, 2.2648866], dtype=float32), array([ 4.067766 , -0.5543452], dtype=float32), array([ 5.386837 , -3.6739511], dtype=float32), array([ 2.1980968, -1.5901895], dtype=float32), array([-0.91329306, -0.9093893 ], dtype=float32), array([ 1.8461926, -0.6865795], dtype=float32), array([ 3.386883 , -4.8690386], dtype=float32), array([-3.4471   ,  2.0941799], dtype=float32), array([ 5.302994 , -4.0042267], dtype=float32), array([ 1.625643  , -0.93655545], dtype=float32), array([ 2.5940154, -2.3981967], dtype=float32), array([0.92838085, 0.9919602 ], dtype=float32), array([ 5.226162 , -4.1750956], dtype=float32), array([1.6521032, 3.0106359], dtype=float32), array([-7.2518806, 18.067795 ], dtype=float32), array([ 4.520995, -3.076251], dtype=float32), array([ 5.562275 , -2.9316325], dtype=float32), array([ 3.0961354, -1.7613897], dtype=float32), array([3.1162016, 6.707982 ], dtype=float32), array([0.14170885, 1.2016702 ], dtype=float32), array([ 1.5664608, -1.3641431], dtype=float32), array([ 6.0572453, -3.7657032], dtype=float32), array([0.47088313, 1.5887392 ], dtype=float32), array([ 3.7520401, -4.0302095], dtype=float32), array([-2.3358212, -2.0770218], dtype=float32), array([-0.63381034, -2.5367777 ], dtype=float32), array([-2.4817843, -2.037596 ], dtype=float32), array([-2.4324567, -2.074202 ], dtype=float32), array([ 4.456768 , -3.8001328], dtype=float32), array([0.11913002, 8.011385  ], dtype=float32), array([ 4.4044123, -3.5498524], dtype=float32), array([-4.270198,  4.082919], dtype=float32), array([1.6080742, 3.5596092], dtype=float32), array([0.7226529, 0.9754613], dtype=float32), array([ 4.0031643, -2.020804 ], dtype=float32), array([-3.787597  , -0.10125631], dtype=float32), array([ 0.9561759, -4.910517 ], dtype=float32), array([ 5.152541 , -1.2095971], dtype=float32), array([-8.409221,  7.065525], dtype=float32), array([4.5635967, 1.4821339], dtype=float32), array([ 2.5859997 , -0.79295045], dtype=float32), array([4.754633, 6.942292], dtype=float32), array([ 2.4659436, -3.1384072], dtype=float32), array([0.41845238, 2.3619576 ], dtype=float32), array([ 4.7853146, -4.2849083], dtype=float32), array([ 5.4254208, -3.2737198], dtype=float32), array([ 6.155979 , -3.7318683], dtype=float32), array([2.0526583 , 0.42359906], dtype=float32), array([0.94060016, 0.19521052], dtype=float32), array([ 1.124181, -2.829023], dtype=float32), array([0.5997069, 4.8176756], dtype=float32), array([-0.09879851,  2.2834644 ], dtype=float32), array([ 1.2159169, -1.5998747], dtype=float32), array([ 3.7411282, -4.104479 ], dtype=float32), array([ 3.1627185, -3.757165 ], dtype=float32), array([-0.6492168, -3.7603984], dtype=float32), array([ 3.919667 , -2.7203627], dtype=float32), array([-0.0540334,  2.882432 ], dtype=float32), array([ 4.370325 , -1.5709977], dtype=float32), array([ 1.3528631, -2.8977625], dtype=float32), array([ 1.0372946, -2.380131 ], dtype=float32), array([ 4.1756783, -1.8040745], dtype=float32), array([-6.004841 , 12.3636675], dtype=float32), array([ 2.723904 , -0.9421933], dtype=float32), array([ 2.126077 , -2.0439773], dtype=float32), array([1.3731382, 5.016347 ], dtype=float32), array([ 3.199978 , -1.3440018], dtype=float32), array([2.7675524, 4.858873 ], dtype=float32), array([ 2.7815223 , -0.83213323], dtype=float32), array([0.85348105, 0.62760085], dtype=float32), array([0.22695744, 6.314777  ], dtype=float32), array([-5.452793 ,  3.5277276], dtype=float32), array([-2.1730912, 14.752721 ], dtype=float32), array([2.7663023, 4.1613593], dtype=float32), array([ 6.2418118, -3.6580386], dtype=float32), array([ 4.1842957, -2.0836408], dtype=float32), array([ 5.850543 , -3.4898176], dtype=float32), array([-0.9274435 , -0.17736465], dtype=float32), array([ 2.0149777, -3.5378747], dtype=float32), array([ 1.9005511, -2.9464638], dtype=float32), array([-4.895194,  8.36077 ], dtype=float32), array([ 2.2672436, -4.140336 ], dtype=float32), array([ 1.9578292, -1.4163783], dtype=float32), array([ 2.5503604, -1.9476867], dtype=float32), array([ 5.493556 , -0.3916722], dtype=float32), array([ 2.7764013, -2.2475975], dtype=float32), array([-3.4600694,  5.424881 ], dtype=float32), array([ 1.5730994, -2.5585294], dtype=float32), array([ 5.9874134, -3.325841 ], dtype=float32), array([ 3.7945645, -1.1379437], dtype=float32), array([1.9887133, 1.9989009], dtype=float32), array([-1.1669377,  1.7421074], dtype=float32), array([2.7370574, 4.2170844], dtype=float32), array([-1.5610094,  3.8164604], dtype=float32), array([ 7.118922 , -3.5395584], dtype=float32), array([ 1.4022262, -2.5798945], dtype=float32), array([ 1.1287422, -4.159547 ], dtype=float32), array([ 1.0915099 , -0.66305476], dtype=float32), array([ 3.7541988, -3.7814198], dtype=float32), array([ 0.21281779, 12.294049  ], dtype=float32), array([-0.822122  , -0.37373018], dtype=float32), array([ 6.1427507, -3.9570966], dtype=float32), array([ 2.2854688, -1.3763528], dtype=float32), array([ 3.3700335, -0.8468856], dtype=float32), array([1.1847398, 0.7450182], dtype=float32), array([-7.492585,  5.806478], dtype=float32), array([ 2.0362983, -0.1810177], dtype=float32), array([ 1.3186569, -1.6371686], dtype=float32), array([-0.999606 ,  0.8256646], dtype=float32), array([ 5.8760386, -3.1192727], dtype=float32), array([ 3.3936784, -1.9918804], dtype=float32), array([-5.7377434, 14.3702545], dtype=float32), array([ 4.1468067, -4.5688186], dtype=float32), array([2.9421728, 3.5904684], dtype=float32), array([-1.4184159,  0.6270097], dtype=float32), array([ 4.4245653, -2.5043087], dtype=float32), array([ 1.4824245, -3.5585537], dtype=float32), array([ 5.500655, -1.476578], dtype=float32), array([ 6.491086, -2.700663], dtype=float32), array([4.879896, 6.402805], dtype=float32), array([ 6.8812456, -4.095391 ], dtype=float32), array([0.1793021, 0.9662531], dtype=float32), array([2.3841202 , 0.93877333], dtype=float32), array([ 4.0634136, -3.187153 ], dtype=float32), array([ 3.7107537, -4.25569  ], dtype=float32), array([-5.6775055,  2.2598267], dtype=float32), array([1.6399236, 1.649101 ], dtype=float32), array([-7.3005524,  2.4463716], dtype=float32), array([-0.85043633,  0.67196697], dtype=float32), array([ 0.89261985, -2.8324587 ], dtype=float32), array([ 4.813341 , -1.5083194], dtype=float32), array([0.9233713, 3.7727182], dtype=float32), array([ 6.5690165, -3.1236575], dtype=float32), array([-1.4750957, 11.730428 ], dtype=float32), array([1.0598799, 2.5143228], dtype=float32), array([ 1.8864765, -2.555775 ], dtype=float32), array([ 3.3995945, -1.5199134], dtype=float32), array([ 2.0717914, -3.2333832], dtype=float32), array([-6.347884,  8.837304], dtype=float32), array([ 4.545779 , -1.9979124], dtype=float32), array([-2.4003286,  1.4869893], dtype=float32), array([ 3.3615563 , -0.01893014], dtype=float32), array([ 4.8854313, -1.6348305], dtype=float32), array([ 3.3509467, -1.9228456], dtype=float32), array([ 3.2299135, -3.2062168], dtype=float32), array([ 6.4768333, -1.4814346], dtype=float32), array([ 2.6745272, -5.2281294], dtype=float32), array([0.7208092, 0.5817376], dtype=float32), array([-4.0423794,  4.6387835], dtype=float32), array([ 4.909507 , -2.3819926], dtype=float32), array([-4.5996857, -1.9035912], dtype=float32), array([-5.591879,  7.978736], dtype=float32), array([ 1.4368973, -0.5069304], dtype=float32), array([-0.35144776, -1.130472  ], dtype=float32), array([-7.0104637, 18.863508 ], dtype=float32), array([ 1.2310455 , -0.96570855], dtype=float32), array([3.3151143 , 0.02437705], dtype=float32), array([ 2.305398 , -2.7081976], dtype=float32), array([ 5.319541 , -2.7655492], dtype=float32), array([ 2.5404058 , -0.76228374], dtype=float32), array([-5.7482824 , -0.70137924], dtype=float32), array([ 5.4480333, -1.6556017], dtype=float32), array([ 4.217001, -4.369751], dtype=float32), array([-0.46244425, -2.505761  ], dtype=float32), array([ 1.020142 , -1.7350247], dtype=float32), array([3.4592297, 1.312677 ], dtype=float32), array([ 6.0412903, -3.3275871], dtype=float32), array([ 4.2990303, -2.5660536], dtype=float32), array([ 0.62207866, -0.20504767], dtype=float32), array([4.605647 , 2.3859112], dtype=float32), array([ 4.3330164, -1.300889 ], dtype=float32), array([-7.6184387,  5.473987 ], dtype=float32), array([ 1.3016965, -2.8682451], dtype=float32), array([-2.8408518, -0.6230424], dtype=float32), array([ 2.952727 , -2.8258007], dtype=float32), array([ 2.182454 , 10.3991165], dtype=float32), array([ 3.9947646, -3.9690204], dtype=float32), array([ 3.0555403, -1.2171183], dtype=float32), array([4.5891867, 7.844636 ], dtype=float32), array([2.078289  , 0.33701503], dtype=float32), array([-5.778631, 13.663753], dtype=float32), array([ 2.1916444, -1.1579769], dtype=float32), array([ 6.3667936, -4.7591248], dtype=float32), array([ 2.2409217, -3.9594378], dtype=float32), array([2.709346, 3.021024], dtype=float32), array([-0.73267454, -2.3947399 ], dtype=float32), array([ 3.459163 , -3.6236672], dtype=float32), array([ 4.328553 , -2.7306626], dtype=float32), array([ 3.8626006, -3.4942722], dtype=float32), array([ 3.553559, -1.013897], dtype=float32), array([ 0.27541983, -1.2250276 ], dtype=float32), array([2.446624 , 0.8120603], dtype=float32), array([-0.25495148,  0.1065709 ], dtype=float32), array([ 5.163213 , -2.7902658], dtype=float32), array([2.103306, 1.677999], dtype=float32), array([ 3.9556992, -4.489334 ], dtype=float32), array([-2.0577042,  1.0870628], dtype=float32), array([ 2.870787 , -1.0751679], dtype=float32), array([ 2.4806793 , -0.42006785], dtype=float32), array([0.4413625, 1.8417945], dtype=float32), array([ 2.9311988, -4.5021477], dtype=float32), array([ 3.8799188, -2.139327 ], dtype=float32), array([ 0.9376409, -2.0899153], dtype=float32), array([-3.7037199, 24.191013 ], dtype=float32), array([ 3.7603395, -1.3957291], dtype=float32), array([-4.3253407 , -0.08632189], dtype=float32), array([ 1.5063694, -2.9326434], dtype=float32), array([ 3.210683 , -2.9244294], dtype=float32), array([ 3.7515357, -2.6722941], dtype=float32), array([0.9240625, 1.1542959], dtype=float32), array([ 1.3777664, -0.9520015], dtype=float32), array([-4.700495 ,  2.6213446], dtype=float32), array([4.0889473, 1.3315163], dtype=float32), array([ 4.185973 , -2.4147327], dtype=float32), array([-7.325238,  9.236083], dtype=float32), array([ 1.363529 , -0.5710359], dtype=float32), array([0.26035964, 2.076458  ], dtype=float32), array([1.5959222 , 0.30174214], dtype=float32), array([2.9385145 , 0.12334961], dtype=float32), array([ 4.055772 , -3.3433542], dtype=float32), array([ 5.871751 , -4.5223513], dtype=float32), array([ 2.3953066, -2.225457 ], dtype=float32), array([ 4.105851 , -5.2913117], dtype=float32), array([ 4.844528 , -2.6394303], dtype=float32), array([-1.8969879, -1.1209443], dtype=float32), array([-5.7324514, 15.548932 ], dtype=float32), array([ 4.6543036, -3.808229 ], dtype=float32), array([ 1.9720914 , -0.77317864], dtype=float32), array([0.39433944, 0.21994501], dtype=float32), array([ 5.798979 , -2.7214298], dtype=float32), array([3.6830142, 2.1850111], dtype=float32), array([1.8976681, 1.3672607], dtype=float32), array([2.28601   , 0.24575776], dtype=float32), array([-1.4233208, -2.27461  ], dtype=float32), array([ 4.296646, -4.721583], dtype=float32), array([ 5.74403  , -3.1036787], dtype=float32), array([-3.0994647,  5.6188045], dtype=float32), array([ 2.5377529 , -0.73409384], dtype=float32), array([ 4.5219297, -3.5246224], dtype=float32), array([-4.0247946,  3.3622875], dtype=float32), array([ 4.3139286, -1.5840034], dtype=float32), array([-1.6281958,  3.4026537], dtype=float32), array([1.1375291, 5.4748006], dtype=float32), array([1.4502776, 2.794804 ], dtype=float32), array([ 2.6529403, -0.9553755], dtype=float32), array([ 5.7607   , -2.8584309], dtype=float32), array([ 3.879864 , -1.9858372], dtype=float32), array([-5.2928143,  7.814348 ], dtype=float32), array([ 0.17628324, -2.218397  ], dtype=float32), array([3.2088706 , 0.40320796], dtype=float32), array([ 4.2971344, -3.433637 ], dtype=float32), array([1.4260967, 1.9498951], dtype=float32), array([3.5897076, 1.9281077], dtype=float32), array([ 4.4176073, -1.1137149], dtype=float32), array([-1.7694521, -0.659488 ], dtype=float32), array([2.7983587, 3.2580094], dtype=float32), array([-5.524866 ,  3.5993686], dtype=float32), array([-0.6853149,  5.9009123], dtype=float32), array([-1.6055872 , -0.29404563], dtype=float32), array([-4.9398994,  7.8253403], dtype=float32), array([-2.09439  ,  8.4279785], dtype=float32), array([ 3.2962697, -2.979006 ], dtype=float32), array([ 3.8122437, -2.6495187], dtype=float32), array([4.0126266 , 0.21976429], dtype=float32), array([ 1.6402326, -2.742716 ], dtype=float32), array([ 6.9734774, -2.782291 ], dtype=float32), array([ 0.4950434, -2.5713599], dtype=float32), array([ 5.609088 , -3.7869382], dtype=float32), array([ 2.8736758, -3.0980937], dtype=float32), array([ 3.4653695, -3.713365 ], dtype=float32), array([ 3.799949 , -2.7982678], dtype=float32), array([-5.232154 ,  2.0572438], dtype=float32), array([ 4.499172 , -1.5011606], dtype=float32), array([ 6.636284, -2.958534], dtype=float32), array([ 3.2512443, -4.7446156], dtype=float32), array([0.8311391, 7.6580954], dtype=float32), array([ 2.6145844 , -0.18070191], dtype=float32), array([ 1.4390287, -1.4826765], dtype=float32), array([ 4.4090424, -3.210873 ], dtype=float32), array([-7.957032,  7.32238 ], dtype=float32), array([ 2.7566142, -3.4931693], dtype=float32), array([3.0101323, 1.1325893], dtype=float32), array([ 1.212168  , -0.02854818], dtype=float32), array([ 1.6468599, -1.3818712], dtype=float32), array([2.2074277, 1.0114684], dtype=float32), array([ 2.9443073, -2.212718 ], dtype=float32), array([3.1134465 , 0.55219406], dtype=float32), array([ 4.114146  , -0.29165787], dtype=float32), array([-0.256963,  8.084847], dtype=float32), array([-2.4844978,  1.0865934], dtype=float32), array([ 2.3392746, -3.8828168], dtype=float32), array([-8.266491, 17.880058], dtype=float32), array([-6.995673,  4.155743], dtype=float32), array([ 5.018152 , -2.3558302], dtype=float32), array([ 5.339941 , -1.6656165], dtype=float32), array([2.8690624, 1.7989712], dtype=float32), array([0.06503165, 1.2089701 ], dtype=float32), array([ 2.0149062, -0.6201108], dtype=float32), array([ 4.358247 , -3.0516374], dtype=float32), array([-4.602848,  3.879376], dtype=float32), array([ 3.1346037, -3.0293016], dtype=float32), array([ 0.9216018, -3.3203483], dtype=float32), array([ 5.215126, -4.276653], dtype=float32), array([ 6.1052933, -3.2853265], dtype=float32), array([ 4.477086 , -1.7891545], dtype=float32), array([5.9928684e+00, 1.9592047e-04], dtype=float32), array([0.6201656, 7.473036 ], dtype=float32), array([ 2.090258 , -1.3184631], dtype=float32), array([ 1.7407067, -0.3286245], dtype=float32), array([-0.02263176, -3.5627275 ], dtype=float32), array([ 3.5446975, -1.6854899], dtype=float32), array([ 1.1355782, -2.5121694], dtype=float32), array([2.6106145, 2.3190637], dtype=float32), array([ 5.256833 , -2.4692903], dtype=float32), array([-0.39130634, -2.1613116 ], dtype=float32), array([ 1.7840064, -6.005374 ], dtype=float32), array([0.8270937 , 0.48250216], dtype=float32), array([1.4957554, 1.0753548], dtype=float32), array([ 0.14125168, -2.6447396 ], dtype=float32), array([ 3.0766222, -3.8718648], dtype=float32), array([-0.6896616, -1.0894785], dtype=float32), array([ 5.7329254, -1.8629851], dtype=float32), array([-4.5686417, 14.657994 ], dtype=float32), array([-2.348939 ,  3.4908466], dtype=float32), array([ 3.9727128, -3.27322  ], dtype=float32), array([-0.41122496, -0.40194082], dtype=float32), array([ 3.744671, -4.525144], dtype=float32), array([-0.15648592,  4.972254  ], dtype=float32), array([4.0152283, 4.127195 ], dtype=float32), array([ 5.502408 , -1.0872102], dtype=float32), array([0.6103122, 3.6325197], dtype=float32), array([2.3751724 , 0.15367633], dtype=float32), array([-0.04141927,  4.458149  ], dtype=float32), array([ 7.4997807, -1.7977824], dtype=float32), array([ 1.3396277, -3.315681 ], dtype=float32), array([ 2.6444352 , -0.95946413], dtype=float32), array([4.924177  , 0.12486452], dtype=float32), array([ 5.3777094, -3.0902095], dtype=float32), array([ 4.525091 , -3.9938288], dtype=float32), array([ 1.0061959, -1.2191958], dtype=float32), array([-2.3769994,  3.0964842], dtype=float32), array([ 4.409724, -3.624507], dtype=float32), array([ 3.0505006, -5.3457675], dtype=float32), array([ 0.8961642, -0.8938666], dtype=float32), array([ 3.4471238, -1.2948229], dtype=float32), array([-2.0948315, 18.405382 ], dtype=float32), array([-4.541423,  4.935261], dtype=float32), array([ 2.5899198, -3.1068265], dtype=float32), array([1.3646572, 1.3889868], dtype=float32), array([-6.085781 ,  2.7916188], dtype=float32), array([ 2.5512393, -1.7694435], dtype=float32), array([-0.12373674,  3.1990426 ], dtype=float32), array([3.9690135, 5.3317432], dtype=float32), array([-0.80982745, 12.390248  ], dtype=float32), array([ 1.4318013, -2.5155063], dtype=float32), array([-2.3586774,  1.2668116], dtype=float32), array([ 4.144229, -2.672885], dtype=float32), array([ 3.1887195 , -0.21246153], dtype=float32), array([ 1.7083871, -3.507587 ], dtype=float32), array([2.8442166, 1.8793769], dtype=float32), array([0.12724507, 6.9719343 ], dtype=float32), array([ 4.688919 , -4.6733136], dtype=float32), array([0.48709512, 0.5397571 ], dtype=float32), array([ 4.582947 , -1.6116114], dtype=float32), array([ 3.3568242, -3.0736544], dtype=float32), array([-1.1728046, -1.3707364], dtype=float32), array([-0.02807575,  1.6374826 ], dtype=float32), array([ 2.7580864, -1.3219113], dtype=float32), array([-1.3178438, -4.223315 ], dtype=float32), array([ 1.752886 , -3.7793121], dtype=float32), array([ 2.0043242, -2.0888271], dtype=float32), array([-4.726911 ,  3.2405298], dtype=float32), array([ 5.8339863, -3.8824859], dtype=float32), array([-1.4473854, -1.4447663], dtype=float32), array([1.8688908, 7.262788 ], dtype=float32), array([-1.9286039,  1.2821519], dtype=float32), array([-2.1203778,  3.7015624], dtype=float32), array([4.052684 , 1.1929216], dtype=float32), array([4.1343184, 1.1135128], dtype=float32), array([ 3.5971596, -6.442162 ], dtype=float32), array([2.0241413, 3.877174 ], dtype=float32), array([ 3.3153746, -3.0294552], dtype=float32), array([ 2.858697 , -3.7308502], dtype=float32), array([ 5.5746775, -3.711728 ], dtype=float32), array([ 1.5549457, -4.393399 ], dtype=float32), array([ 0.35953093, -2.4839525 ], dtype=float32), array([-2.8963127, 14.680565 ], dtype=float32), array([ 0.794153, -2.106017], dtype=float32), array([2.2150183 , 0.00703132], dtype=float32), array([0.6070814, 1.7963107], dtype=float32), array([ 5.0272694, -2.3953824], dtype=float32), array([ 5.796837 , -3.5214386], dtype=float32), array([ 1.6989186, -3.909378 ], dtype=float32), array([ 0.72827935, -0.6520167 ], dtype=float32), array([2.029116, 4.679117], dtype=float32), array([ 4.91714 , -4.060256], dtype=float32), array([1.9027092, 3.25043  ], dtype=float32), array([-1.3024704, -1.0962546], dtype=float32), array([0.04685366, 8.794884  ], dtype=float32), array([ 1.720593 , -2.1560073], dtype=float32), array([ 4.1832514, -3.8980522], dtype=float32), array([-0.79484975, -0.17232937], dtype=float32), array([-6.771988,  3.832996], dtype=float32), array([ 2.5636294, -3.819499 ], dtype=float32), array([ 4.50721  , -4.0269527], dtype=float32), array([ 2.7876997, -3.6108313], dtype=float32), array([ 3.9658782 , -0.28485966], dtype=float32), array([3.5233257, 9.057981 ], dtype=float32), array([ 1.8940129, -2.6150553], dtype=float32), array([6.692539 , 1.4879928], dtype=float32), array([-6.9740286,  5.6944504], dtype=float32), array([-2.4328644, -0.8484265], dtype=float32), array([ 1.0571563, -1.5853012], dtype=float32), array([ 4.900222 , -4.7920504], dtype=float32), array([ 2.3181121, -1.1225078], dtype=float32), array([0.9943167 , 0.49505812], dtype=float32), array([-2.9630115 ,  0.33879656], dtype=float32), array([ 4.884082 , -1.8073895], dtype=float32), array([ 4.5442343, -4.565354 ], dtype=float32), array([3.9084876, 1.2926565], dtype=float32), array([0.5262567, 3.167636 ], dtype=float32), array([ 2.387481 , -1.3975108], dtype=float32), array([-1.2465665, -5.2056856], dtype=float32), array([ 4.2596893, -1.7471159], dtype=float32), array([ 7.485794, -4.00075 ], dtype=float32), array([0.61834586, 4.99428   ], dtype=float32), array([ 0.7433975, 14.480933 ], dtype=float32), array([2.6197994, 1.4355533], dtype=float32), array([5.0134315, 0.3564821], dtype=float32), array([-5.3940763,  1.8435383], dtype=float32), array([-1.9184065,  2.5924833], dtype=float32), array([ 2.352856 , -2.2908845], dtype=float32), array([-0.73704445,  5.8067837 ], dtype=float32), array([ 2.010945 , -0.7791572], dtype=float32), array([ 2.5407777, -3.2071202], dtype=float32), array([ 5.6662235, -3.516386 ], dtype=float32), array([ 6.612053 , -3.5374494], dtype=float32), array([5.8020554, 0.3054164], dtype=float32), array([ 3.747636  , -0.25132614], dtype=float32), array([3.2170055, 4.8918834], dtype=float32), array([-4.554061,  2.987166], dtype=float32), array([4.1841993, 0.6175708], dtype=float32), array([-3.239106 ,  5.0235224], dtype=float32), array([4.3549223, 2.618651 ], dtype=float32), array([ 2.9782689, -3.1352334], dtype=float32), array([0.28466296, 3.63243   ], dtype=float32), array([-5.30846 ,  4.421939], dtype=float32), array([-1.0657017, -2.1152987], dtype=float32), array([-2.9168699, -2.3214285], dtype=float32), array([ 6.1116343, -2.895894 ], dtype=float32), array([ 0.9094473, -2.0283628], dtype=float32), array([-3.3251002,  0.3157248], dtype=float32), array([ 5.595253, -4.898739], dtype=float32), array([1.3269198, 1.3140378], dtype=float32), array([0.7765708, 7.8437724], dtype=float32), array([ 3.9564726, -1.0537283], dtype=float32), array([-0.14975107, -0.11403042], dtype=float32), array([0.3889246 , 0.56556684], dtype=float32), array([-0.41889882,  1.8809958 ], dtype=float32), array([-6.096426 ,  2.4234064], dtype=float32), array([2.4593885, 1.8512037], dtype=float32), array([ 4.9109945, -1.8984513], dtype=float32), array([-0.27101135, -0.79530114], dtype=float32), array([ 3.8905413 , -0.94633514], dtype=float32), array([ 6.6305027, -4.873186 ], dtype=float32), array([ 7.0983896, -3.2891748], dtype=float32), array([-2.6150389, 13.923377 ], dtype=float32), array([ 2.65621  , -3.0243673], dtype=float32), array([ 1.2249818, -2.041491 ], dtype=float32), array([ 0.10722196, -1.9148571 ], dtype=float32), array([ 3.825598 , -1.5983658], dtype=float32), array([ 4.378895 , -4.8752303], dtype=float32), array([ 2.538644, -3.086619], dtype=float32), array([ 2.7913415, -3.6911316], dtype=float32), array([-3.2582328, 12.354308 ], dtype=float32), array([-1.428479 , -2.8658276], dtype=float32), array([4.199188 , 1.0423291], dtype=float32), array([ 1.7438583, -2.2052543], dtype=float32), array([0.24917269, 1.04901   ], dtype=float32), array([ 0.67497754, -1.3942492 ], dtype=float32), array([-0.77964395,  1.614444  ], dtype=float32), array([0.64330256, 9.457566  ], dtype=float32), array([-2.0895324, -1.456363 ], dtype=float32), array([2.0779674 , 0.02771622], dtype=float32), array([ 2.9119604, -5.3053575], dtype=float32), array([ 5.0096464, -3.5239697], dtype=float32), array([ 0.9926059, -1.8272486], dtype=float32), array([2.5513031, 5.6415224], dtype=float32), array([-3.0640404,  7.0129175], dtype=float32), array([ 6.9142036, -2.9062629], dtype=float32), array([ 4.0077   , -0.7996016], dtype=float32), array([3.5660136, 0.784669 ], dtype=float32), array([ 4.9498234, -1.6465971], dtype=float32), array([-1.1482898 , -0.33321565], dtype=float32), array([-0.9112823, -2.2779148], dtype=float32), array([ 3.7797692, -2.896991 ], dtype=float32), array([ 1.7332337, -2.200324 ], dtype=float32), array([ 2.503167 , -3.1573718], dtype=float32), array([ 6.488574 , -3.9724374], dtype=float32), array([ 3.8603928, -2.921284 ], dtype=float32), array([ 4.642435  , -0.44627577], dtype=float32), array([1.4630306, 3.2497406], dtype=float32), array([-4.3982153,  3.6517558], dtype=float32), array([-0.54543054, -1.006279  ], dtype=float32), array([ 4.6830006, -4.0691414], dtype=float32), array([1.2891874, 6.0808997], dtype=float32), array([-5.4081116,  9.494191 ], dtype=float32), array([ 7.4912605, -3.419258 ], dtype=float32), array([-1.4943268,  6.2722783], dtype=float32), array([-4.8690586,  3.3546054], dtype=float32), array([ 2.935301 , -3.7825332], dtype=float32), array([ 3.4760687, -4.7150636], dtype=float32), array([3.129615 , 1.6570842], dtype=float32), array([ 1.1204977 , -0.38455027], dtype=float32), array([ 3.5922592, -0.6532294], dtype=float32), array([-0.32329297, -1.0273688 ], dtype=float32), array([-4.3169985,  6.7054386], dtype=float32), array([ 0.03572106, -2.5826597 ], dtype=float32), array([ 4.5369377, -3.930944 ], dtype=float32), array([-3.8892143,  1.6412425], dtype=float32), array([ 4.248934  , -0.04316455], dtype=float32), array([-5.959773,  9.720946], dtype=float32), array([ 4.233554 , -2.4702492], dtype=float32), array([ 0.314363, 11.476495], dtype=float32), array([ 4.6532106, -4.3153906], dtype=float32), array([ 4.916048 , -2.0919871], dtype=float32), array([1.7398994, 3.1004348], dtype=float32), array([ 3.3834517, -2.6351957], dtype=float32), array([ 3.4070437, -2.1317725], dtype=float32), array([ 3.0638878, -3.4933138], dtype=float32), array([-0.42244375, -1.1519487 ], dtype=float32), array([3.6182015, 2.8974514], dtype=float32), array([ 0.4262476, -3.104236 ], dtype=float32), array([-2.960682, 10.323472], dtype=float32), array([3.984954  , 0.80019325], dtype=float32), array([ 0.96994925, -3.2365236 ], dtype=float32), array([0.67083406, 4.610963  ], dtype=float32), array([ 2.3101056, -1.5962   ], dtype=float32), array([3.2138112 , 0.41423494], dtype=float32), array([ 3.3888032, -0.8420797], dtype=float32), array([1.6603274, 2.004627 ], dtype=float32), array([ 4.7240515, -0.6345349], dtype=float32), array([ 3.360708 , -1.2325752], dtype=float32), array([ 3.0495107, -3.098785 ], dtype=float32), array([-7.086214, 11.838782], dtype=float32), array([1.6645317 , 0.20833343], dtype=float32), array([ 2.800846 , -3.0484314], dtype=float32), array([ 1.0389351, -3.7113857], dtype=float32), array([ 3.2254221, -2.643406 ], dtype=float32), array([ 0.80405784, -1.8995802 ], dtype=float32), array([ 0.9494276, -4.012529 ], dtype=float32), array([-2.4620564,  3.1398587], dtype=float32), array([ 3.4393275, -1.0080366], dtype=float32), array([ 3.180652  , -0.49183065], dtype=float32), array([ 6.1008253, -2.8803194], dtype=float32), array([2.9714673, 5.0414734], dtype=float32), array([ 4.1830597, -4.4645243], dtype=float32), array([3.1181428, 6.1514335], dtype=float32), array([1.0243927, 1.7969792], dtype=float32), array([ 2.209782 , -4.8391066], dtype=float32), array([-4.4900007,  1.5983155], dtype=float32), array([ 2.887289, -3.574688], dtype=float32), array([ 4.8906918, -2.7536087], dtype=float32), array([ 0.3479445, -0.6150746], dtype=float32), array([-4.874894 ,  1.5296443], dtype=float32), array([-2.1509473 , -0.25062186], dtype=float32), array([2.0379384, 3.845421 ], dtype=float32), array([-2.6975853,  8.800697 ], dtype=float32), array([4.909423  , 0.50737685], dtype=float32), array([ 5.91833  , -3.0687208], dtype=float32), array([ 0.8300115, -4.6764603], dtype=float32), array([ 4.757881  , -0.04402864], dtype=float32), array([ 1.9874165, -0.8342653], dtype=float32), array([ 1.5132   , -2.8361511], dtype=float32), array([ 4.1794825, -4.3896437], dtype=float32), array([ 4.351119 , -2.1440477], dtype=float32), array([ 2.8075526, -3.41048  ], dtype=float32), array([ 1.0695871, -2.3673337], dtype=float32), array([-0.24175364,  5.100992  ], dtype=float32), array([0.5409062, 2.3736866], dtype=float32), array([-3.286611,  1.069885], dtype=float32), array([ 2.8982558, -4.949054 ], dtype=float32), array([2.067788, 5.311589], dtype=float32), array([-3.0734038,  2.3712952], dtype=float32), array([ 2.0975645, -3.6248245], dtype=float32), array([-0.79689884,  6.7462873 ], dtype=float32), array([ 2.8590648, -4.416446 ], dtype=float32), array([-0.6207392, 13.238181 ], dtype=float32), array([ 2.7200387, -2.1727753], dtype=float32), array([ 3.0727208, -2.3745732], dtype=float32), array([ 2.630137 , -1.0213897], dtype=float32), array([ 5.4355946, -2.7815576], dtype=float32), array([ 3.683533 , -3.3422332], dtype=float32), array([-4.9897738,  5.702732 ], dtype=float32), array([2.753381 , 0.4970851], dtype=float32), array([ 2.745535 , -4.8075266], dtype=float32), array([ 5.5901318, -3.1819289], dtype=float32), array([ 2.7403812, -2.4680996], dtype=float32), array([0.4884591, 6.439273 ], dtype=float32), array([ 3.7921774, -4.8540287], dtype=float32), array([ 0.19041884, -1.3458054 ], dtype=float32), array([4.104683  , 0.53971547], dtype=float32), array([-4.4926434 ,  0.43753642], dtype=float32), array([3.7877486, 3.355339 ], dtype=float32), array([0.86256933, 1.4241371 ], dtype=float32), array([-2.2150693,  4.003111 ], dtype=float32), array([-4.6687665, 11.306031 ], dtype=float32), array([ 1.3465593, -2.381005 ], dtype=float32), array([ 6.0714445, -3.214126 ], dtype=float32), array([ 5.787154 , -3.5346165], dtype=float32), array([2.4206634, 5.0334272], dtype=float32), array([-4.514578, 13.474111], dtype=float32), array([ 1.2281601, -2.2163763], dtype=float32), array([ 3.0170171, -3.6458445], dtype=float32), array([ 0.7771876, -1.0628741], dtype=float32), array([-3.1948044,  2.56768  ], dtype=float32), array([-4.5483418,  7.9618263], dtype=float32), array([ 3.4870112, -1.2767503], dtype=float32), array([2.2500265, 4.888476 ], dtype=float32), array([ 3.3028486, -2.287813 ], dtype=float32), array([ 2.8102033, -0.2845344], dtype=float32), array([ 4.155529, -3.384501], dtype=float32), array([ 4.6543226, -4.664154 ], dtype=float32), array([-5.4479704, -1.8515577], dtype=float32), array([3.0580177, 7.942882 ], dtype=float32), array([ 1.1488116, -3.2625623], dtype=float32), array([ 4.229434, -3.466063], dtype=float32), array([ 5.794738 , -1.5414019], dtype=float32), array([ 3.5135853 , -0.02749568], dtype=float32), array([ 5.608494 , -2.2287726], dtype=float32), array([ 0.03660834, -2.131361  ], dtype=float32), array([-6.886799,  3.073797], dtype=float32), array([-4.0813093, -0.9874061], dtype=float32), array([ 5.295479 , -2.4494367], dtype=float32), array([ 4.9690905, -4.31704  ], dtype=float32), array([2.290258 , 0.6955722], dtype=float32), array([ 7.0112953, -1.3384051], dtype=float32), array([0.9970522, 6.7719693], dtype=float32), array([3.3936684, 1.4791977], dtype=float32), array([ 3.2518437 , -0.58979374], dtype=float32), array([ 3.442507 , -3.1593244], dtype=float32), array([ 3.3347056, -1.5338621], dtype=float32), array([ 0.2689736, -3.208713 ], dtype=float32), array([0.7883915, 1.9030769], dtype=float32), array([ 7.299505 , -2.1308584], dtype=float32), array([ 3.814669 , -2.3234284], dtype=float32), array([ 1.3735511, -3.9730988], dtype=float32), array([ 1.8372471, -1.9964991], dtype=float32), array([-7.174135,  8.892894], dtype=float32), array([-2.9452417, 10.440225 ], dtype=float32), array([ 3.9371536, -1.903321 ], dtype=float32), array([-1.2966133, -2.792173 ], dtype=float32), array([ 1.9859815, -2.2502186], dtype=float32), array([ 3.4307258, -3.8656788], dtype=float32), array([ 6.5204086, -3.276404 ], dtype=float32), array([ 7.1336193, -3.9035816], dtype=float32), array([0.6638372, 5.2138557], dtype=float32), array([ 5.2087336, -2.8781908], dtype=float32), array([-0.5177078 ,  0.31319916], dtype=float32), array([ 3.4116032, -3.0298402], dtype=float32), array([-2.3769112,  9.21881  ], dtype=float32), array([ 3.952048 , -1.6495795], dtype=float32), array([ 4.546919 , -4.1493397], dtype=float32), array([-5.0700445,  1.8683624], dtype=float32), array([0.28921866, 1.9302332 ], dtype=float32), array([ 0.57322085, -2.4335847 ], dtype=float32), array([6.041135 , 2.4109285], dtype=float32), array([ 0.11552286, -0.03166741], dtype=float32), array([-1.2425938,  5.613146 ], dtype=float32), array([-1.2842511,  2.8734503], dtype=float32), array([-0.10938907,  4.134305  ], dtype=float32), array([ 1.4821794, -3.191859 ], dtype=float32), array([-4.8014975, 12.348251 ], dtype=float32), array([-0.4511671,  5.6017423], dtype=float32), array([-3.2080283,  4.616085 ], dtype=float32), array([ 6.4033318, -3.9935327], dtype=float32), array([-1.4513366, -0.2654091], dtype=float32), array([ 1.9674585, -2.194685 ], dtype=float32), array([ 0.20067358, -0.6848194 ], dtype=float32), array([ 1.684336, -3.679346], dtype=float32), array([2.4263482 , 0.11983818], dtype=float32), array([ 4.835538 , -1.2144763], dtype=float32), array([1.3177977, 9.688429 ], dtype=float32), array([ 4.3624525, -1.4894323], dtype=float32), array([ 4.73837  , -3.4964375], dtype=float32), array([ 1.4532435, -1.6747372], dtype=float32), array([ 2.6377873, -1.4827967], dtype=float32), array([ 5.7395372, -5.084304 ], dtype=float32), array([-3.831639 ,  0.6892592], dtype=float32), array([1.8595521, 2.0783973], dtype=float32), array([ 3.613251 , -2.7520983], dtype=float32), array([1.9196298, 4.0375786], dtype=float32), array([ 3.83801  , -1.0641892], dtype=float32), array([ 0.53281283, -2.3882487 ], dtype=float32), array([ 2.4394915, -1.2733142], dtype=float32), array([-5.1000576, 12.780206 ], dtype=float32), array([-1.285189 , -2.8996406], dtype=float32), array([-0.16430205,  0.8204532 ], dtype=float32), array([ 6.1131706, -1.8270679], dtype=float32), array([ 4.7258387, -5.209011 ], dtype=float32), array([-6.999242,  4.161932], dtype=float32), array([ 0.6385813 , -0.55665356], dtype=float32), array([ 5.7554245, -2.4858131], dtype=float32), array([3.4325945, 5.2131166], dtype=float32), array([ 4.1950235, -2.7070036], dtype=float32), array([ 4.9345465, -3.120324 ], dtype=float32), array([ 6.3576984, -3.7462692], dtype=float32), array([ 2.3838632, -1.9513743], dtype=float32), array([-0.20082855, -1.3131478 ], dtype=float32), array([-3.113114, 10.40399 ], dtype=float32), array([ 2.5991797, -1.860703 ], dtype=float32), array([-3.4279058,  2.864913 ], dtype=float32), array([-5.191983 ,  2.0634038], dtype=float32), array([1.398844 , 4.7299476], dtype=float32), array([ 4.654584 , -0.3119794], dtype=float32), array([4.4142084 , 0.02605349], dtype=float32), array([-1.6996524, -2.4238122], dtype=float32), array([5.1253605, 0.3037067], dtype=float32), array([-0.33400685,  1.937129  ], dtype=float32), array([ 1.6337261, -2.9801855], dtype=float32), array([ 2.8256238, -2.5279827], dtype=float32), array([ 0.01681113, -3.6930642 ], dtype=float32), array([ 5.468238, -3.34066 ], dtype=float32), array([2.2055867, 1.2504697], dtype=float32), array([ 3.0604477, -2.47402  ], dtype=float32), array([ 3.1559174, -4.539113 ], dtype=float32), array([-1.2825366, 12.214347 ], dtype=float32), array([ 0.3968755, -2.3638043], dtype=float32), array([2.6609867 , 0.03960574], dtype=float32), array([-0.2260201,  6.477233 ], dtype=float32), array([ 5.542034, -4.071987], dtype=float32), array([3.0760248, 5.0700774], dtype=float32), array([ 4.0297794, -2.040842 ], dtype=float32), array([ 3.8995202, -1.1702392], dtype=float32), array([ 4.076619 , -3.4635267], dtype=float32), array([-3.0300987,  3.6872754], dtype=float32), array([-0.86441714, -3.3147764 ], dtype=float32), array([ 4.680724 , -3.5133572], dtype=float32), array([1.5322945 , 0.50977796], dtype=float32), array([ 4.095357 , -3.3380046], dtype=float32), array([-0.85428923,  2.3947453 ], dtype=float32), array([ 3.9783165, -4.410319 ], dtype=float32), array([ 3.4945605, -1.8253827], dtype=float32), array([-5.344511 ,  3.6951861], dtype=float32), array([1.7659838, 4.7873087], dtype=float32), array([ 4.244212, -1.465086], dtype=float32), array([ 2.8648999, -1.4281137], dtype=float32), array([-0.53096014, -2.146541  ], dtype=float32), array([-3.5775583, 10.1548395], dtype=float32), array([ 3.2001894, -1.5787129], dtype=float32), array([0.45166183, 0.8566877 ], dtype=float32), array([0.9710363, 0.3754329], dtype=float32), array([ 1.4528129, -2.7191741], dtype=float32), array([ 5.7166214, -2.928775 ], dtype=float32), array([ 1.5410535, -3.3222408], dtype=float32), array([2.1945417, 1.8508489], dtype=float32), array([ 1.6968324, -4.569131 ], dtype=float32), array([-2.505117, 14.636354], dtype=float32), array([ 3.4327214, -2.131782 ], dtype=float32), array([3.3339627 , 0.47545165], dtype=float32), array([ 2.1616793, -1.7876723], dtype=float32), array([ 0.7736769, -1.3196294], dtype=float32), array([1.3055737, 1.5110002], dtype=float32), array([ 3.07602   , -0.66143996], dtype=float32), array([ 2.4161856, -5.091891 ], dtype=float32), array([-7.753336, 10.349471], dtype=float32), array([ 5.258479, -3.156878], dtype=float32), array([ 2.7295601, -4.6994658], dtype=float32), array([-1.5483872,  6.2010717], dtype=float32), array([ 4.44536 , -4.164571], dtype=float32), array([3.0379498, 4.546281 ], dtype=float32), array([-7.692642 , -1.2210848], dtype=float32), array([ 3.3396556, -1.9805071], dtype=float32), array([ 5.7105293, -2.2137349], dtype=float32), array([-3.2737727, 18.151352 ], dtype=float32), array([-0.13373518, -0.13474524], dtype=float32), array([ 0.44552946, -2.3532047 ], dtype=float32), array([2.0268877, 1.8110018], dtype=float32), array([1.8112919, 3.7987456], dtype=float32), array([ 6.7198153, -3.1798167], dtype=float32), array([ 4.3250713, -2.99191  ], dtype=float32), array([ 2.908671 , -1.1801238], dtype=float32), array([ 4.0803337, -2.190716 ], dtype=float32), array([ 4.3628063, -5.356202 ], dtype=float32), array([ 3.8366044, -0.4771697], dtype=float32), array([ 3.2739441, -0.5888607], dtype=float32), array([-4.6687937,  1.2683473], dtype=float32), array([2.7690196 , 0.67573076], dtype=float32), array([ 1.9068508, -2.7826836], dtype=float32), array([ 3.3309453, -2.307765 ], dtype=float32), array([-4.4549446,  6.054584 ], dtype=float32), array([-3.267046,  1.718097], dtype=float32), array([ 3.220658 , -1.8813081], dtype=float32), array([ 2.597032 , -2.2029047], dtype=float32), array([ 3.7772443, -4.1424727], dtype=float32), array([3.3891184, 7.315005 ], dtype=float32), array([ 2.3688447, -1.743623 ], dtype=float32), array([ 4.9221096, -3.0357513], dtype=float32), array([1.2107646, 4.1280127], dtype=float32), array([ 6.638401, -3.082734], dtype=float32), array([ 1.0757296, -3.3093247], dtype=float32), array([ 1.3783939, -5.536811 ], dtype=float32), array([2.955714, 9.860627], dtype=float32), array([ 0.70477295, -0.6291731 ], dtype=float32), array([-3.2461612, 10.142818 ], dtype=float32), array([-1.2181478,  1.1223941], dtype=float32), array([-3.8409362 ,  0.80490106], dtype=float32), array([ 0.17630422, -0.3359812 ], dtype=float32), array([ 3.162553 , -2.5711253], dtype=float32), array([ 5.5714703, -3.9132862], dtype=float32), array([0.6824709, 1.0390799], dtype=float32), array([ 5.6567373, -2.6740744], dtype=float32), array([-3.8036616, 16.4591   ], dtype=float32), array([-3.795615 ,  2.2825863], dtype=float32), array([2.4175534 , 0.45500308], dtype=float32), array([ 2.0878985, -1.9958258], dtype=float32), array([ 6.3414783, -2.6905875], dtype=float32), array([-5.1466656,  2.6634648], dtype=float32), array([-8.707109  ,  0.64233047], dtype=float32), array([4.6528254 , 0.03323114], dtype=float32), array([ 1.9201326 , -0.20156187], dtype=float32), array([0.8930888, 5.8741775], dtype=float32), array([-3.6915066, -2.7995853], dtype=float32), array([ 3.4145215, -3.826417 ], dtype=float32), array([2.1725988, 5.2677603], dtype=float32), array([ 4.6788597, -2.7668111], dtype=float32), array([ 5.0361547, -1.6059697], dtype=float32), array([ 2.7244585, -1.3038709], dtype=float32), array([ 3.7727282, -3.802946 ], dtype=float32), array([ 5.51985 , -2.861322], dtype=float32), array([ 4.8865786, -3.8939414], dtype=float32), array([ 2.7645738, -3.005938 ], dtype=float32), array([3.897918 , 7.4340477], dtype=float32), array([-0.01438034,  4.0581083 ], dtype=float32), array([-3.7049563, -1.2178946], dtype=float32), array([ 4.0749903, -3.5924082], dtype=float32), array([ 3.4839041, -1.6680112], dtype=float32), array([-3.307741, 11.186381], dtype=float32), array([ 2.1532578, -2.519793 ], dtype=float32), array([ 5.407796, -4.347342], dtype=float32), array([1.2314942, 6.817729 ], dtype=float32), array([ 4.2724915, -2.5106122], dtype=float32), array([4.0798893, 1.2258441], dtype=float32), array([ 0.20063937, -2.5420659 ], dtype=float32), array([ 1.7092626, -2.6402671], dtype=float32), array([-0.64775467, -0.49557644], dtype=float32), array([ 2.8153965, -3.0772638], dtype=float32), array([-1.8935461, -1.5658247], dtype=float32), array([ 5.788761, -2.461998], dtype=float32), array([-3.4844134, -1.0251243], dtype=float32), array([2.814545 , 4.6694465], dtype=float32), array([ 4.272208 , -2.9409428], dtype=float32), array([-1.192657 , -0.1729021], dtype=float32), array([1.9801354, 4.026291 ], dtype=float32)]\n","[0, 3, 4, 1, 1, 2, 0, 3, 4, 3, 4, 4, 3, 0, 1, 2, 2, 4, 1, 2, 2, 3, 4, 4, 2, 1, 1, 4, 1, 4, 0, 2, 4, 1, 4, 4, 3, 2, 1, 2, 1, 3, 1, 3, 3, 4, 0, 4, 1, 0, 2, 1, 0, 1, 2, 2, 4, 4, 4, 4, 2, 1, 0, 2, 0, 4, 4, 3, 1, 4, 2, 2, 3, 0, 2, 4, 2, 3, 3, 3, 0, 2, 3, 2, 2, 4, 4, 1, 1, 2, 4, 3, 1, 1, 3, 2, 4, 4, 1, 3, 4, 1, 4, 4, 1, 1, 2, 3, 2, 2, 3, 3, 0, 0, 2, 1, 4, 0, 3, 4, 3, 3, 2, 0, 2, 4, 2, 4, 4, 4, 4, 1, 4, 4, 1, 2, 4, 1, 0, 3, 3, 2, 2, 4, 0, 4, 1, 4, 4, 4, 1, 3, 3, 4, 4, 0, 1, 4, 2, 1, 4, 4, 2, 0, 2, 4, 1, 4, 0, 1, 1, 2, 3, 1, 1, 2, 4, 3, 2, 4, 3, 1, 4, 2, 2, 2, 4, 2, 4, 4, 4, 2, 1, 0, 0, 4, 3, 3, 2, 4, 1, 2, 0, 4, 4, 1, 3, 2, 1, 4, 1, 4, 3, 1, 4, 1, 3, 1, 4, 4, 2, 1, 1, 2, 4, 2, 3, 1, 4, 0, 0, 4, 3, 1, 3, 3, 0, 2, 4, 4, 3, 3, 1, 4, 2, 4, 4, 3, 4, 2, 2, 4, 0, 2, 2, 0, 1, 2, 1, 2, 0, 4, 2, 3, 1, 3, 0, 3, 3, 0, 4, 2, 2, 1, 2, 1, 2, 1, 2, 3, 4, 1, 4, 2, 2, 2, 3, 3, 1, 2, 2, 0, 3, 2, 4, 4, 0, 1, 4, 1, 4, 3, 0, 1, 1, 2, 2, 2, 2, 2, 3, 3, 4, 0, 1, 0, 4, 0, 1, 3, 3, 4, 1, 1, 1, 4, 1, 4, 3, 1, 0, 2, 3, 4, 1, 2, 4, 4, 0, 3, 4, 3, 3, 4, 4, 1, 4, 4, 1, 3, 3, 2, 2, 0, 1, 0, 0, 1, 0, 4, 3, 0, 1, 2, 4, 3, 2, 0, 3, 0, 4, 3, 2, 1, 4, 3, 3, 4, 0, 1, 1, 3, 1, 4, 3, 1, 4, 0, 2, 3, 1, 2, 4, 4, 4, 4, 1, 3, 4, 1, 3, 1, 2, 2, 1, 2, 4, 2, 1, 3, 0, 4, 3, 2, 2, 2, 4, 4, 3, 3, 4, 3, 1, 0, 1, 0, 4, 4, 2, 1, 0, 2, 3, 2, 4, 4, 4, 0, 1, 2, 3, 3, 3, 3, 1, 1, 3, 0, 3, 4, 3, 2, 2, 1, 2, 4, 1, 3, 0, 1, 4, 0, 1, 0, 3, 0, 2, 4, 4, 2, 3, 2, 2, 2, 2, 1, 1, 2, 2, 3, 0, 2, 2, 0, 0, 4, 2, 2, 0, 1, 3, 3, 2, 3, 0, 2, 4, 2, 2, 4, 1, 0, 4, 1, 0, 4, 2, 3, 2, 1, 4, 4, 1, 4, 4, 1, 4, 0, 4, 4, 4, 0, 4, 3, 0, 1, 1, 1, 3, 1, 2, 4, 3, 0, 4, 2, 1, 1, 4, 4, 2, 2, 0, 0, 2, 2, 1, 4, 2, 1, 4, 0, 0, 2, 3, 3, 4, 1, 3, 2, 3, 2, 4, 1, 2, 1, 3, 2, 4, 4, 0, 3, 2, 0, 3, 3, 0, 4, 4, 1, 1, 0, 3, 4, 4, 0, 1, 3, 2, 4, 0, 4, 2, 1, 3, 0, 4, 4, 4, 1, 2, 0, 4, 0, 4, 4, 0, 4, 0, 2, 0, 4, 0, 2, 1, 2, 2, 4, 4, 2, 2, 3, 2, 0, 2, 2, 1, 0, 0, 3, 1, 3, 4, 4, 2, 3, 3, 4, 4, 3, 3, 1, 1, 4, 1, 0, 3, 4, 1, 1, 4, 2, 3, 4, 2, 1, 2, 1, 0, 4, 0, 4, 3, 1, 3, 3, 1, 4, 0, 2, 4, 0, 3, 4, 1, 1, 4, 4, 3, 2, 4, 2, 0, 0, 4, 4, 3, 2, 4, 0, 2, 3, 0, 1, 2, 0, 3, 1, 3, 1, 3, 2, 4, 4, 4, 2, 2, 1, 1, 1, 3, 4, 1, 1, 0, 0, 0, 4, 0, 2, 2, 0, 2, 1, 4, 2, 2, 4, 1, 0, 1, 0, 4, 4, 0, 2, 3, 4, 2, 3, 3, 3, 2, 0, 1, 0, 4, 2, 2, 2, 3, 0, 2, 3, 4, 3, 1, 4, 4, 4, 3, 0, 1, 2, 4, 1, 2, 4, 3, 2, 4, 0, 4, 3, 4, 0, 3, 2, 1, 4, 4, 4, 2, 4, 1, 0, 3, 2, 2, 4, 0, 3, 0, 2, 2, 4, 4, 4, 0, 1, 4, 3, 3, 0, 1, 1, 4, 3, 4, 0, 4, 0, 0, 2, 4, 0, 4, 1, 4, 1, 4, 0, 2, 2, 1, 3, 4, 1, 3, 3, 4, 1, 0, 4, 0, 1, 2, 1, 0, 4, 2, 2, 2, 2, 0, 3, 3, 0, 2, 3, 0, 4, 0, 3, 2, 4, 1, 1, 2, 0, 4, 3, 1, 3, 2, 0, 0, 4, 2, 1, 4, 4, 3, 3, 4, 4, 3, 0, 0, 2, 1, 4, 3, 4, 2, 0, 1, 2, 4, 3, 3, 1, 3, 4, 0, 2, 4, 1, 3, 4, 2, 2, 0, 3, 3, 3, 4, 3, 1, 1, 3, 2, 1, 2, 2, 4, 0, 1, 3, 3, 4, 1, 3, 0, 2, 2, 2, 0, 0, 2, 3, 4, 4, 0, 2, 4, 3, 1, 3, 0, 4, 2, 4, 1, 3, 0, 3, 4, 4, 4, 3, 0, 2, 4, 1, 0, 3, 4, 1, 4, 0, 4, 4, 0, 0, 2, 0, 2, 1, 0, 3, 0, 0, 3, 4, 2, 4, 2, 2, 4, 1, 3, 2, 3, 4, 0, 4, 4, 4, 0, 0, 2, 3, 1, 2, 2, 0, 2, 3, 1, 1, 2, 4, 4, 2, 1, 3, 0, 3, 4, 2, 4, 3, 4, 3, 3, 0, 1, 0, 4, 0, 2, 0, 3, 0, 4, 1, 4, 4, 1, 4, 1, 2, 3, 3, 1, 3, 1, 4, 3, 0, 0, 2, 2, 3, 4, 3, 0, 0, 0, 3, 4, 3, 2, 3, 2, 2, 1, 3, 3, 4, 4, 2, 4, 4, 3, 3, 4, 1, 4, 0, 3, 2, 0, 2, 4, 1, 0, 1, 3, 3, 3, 1, 0, 4, 3, 2, 4, 1, 2, 0, 0, 4, 3, 2, 0, 2, 4, 0, 3, 4, 4, 1, 3, 0, 2, 4, 0, 2, 3, 3, 1, 1, 2, 3, 4, 0, 0, 3, 2, 4, 3, 4, 2, 3, 0, 3, 0, 2, 1, 1, 4, 0, 0, 1, 1, 1, 3, 4, 3, 1, 4, 4, 1, 0, 3, 4, 4, 0, 0, 2, 3, 2, 3, 0, 2, 1, 4, 4, 4, 3, 3, 3, 3, 3, 4, 2, 3, 3, 4, 2, 1, 4, 4, 0, 2, 1, 3, 1, 1, 3, 3, 3, 2, 1, 1, 1, 1, 4, 2, 4, 4, 1, 3, 3, 1, 4, 2, 0, 3, 0, 2, 2, 3, 4, 2, 3, 0, 4, 3, 4, 1, 4, 4, 3, 2, 2, 0, 1, 0, 1, 4, 4, 4, 4, 4, 1, 4, 2, 1, 2, 0, 2, 4, 0, 4, 0, 0, 0, 3, 2, 3, 4, 4, 0, 1, 4, 1, 3, 1, 1, 2, 2, 4, 1, 3, 1, 0, 4, 2, 1, 0, 2, 4, 0, 4, 2, 3, 2, 3, 4, 3, 1, 1, 3, 2, 4, 0, 4, 4, 4, 4, 2, 1, 0, 3, 4, 4, 4, 4, 0, 2, 4, 2, 1, 1, 4, 0, 1, 2, 2, 3, 0, 1, 4, 0, 0, 4, 1, 3, 3, 2, 2, 0, 1, 4, 4, 2, 4, 4, 4, 0, 1, 0, 2, 2, 1, 2, 0, 4, 0, 0, 4, 4, 2, 2, 2, 3, 2, 2, 1, 4, 3, 4, 4, 0, 4, 1, 4, 2, 0, 1, 0, 2, 4, 3, 2, 0, 1, 1, 4, 0, 0, 4, 4, 1, 2, 0, 4, 4, 2, 3, 4, 2, 3, 0, 0, 3, 2, 3, 3, 0, 2, 1, 3, 4, 3, 1, 1, 0, 4, 2, 2, 3, 4, 0, 1, 4, 4, 4, 1, 2, 0, 4, 4, 4, 2, 4, 4, 0, 3, 4, 1, 0, 3, 0, 1, 2, 3, 3, 2, 1, 1, 4, 4, 3, 1, 1, 4, 1, 0, 3, 0, 4, 0, 4, 4, 2, 2, 3, 2, 4, 0, 0, 0, 4, 0, 3, 2, 4, 0, 4, 1, 4, 4, 4, 1, 1, 2, 4, 2, 0, 2, 1, 3, 0, 3, 1, 4, 0, 3, 4, 4, 2, 3, 4, 1, 4, 2, 4, 0, 4, 0, 0, 4, 4, 4, 0, 1, 2, 4, 0, 4, 2, 2, 2, 2, 4, 0, 4, 1, 0, 0, 4, 1, 2, 3, 1, 3, 3, 0, 2, 0, 4, 4, 4, 3, 3, 2, 0, 0, 2, 4, 4, 2, 4, 0, 3, 2, 2, 3, 1, 3, 2, 0, 3, 0, 3, 3, 0, 1, 0, 0, 4, 4, 4, 1, 2, 2, 1, 2, 2, 4, 3, 4, 1, 2, 4, 3, 0, 1, 0, 1, 4, 2, 2, 2, 4, 0, 1, 4, 4, 3, 3, 1, 3, 2, 1, 4, 4, 0, 4, 0, 1, 4, 2, 3, 3, 0, 4, 1, 3, 4, 4, 1, 0, 1, 1, 2, 3, 0, 4, 0, 0, 2, 4, 2, 1, 4, 1, 3, 3, 4, 4, 3, 2, 3, 1, 4, 1, 1, 2, 2, 0, 2, 3, 2, 4, 3, 1, 3, 4, 2, 2, 3, 4, 4, 2, 0, 3, 1, 2, 1, 4, 0, 4, 3, 0, 3, 0, 4, 4, 2, 0, 2, 4, 4, 4, 3, 2, 3, 1, 4, 0, 4, 1, 3, 3, 0, 4, 4, 2, 0, 3, 1, 2, 4, 4, 0, 2, 1, 3, 4, 1, 2, 0, 3, 3, 4, 4, 4, 4, 4, 2, 0, 4, 3, 2, 4, 0, 0, 3, 2, 4, 4, 1, 3, 4, 1, 3, 1, 0, 0, 3, 4, 4, 1, 2, 3, 4, 0, 0, 3, 1, 0, 1, 0, 1, 1, 0, 4, 4, 3, 2, 3, 2, 4, 4, 4, 4, 1, 3, 4, 4, 0, 3, 2, 4, 1, 4, 3, 3, 2, 3, 4, 3, 3, 0, 1, 2, 0, 1, 4, 3, 0, 1, 3, 4, 1, 4, 2, 4, 4, 3, 3, 0, 4, 3, 2, 4, 2, 0, 4, 2, 2, 3, 0, 2, 4, 2, 3, 0, 1, 4, 2, 4, 0, 0, 3, 0, 3, 0, 3, 2, 3, 3, 4, 4, 2, 1, 4, 2, 2, 3, 0, 1, 4, 0, 1, 4, 0, 0, 0, 2, 1, 4, 3, 2, 0, 0, 4, 1, 3, 4, 2, 1, 4, 2, 2, 2, 1, 4, 2, 0, 1, 1, 3, 3, 4, 0, 4, 4, 4, 2, 2, 0, 2, 3, 0, 4, 4, 2, 2, 0, 4, 0, 2, 0, 2, 4, 1, 1, 2, 4, 4, 3, 0, 2, 3, 1, 1, 2, 4, 4, 3, 1, 3, 4, 3, 0, 4, 2, 3, 4, 0, 0, 0, 3, 1, 1, 4, 0, 3, 2, 4, 4, 3, 3, 0, 1, 3, 1, 0, 4, 0, 1, 2, 2, 4, 2, 1, 4, 0, 0, 3, 2, 1, 1, 1, 0, 3, 2, 3, 4, 4, 0, 4, 2, 2, 3, 4, 4, 4, 0, 2, 3, 2, 1, 2, 1, 0, 2, 3, 2, 4, 2, 0, 1, 3, 3, 3, 3, 1, 2, 4, 2, 2, 4, 4, 3, 0, 1, 2, 4, 0, 1, 4, 0, 1, 4, 4, 0, 2, 3, 2, 1, 2, 4, 1, 3, 1, 4, 0, 4, 3, 0, 4, 4, 4, 2, 0, 2, 0, 3, 2, 0, 4, 3, 3, 0, 3, 3, 4, 1, 3, 4, 2, 4, 2, 2, 1, 3, 3, 4, 0, 4, 0, 0, 2, 1, 4, 4, 2, 1, 1, 0, 0, 3, 4, 2, 3, 3, 2, 4, 4, 4, 2, 0, 0, 1, 2, 0, 1, 2, 0, 2, 0, 4, 4, 3, 4, 4, 1, 3, 2, 4, 4, 0, 4, 2, 3, 1, 0, 0, 1, 0, 2, 4, 4, 0, 0, 2, 4, 2, 1, 1, 3, 0, 4, 3, 4, 4, 1, 0, 2, 4, 3, 3, 3, 2, 1, 1, 4, 4, 3, 3, 0, 3, 3, 4, 4, 2, 0, 3, 4, 2, 2, 1, 0, 4, 2, 2, 4, 4, 4, 0, 4, 1, 4, 0, 3, 4, 1, 0, 2, 3, 2, 0, 1, 0, 2, 0, 0, 1, 4, 1, 2, 2, 2, 3, 3, 0, 3, 4, 2, 4, 4, 1, 0, 4, 0, 3, 2, 4, 0, 2, 1, 3, 4, 1, 2, 4, 0, 4, 4, 4, 4, 2, 0, 4, 1, 1, 0, 3, 3, 2, 3, 1, 2, 4, 2, 4, 0, 4, 4, 0, 2, 3, 0, 4, 0, 4, 3, 4, 1, 2, 4, 3, 4, 1, 4, 4, 1, 0, 3, 4, 2, 0, 4, 1, 3, 2, 4, 2, 0, 2, 0, 4, 3, 2, 2, 0, 3, 2, 1, 4, 2, 0, 4, 0, 1, 4, 3, 0, 2, 2, 0, 0, 4, 4, 3, 4, 4, 3, 3, 1, 3, 2, 4, 1, 1, 4, 4, 4, 0, 4, 4, 0, 4, 2, 2, 0, 2, 0, 1, 1, 2, 4, 4, 0, 4, 0, 1, 3, 2, 3, 1, 1, 3, 3, 0, 2, 4, 0, 4, 3, 4, 4, 4, 4, 4, 0, 0, 1, 4, 4, 0, 2, 4, 0, 4, 3, 2, 2, 2, 4, 2, 3, 1, 0, 4, 1, 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FoeD3fFRmkUR","colab_type":"code","outputId":"c76a546e-5f68-4b5d-a087-c2dadb310e94","executionInfo":{"status":"ok","timestamp":1565565810957,"user_tz":-540,"elapsed":742,"user":{"displayName":"서민성","photoUrl":"","userId":"06770200570856547998"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["no_list = {}\n","\n","no_list[0] = 0\n","no_list[1] = 0\n","no_list[2] = 0\n","no_list[3] = 0\n","no_list[4] = 0\n","\n","for item in argm_list:\n","  no_list[item] = no_list[item] + 1\n","\n","print(no_list)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{0: 408, 1: 389, 2: 467, 3: 426, 4: 633}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_Hv40U7IhSfA","colab_type":"code","colab":{}},"source":["# 모든 데이터의 xy 좌표값 저장\n","import pickle\n","\n","with open('drive/My Drive/AWD/xylist.pk', 'wb') as f:\n","    pickle.dump(xy_list, f, pickle.HIGHEST_PROTOCOL)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pyPuK1iThdDF","colab_type":"code","outputId":"730c4a2e-ddc0-4b55-f320-3b784e1ac041","executionInfo":{"status":"ok","timestamp":1565582118879,"user_tz":-540,"elapsed":1484,"user":{"displayName":"서민성","photoUrl":"","userId":"06770200570856547998"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# 모든 데이터의 xy 좌표값 가져오기\n","import pickle\n","with open('drive/My Drive/AWD/xylist.pk', 'rb') as f:\n","    xy_list = pickle.load(f)\n","\n","print(len(xy_list))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["2323\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oVqfxMfmhgsm","colab_type":"code","colab":{}},"source":["'''\n","xy 좌표를 x와 y를 따로 나누기\n","'''\n","import numpy as np\n","\n","n_list = []\n","for i in range(len(xy_list)):\n","    n_list.append(xy_list[i].tolist())\n","    \n","n_list=np.array(n_list)\n","\n","x_l = []\n","y_l = []\n","\n","for data in n_list:\n","    x_l.append(data[0])\n","    y_l.append(data[1])\n","\n","x_l = np.array(x_l)\n","y_l = np.array(y_l)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wonDXQx5sb6S","colab_type":"code","outputId":"2679d188-b222-48a3-b2ff-7ceafb446026","executionInfo":{"status":"ok","timestamp":1565565824103,"user_tz":-540,"elapsed":750,"user":{"displayName":"서민성","photoUrl":"","userId":"06770200570856547998"}},"colab":{"base_uri":"https://localhost:8080/","height":143}},"source":["'''\n","x좌표와 y좌표가 일정한 범위 내에 있도록 범위제한\n","'''\n","x_l = x_l - min(x_l)\n","y_l = y_l - min(y_l)\n","\n","x_l = (x_l/max(x_l))*10000-5000\n","y_l = (y_l/max(y_l))*10000-5000\n","\n","print('min_x: ', min(x_l))\n","print('max_x: ', max(x_l))\n","print('mean_x: ', sum(x_l)/len(x_l))\n","print()\n","print('min_y: ', min(y_l))\n","print('max_y: ', max(y_l))\n","print('mean_y: ', sum(y_l)/len(y_l))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["min_x:  -5000.0\n","max_x:  5000.0\n","mean_x:  1341.750756307506\n","\n","min_y:  -5000.0\n","max_y:  5000.0\n","mean_y:  -2613.630332677397\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KdEbxnpqsgOg","colab_type":"code","colab":{}},"source":["'''\n","K-Means를 위한 입력데이터 옮기기\n","'''\n","\n","# 입력 데이터\n","X = np.zeros((len(x_l), 2))\n","\n","for i in range(len(x_l)):\n","    X[i,0]=x_l[i]\n","    X[i,1]=y_l[i]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HOW0lYopsjwt","colab_type":"code","colab":{}},"source":["'''\n","입력 데이터 그려보기\n","'''\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sns\n","\n","df = pd.DataFrame({\"x\": [v[0] for v in X],\n","                   \"y\": [v[1] for v in X]})\n","sns.lmplot(\"x\", \"y\", data=df, fit_reg=False, size=6)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xw-YtiT0svZ4","colab_type":"code","outputId":"de94bf54-9e82-4365-8af4-76e3c198582c","executionInfo":{"status":"ok","timestamp":1565582251495,"user_tz":-540,"elapsed":6601,"user":{"displayName":"서민성","photoUrl":"","userId":"06770200570856547998"}},"colab":{"base_uri":"https://localhost:8080/","height":181}},"source":["'''\n","K-means 구현\n","5개의 군집으로 그룹화\n","'''\n","\n","import tensorflow as tf\n","import pandas as pd\n","import seaborn as sns\n","\n","tf.reset_default_graph()\n","\n","# 모든 데이터를 상수 텐서로 옮김\n","vectors = tf.constant(X.tolist())\n","\n","# 초기 단계 : 중심 k(4)개를 입력데이터에서 무작위로 선택\n","k = 5\n","centroides = tf.Variable(tf.slice(tf.random_shuffle(vectors),[0,0],[k,-1]))\n","  \n","expanded_vectors = tf.expand_dims(vectors, 0)\n","expanded_centroides = tf.expand_dims(centroides, 1)\n","\n","# 할당 단계 : 유클리드 제곱거리 사용\n","diff = expanded_vectors - expanded_centroides\n","sqr = tf.square(diff)\n","distances = tf.reduce_sum(sqr, 2)\n","assignments = tf.argmin(distances,0)\n","\n","# 업데이트 : 새로운 중심 계산\n","conc_list = []\n","for c in range(k):    \n","    equal_s = tf.equal(assignments, c)\n","    where_s = tf.where(equal_s)\n","    reshape_s = tf.reshape(where_s,[1, -1])\n","    gather_s = tf.gather(vectors,reshape_s)\n","    reduce_s = tf.reduce_mean(gather_s, reduction_indices=[1])\n","    conc_list.append(reduce_s)\n","\n","means = tf.concat(conc_list, 0)\n","\n","update_centroides = tf.assign(centroides, means)\n","\n","init_op = tf.initialize_all_variables()\n","\n","sess = tf.Session()\n","sess.run(init_op)\n","\n","for step in range(100):\n","    _, centroid_values, assignment_values = sess.run([update_centroides, centroides, assignments])\n","\n","\n","# assignment_values 텐서의 결과를 확인\n","\n","data = {\"x\": [], \"y\": [], \"cluster\": []}\n","\n","for i in range(len(assignment_values)):\n","    data[\"x\"].append(X[i][0])\n","    data[\"y\"].append(X[i][1])\n","    data[\"cluster\"].append(assignment_values[i])\n","\n","'''\n","df = pd.DataFrame(data)\n","sns.lmplot(\"x\", \"y\", data=df, fit_reg=False, size=6, hue=\"cluster\", legend=False)\n","plt.show()\n","'''"],"execution_count":6,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0812 03:57:28.328675 140514493831040 deprecation.py:323] From <ipython-input-6-6b1890c92479>:32: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0812 03:57:28.366716 140514493831040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n","Instructions for updating:\n","Use `tf.global_variables_initializer` instead.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["'\\ndf = pd.DataFrame(data)\\nsns.lmplot(\"x\", \"y\", data=df, fit_reg=False, size=6, hue=\"cluster\", legend=False)\\nplt.show()\\n'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"gLkUzW3Ac9jV","colab_type":"code","outputId":"04138f36-fa67-49db-d726-5e6692e958dd","executionInfo":{"status":"ok","timestamp":1565582351650,"user_tz":-540,"elapsed":920,"user":{"displayName":"서민성","photoUrl":"","userId":"06770200570856547998"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["# 위 군집화의 결과를 클러스터 사전 데이터로 바꾸기\n","clusters={}\n","for i in range(5):\n","    clusters[i]= []\n","\n","for i in range(len(assignment_values)):\n","    clusters[assignment_values[i]].append(i)\n","    \n","for i in range(5):\n","    clusters[i].sort()\n","\n","for i in range(5):\n","    print('{} : {}'.format(i, clusters[i]))\n","    \n","print()\n","\n","for i in range(5):\n","    print('{} 번 클래스인 탬플릿 {} 개'.format(i, len(clusters[i])))\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["0 : [1, 2, 7, 8, 9, 10, 11, 12, 17, 22, 27, 32, 34, 35, 36, 44, 45, 47, 54, 56, 57, 58, 59, 65, 66, 67, 69, 72, 75, 78, 79, 82, 85, 86, 90, 94, 96, 97, 99, 100, 102, 103, 107, 110, 111, 116, 118, 119, 120, 121, 125, 127, 128, 129, 130, 132, 133, 136, 140, 143, 145, 147, 149, 153, 154, 157, 160, 161, 165, 172, 176, 177, 179, 182, 186, 188, 189, 190, 195, 196, 199, 203, 204, 209, 211, 214, 216, 219, 224, 228, 231, 232, 237, 238, 239, 240, 241, 243, 245, 246, 248, 261, 262, 267, 268, 271, 272, 274, 276, 280, 282, 286, 287, 290, 292, 294, 295, 298, 301, 311, 312, 316, 319, 321, 327, 328, 332, 333, 336, 337, 339, 340, 341, 342, 343, 344, 346, 349, 359, 364, 365, 368, 370, 371, 374, 377, 381, 383, 384, 386, 389, 392, 393, 394, 395, 398, 400, 411, 412, 416, 417, 419, 420, 421, 426, 427, 434, 435, 436, 440, 442, 448, 449, 451, 455, 457, 460, 467, 468, 470, 479, 485, 491, 493, 496, 499, 502, 505, 510, 511, 513, 514, 516, 518, 519, 520, 522, 523, 528, 531, 532, 534, 538, 539, 547, 550, 555, 556, 558, 561, 562, 568, 569, 577, 578, 582, 583, 584, 587, 589, 591, 594, 596, 597, 598, 602, 604, 605, 607, 611, 613, 617, 618, 621, 629, 631, 632, 633, 634, 635, 636, 637, 638, 640, 643, 647, 650, 652, 653, 659, 661, 665, 667, 669, 670, 673, 676, 677, 678, 680, 684, 685, 686, 688, 690, 691, 696, 698, 700, 701, 702, 703, 704, 710, 711, 717, 724, 727, 732, 733, 736, 737, 739, 746, 748, 750, 754, 755, 757, 758, 759, 764, 767, 770, 772, 774, 776, 779, 780, 781, 783, 789, 795, 796, 797, 800, 806, 808, 810, 814, 816, 818, 820, 822, 825, 826, 829, 830, 833, 839, 845, 846, 851, 853, 855, 860, 861, 867, 870, 871, 872, 874, 875, 881, 882, 883, 888, 889, 896, 899, 905, 906, 907, 915, 920, 922, 926, 930, 931, 932, 935, 936, 940, 942, 944, 947, 949, 950, 953, 957, 959, 961, 962, 970, 974, 975, 976, 979, 981, 983, 984, 986, 987, 988, 994, 1002, 1003, 1006, 1008, 1009, 1011, 1012, 1013, 1019, 1023, 1027, 1028, 1030, 1033, 1034, 1036, 1038, 1044, 1045, 1051, 1054, 1056, 1059, 1060, 1061, 1063, 1064, 1067, 1069, 1075, 1080, 1081, 1084, 1085, 1087, 1092, 1097, 1100, 1101, 1106, 1109, 1114, 1115, 1119, 1120, 1121, 1122, 1124, 1126, 1131, 1138, 1139, 1141, 1142, 1145, 1146, 1147, 1155, 1158, 1159, 1161, 1165, 1167, 1169, 1172, 1173, 1177, 1180, 1181, 1182, 1188, 1190, 1191, 1193, 1194, 1196, 1197, 1199, 1203, 1204, 1205, 1206, 1208, 1209, 1210, 1212, 1213, 1214, 1221, 1223, 1224, 1225, 1227, 1233, 1235, 1241, 1242, 1243, 1246, 1248, 1251, 1253, 1255, 1258, 1263, 1265, 1270, 1271, 1276, 1278, 1280, 1281, 1286, 1287, 1288, 1289, 1292, 1296, 1304, 1307, 1309, 1315, 1316, 1318, 1319, 1320, 1329, 1332, 1333, 1334, 1336, 1337, 1338, 1341, 1342, 1343, 1344, 1346, 1348, 1354, 1360, 1363, 1364, 1368, 1369, 1370, 1372, 1379, 1380, 1384, 1385, 1386, 1390, 1391, 1394, 1397, 1398, 1399, 1403, 1404, 1405, 1407, 1408, 1410, 1411, 1417, 1418, 1419, 1423, 1424, 1425, 1428, 1433, 1435, 1436, 1441, 1447, 1449, 1451, 1453, 1454, 1455, 1459, 1466, 1468, 1470, 1471, 1475, 1477, 1479, 1481, 1484, 1485, 1486, 1490, 1492, 1497, 1499, 1503, 1506, 1511, 1513, 1514, 1515, 1522, 1523, 1525, 1527, 1530, 1535, 1543, 1544, 1545, 1550, 1553, 1554, 1557, 1558, 1563, 1567, 1570, 1572, 1578, 1579, 1581, 1584, 1586, 1587, 1589, 1592, 1593, 1599, 1601, 1605, 1608, 1611, 1612, 1613, 1614, 1615, 1618, 1627, 1630, 1631, 1635, 1636, 1639, 1643, 1645, 1646, 1651, 1652, 1655, 1656, 1657, 1658, 1662, 1664, 1669, 1670, 1673, 1676, 1677, 1682, 1688, 1689, 1691, 1692, 1695, 1698, 1703, 1704, 1707, 1709, 1714, 1715, 1719, 1722, 1730, 1731, 1732, 1734, 1736, 1737, 1738, 1739, 1741, 1742, 1743, 1747, 1749, 1754, 1756, 1759, 1762, 1763, 1767, 1769, 1771, 1772, 1773, 1774, 1779, 1782, 1784, 1788, 1790, 1793, 1795, 1798, 1802, 1805, 1806, 1807, 1810, 1811, 1813, 1816, 1825, 1830, 1832, 1833, 1841, 1848, 1850, 1851, 1852, 1859, 1860, 1864, 1869, 1872, 1873, 1874, 1875, 1878, 1882, 1886, 1887, 1892, 1893, 1897, 1903, 1904, 1905, 1906, 1907, 1910, 1913, 1918, 1921, 1924, 1930, 1932, 1933, 1934, 1936, 1939, 1940, 1941, 1942, 1953, 1954, 1958, 1959, 1961, 1964, 1966, 1967, 1968, 1969, 1973, 1976, 1979, 1980, 1983, 1987, 1989, 1991, 1993, 1994, 1996, 1997, 1998, 2003, 2008, 2010, 2011, 2012, 2015, 2017, 2021, 2023, 2025, 2028, 2030, 2031, 2037, 2038, 2040, 2043, 2044, 2045, 2050, 2053, 2055, 2057, 2058, 2060, 2061, 2064, 2065, 2066, 2068, 2070, 2077, 2078, 2082, 2086, 2088, 2090, 2091, 2095, 2096, 2097, 2098, 2102, 2103, 2105, 2109, 2110, 2113, 2114, 2119, 2122, 2123, 2124, 2126, 2128, 2130, 2131, 2144, 2150, 2152, 2153, 2156, 2159, 2161, 2167, 2168, 2171, 2173, 2174, 2175, 2183, 2184, 2186, 2189, 2191, 2193, 2194, 2199, 2201, 2202, 2203, 2206, 2208, 2210, 2211, 2214, 2218, 2222, 2225, 2227, 2233, 2235, 2236, 2238, 2241, 2242, 2248, 2249, 2251, 2252, 2253, 2254, 2258, 2261, 2262, 2263, 2266, 2268, 2270, 2277, 2278, 2280, 2285, 2288, 2292, 2294, 2295, 2297, 2298, 2299, 2300, 2304, 2305, 2308, 2310, 2315, 2317, 2320]\n","1 : [3, 4, 18, 25, 28, 53, 87, 93, 98, 101, 115, 131, 137, 138, 146, 150, 156, 169, 192, 200, 208, 210, 213, 215, 222, 227, 233, 260, 264, 273, 275, 277, 288, 297, 303, 314, 318, 324, 329, 334, 345, 354, 357, 379, 401, 408, 429, 430, 438, 444, 445, 453, 476, 489, 503, 509, 526, 527, 536, 537, 549, 563, 565, 580, 585, 586, 599, 626, 642, 644, 648, 649, 663, 666, 675, 699, 709, 723, 728, 730, 744, 756, 762, 778, 784, 799, 805, 819, 824, 837, 856, 880, 886, 891, 897, 908, 912, 937, 958, 968, 980, 989, 1000, 1017, 1026, 1029, 1031, 1037, 1057, 1068, 1078, 1082, 1088, 1111, 1112, 1134, 1135, 1140, 1143, 1156, 1171, 1176, 1178, 1184, 1186, 1195, 1211, 1220, 1226, 1229, 1245, 1247, 1249, 1250, 1254, 1256, 1272, 1283, 1294, 1295, 1308, 1314, 1322, 1326, 1340, 1365, 1412, 1416, 1421, 1426, 1427, 1452, 1463, 1467, 1476, 1500, 1504, 1507, 1540, 1555, 1560, 1562, 1574, 1594, 1596, 1609, 1617, 1619, 1629, 1640, 1642, 1665, 1674, 1680, 1705, 1708, 1710, 1725, 1740, 1758, 1761, 1768, 1792, 1809, 1815, 1818, 1824, 1840, 1844, 1845, 1871, 1879, 1885, 1898, 1899, 1909, 1911, 1915, 1920, 1928, 1971, 1978, 1985, 1988, 2020, 2029, 2033, 2049, 2052, 2062, 2071, 2074, 2084, 2092, 2100, 2101, 2132, 2138, 2143, 2157, 2169, 2180, 2181, 2204, 2212, 2240, 2255, 2259, 2260, 2275, 2282, 2286, 2287, 2303, 2318]\n","2 : [5, 15, 16, 19, 20, 23, 24, 29, 31, 33, 37, 39, 41, 42, 43, 48, 50, 51, 55, 60, 63, 70, 71, 74, 76, 77, 81, 83, 84, 88, 89, 91, 95, 104, 105, 106, 108, 109, 114, 117, 122, 124, 126, 135, 139, 141, 142, 148, 151, 152, 158, 162, 164, 167, 171, 173, 174, 175, 178, 183, 184, 185, 187, 191, 197, 198, 201, 205, 206, 207, 217, 218, 220, 221, 223, 225, 226, 230, 235, 244, 247, 249, 250, 251, 253, 254, 256, 257, 258, 259, 265, 270, 278, 281, 283, 284, 285, 289, 293, 299, 300, 302, 305, 306, 307, 308, 309, 310, 320, 325, 326, 331, 335, 347, 348, 350, 351, 352, 360, 362, 363, 366, 372, 373, 375, 376, 380, 382, 388, 391, 397, 399, 402, 403, 404, 405, 406, 407, 409, 413, 414, 415, 418, 424, 428, 431, 432, 433, 439, 443, 446, 450, 452, 454, 456, 459, 462, 466, 469, 471, 472, 473, 474, 475, 477, 478, 481, 482, 486, 487, 490, 492, 495, 497, 498, 506, 507, 508, 515, 529, 530, 535, 540, 541, 544, 545, 546, 548, 552, 553, 554, 557, 559, 560, 564, 566, 567, 571, 572, 574, 575, 588, 592, 600, 609, 615, 616, 619, 620, 622, 624, 625, 639, 651, 654, 656, 657, 664, 672, 679, 681, 687, 694, 697, 705, 706, 719, 720, 722, 725, 726, 735, 738, 740, 741, 742, 747, 749, 752, 753, 760, 763, 765, 766, 768, 769, 773, 777, 782, 786, 787, 788, 791, 793, 794, 801, 802, 807, 813, 817, 823, 827, 831, 836, 840, 841, 842, 843, 848, 849, 854, 857, 858, 863, 864, 868, 869, 873, 876, 879, 884, 887, 892, 893, 895, 898, 900, 901, 903, 904, 910, 911, 913, 914, 917, 919, 921, 924, 925, 929, 934, 938, 941, 946, 948, 952, 956, 965, 967, 973, 977, 978, 982, 991, 993, 995, 997, 998, 999, 1001, 1004, 1010, 1015, 1021, 1025, 1032, 1039, 1042, 1043, 1046, 1050, 1052, 1053, 1055, 1058, 1062, 1065, 1066, 1071, 1072, 1074, 1086, 1089, 1093, 1094, 1096, 1098, 1099, 1103, 1105, 1108, 1113, 1118, 1123, 1128, 1129, 1130, 1136, 1137, 1150, 1151, 1152, 1157, 1160, 1162, 1163, 1164, 1166, 1170, 1175, 1183, 1189, 1192, 1201, 1202, 1215, 1216, 1218, 1222, 1228, 1230, 1232, 1240, 1252, 1259, 1262, 1266, 1267, 1268, 1269, 1274, 1275, 1279, 1282, 1285, 1291, 1293, 1298, 1299, 1300, 1303, 1310, 1311, 1312, 1317, 1324, 1325, 1327, 1335, 1339, 1347, 1349, 1351, 1353, 1355, 1356, 1359, 1366, 1371, 1373, 1377, 1378, 1382, 1383, 1387, 1388, 1392, 1393, 1400, 1401, 1406, 1414, 1420, 1429, 1431, 1437, 1438, 1439, 1440, 1445, 1448, 1458, 1460, 1462, 1464, 1472, 1473, 1474, 1478, 1480, 1488, 1489, 1493, 1494, 1495, 1496, 1502, 1505, 1509, 1516, 1517, 1518, 1521, 1524, 1528, 1529, 1532, 1533, 1537, 1538, 1546, 1547, 1548, 1551, 1552, 1556, 1564, 1565, 1566, 1569, 1571, 1573, 1575, 1576, 1577, 1583, 1585, 1590, 1591, 1597, 1598, 1604, 1606, 1610, 1616, 1621, 1622, 1624, 1625, 1626, 1628, 1632, 1633, 1637, 1641, 1648, 1650, 1654, 1659, 1660, 1661, 1666, 1667, 1671, 1675, 1678, 1679, 1684, 1686, 1687, 1690, 1693, 1696, 1697, 1701, 1702, 1706, 1713, 1717, 1718, 1723, 1727, 1733, 1735, 1745, 1746, 1751, 1752, 1753, 1755, 1765, 1766, 1770, 1776, 1777, 1778, 1780, 1783, 1785, 1786, 1787, 1789, 1794, 1800, 1803, 1804, 1808, 1812, 1819, 1823, 1826, 1827, 1831, 1834, 1836, 1837, 1838, 1839, 1842, 1853, 1854, 1856, 1857, 1861, 1862, 1866, 1868, 1870, 1877, 1880, 1881, 1883, 1884, 1890, 1891, 1900, 1902, 1916, 1917, 1919, 1925, 1926, 1931, 1937, 1938, 1944, 1946, 1947, 1948, 1951, 1952, 1955, 1960, 1962, 1963, 1965, 1972, 1982, 1984, 1986, 1999, 2001, 2004, 2006, 2007, 2014, 2016, 2018, 2019, 2022, 2032, 2034, 2039, 2041, 2042, 2046, 2059, 2063, 2069, 2076, 2081, 2083, 2089, 2094, 2099, 2104, 2108, 2111, 2115, 2116, 2120, 2121, 2127, 2134, 2136, 2140, 2145, 2146, 2147, 2148, 2149, 2154, 2155, 2162, 2163, 2165, 2166, 2170, 2176, 2177, 2179, 2185, 2188, 2190, 2196, 2197, 2205, 2207, 2215, 2216, 2219, 2220, 2221, 2223, 2228, 2229, 2230, 2232, 2244, 2245, 2250, 2256, 2257, 2265, 2269, 2272, 2274, 2276, 2279, 2283, 2284, 2289, 2291, 2296, 2307, 2312, 2313, 2314, 2316, 2321]\n","3 : [0, 26, 49, 52, 61, 68, 80, 92, 112, 113, 123, 134, 144, 155, 163, 170, 181, 193, 194, 202, 236, 242, 252, 291, 323, 356, 367, 387, 390, 396, 410, 423, 465, 494, 500, 504, 543, 570, 581, 593, 606, 608, 614, 628, 630, 658, 671, 674, 693, 707, 745, 761, 790, 792, 834, 835, 850, 862, 885, 902, 923, 943, 954, 964, 990, 996, 1005, 1018, 1035, 1048, 1070, 1073, 1076, 1091, 1102, 1133, 1154, 1179, 1185, 1187, 1200, 1207, 1217, 1237, 1244, 1260, 1264, 1284, 1302, 1330, 1357, 1375, 1376, 1381, 1389, 1396, 1402, 1415, 1432, 1442, 1457, 1465, 1483, 1498, 1531, 1541, 1549, 1568, 1580, 1602, 1607, 1620, 1623, 1644, 1649, 1672, 1683, 1694, 1716, 1728, 1729, 1748, 1757, 1760, 1791, 1814, 1822, 1855, 1867, 1895, 1935, 1943, 1950, 1957, 1975, 1990, 1992, 2002, 2013, 2036, 2056, 2075, 2080, 2085, 2117, 2118, 2129, 2141, 2151, 2164, 2178, 2195, 2217, 2226, 2234, 2243, 2271, 2273, 2281, 2306]\n","4 : [6, 13, 14, 21, 30, 38, 40, 46, 62, 64, 73, 159, 166, 168, 180, 212, 229, 234, 255, 263, 266, 269, 279, 296, 304, 313, 315, 317, 322, 330, 338, 353, 355, 358, 361, 369, 378, 385, 422, 425, 437, 441, 447, 458, 461, 463, 464, 480, 483, 484, 488, 501, 512, 517, 521, 524, 525, 533, 542, 551, 573, 576, 579, 590, 595, 601, 603, 610, 612, 623, 627, 641, 645, 646, 655, 660, 662, 668, 682, 683, 689, 692, 695, 708, 712, 713, 714, 715, 716, 718, 721, 729, 731, 734, 743, 751, 771, 775, 785, 798, 803, 804, 809, 811, 812, 815, 821, 828, 832, 838, 844, 847, 852, 859, 865, 866, 877, 878, 890, 894, 909, 916, 918, 927, 928, 933, 939, 945, 951, 955, 960, 963, 966, 969, 971, 972, 985, 992, 1007, 1014, 1016, 1020, 1022, 1024, 1040, 1041, 1047, 1049, 1077, 1079, 1083, 1090, 1095, 1104, 1107, 1110, 1116, 1117, 1125, 1127, 1132, 1144, 1148, 1149, 1153, 1168, 1174, 1198, 1219, 1231, 1234, 1236, 1238, 1239, 1257, 1261, 1273, 1277, 1290, 1297, 1301, 1305, 1306, 1313, 1321, 1323, 1328, 1331, 1345, 1350, 1352, 1358, 1361, 1362, 1367, 1374, 1395, 1409, 1413, 1422, 1430, 1434, 1443, 1444, 1446, 1450, 1456, 1461, 1469, 1482, 1487, 1491, 1501, 1508, 1510, 1512, 1519, 1520, 1526, 1534, 1536, 1539, 1542, 1559, 1561, 1582, 1588, 1595, 1600, 1603, 1634, 1638, 1647, 1653, 1663, 1668, 1681, 1685, 1699, 1700, 1711, 1712, 1720, 1721, 1724, 1726, 1744, 1750, 1764, 1775, 1781, 1796, 1797, 1799, 1801, 1817, 1820, 1821, 1828, 1829, 1835, 1843, 1846, 1847, 1849, 1858, 1863, 1865, 1876, 1888, 1889, 1894, 1896, 1901, 1908, 1912, 1914, 1922, 1923, 1927, 1929, 1945, 1949, 1956, 1970, 1974, 1977, 1981, 1995, 2000, 2005, 2009, 2024, 2026, 2027, 2035, 2047, 2048, 2051, 2054, 2067, 2072, 2073, 2079, 2087, 2093, 2106, 2107, 2112, 2125, 2133, 2135, 2137, 2139, 2142, 2158, 2160, 2172, 2182, 2187, 2192, 2198, 2200, 2209, 2213, 2224, 2231, 2237, 2239, 2246, 2247, 2264, 2267, 2290, 2293, 2301, 2302, 2309, 2311, 2319, 2322]\n","\n","0 번 클래스인 탬플릿 887 개\n","1 번 클래스인 탬플릿 235 개\n","2 번 클래스인 탬플릿 700 개\n","3 번 클래스인 탬플릿 160 개\n","4 번 클래스인 탬플릿 341 개\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vJ6Cqkn3s-g6","colab_type":"code","colab":{}},"source":["# 저장하기  \n","with open('drive/My Drive/AWD/clusters.pickle', 'wb') as f:\n","    pickle.dump(clusters, f, pickle.HIGHEST_PROTOCOL)\n"],"execution_count":0,"outputs":[]}]}